<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Edge Computing Lab Quiz Notes</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1bfaecc5-ba04-809f-b457-e564a1a80bf5" class="page sans"><header><h1 class="page-title">Edge Computing Lab Quiz Notes</h1><p class="page-description"></p></header><div class="page-body"><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Week 1 Setup Raspberry Pi</summary><div class="indented"><h3 id="1bfaecc5-ba04-8053-9d2b-f573d550ed27" class="">Guide on how to configure the Raspberry Pi 400 with a Webcam</h3><h3 id="1bfaecc5-ba04-80b7-aa87-c0b944baff00" class="">Materials Needed</h3><ul id="1bfaecc5-ba04-8000-90b4-e29f2589f131" class="bulleted-list"><li style="list-style-type:disc">Raspberry Pi 400</li></ul><ul id="1bfaecc5-ba04-8043-9907-d5d106ac5719" class="bulleted-list"><li style="list-style-type:disc">MicroSD card with Raspberry Pi OS</li></ul><ul id="1bfaecc5-ba04-80a5-94c5-e337f1b939b6" class="bulleted-list"><li style="list-style-type:disc">Logitech C310 HD Webcam</li></ul><ul id="1bfaecc5-ba04-80ad-aa1f-c215ad02731b" class="bulleted-list"><li style="list-style-type:disc">Internet connection (Mobile Hotspot)</li></ul><h3 id="1bfaecc5-ba04-80ae-8101-d8072eb73c5e" class="">Section 1: Setting Up Raspberry Pi 400</h3><ol type="1" id="1bfaecc5-ba04-807a-aa49-d943a0690c0d" class="numbered-list" start="1"><li><strong>Install Raspberry Pi OS</strong>:</li></ol><ul id="1bfaecc5-ba04-809c-8696-f1d1d202ca5b" class="bulleted-list"><li style="list-style-type:disc">Download the official Imager Software from the following link and install it: <a href="https://downloads.raspberrypi.org/imager/imager_latest.exe">https://downloads.raspberrypi.org/imager/imager_latest.exe</a></li></ul><ul id="1bfaecc5-ba04-80ae-8c50-e1605e5f72de" class="bulleted-list"><li style="list-style-type:disc">Connect the microSD card into the PC</li></ul><ul id="1bfaecc5-ba04-808e-bd20-e3b22bad89eb" class="bulleted-list"><li style="list-style-type:disc">Start the Imager software</li></ul><ul id="1bfaecc5-ba04-8049-b411-c5232bb9588f" class="bulleted-list"><li style="list-style-type:disc">Choose the following options and click &#x27;Next&#x27;:<figure id="1bfaecc5-ba04-8077-959e-cb64c4725928" class="image"><a href="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/9abdedc2-2693-44ff-9a48-0ac1dfd688ad"><img src="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/9abdedc2-2693-44ff-9a48-0ac1dfd688ad"/></a></figure></li></ul><ul id="1bfaecc5-ba04-8096-954c-d1154f933733" class="bulleted-list"><li style="list-style-type:disc">You will see the following when you click on &quot;Edit Settings&quot;.<figure id="1bfaecc5-ba04-8087-963b-ea06c7af33cc" class="image"><a href="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/21ba2607-cd45-4406-830f-95732daf01ed"><img src="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/21ba2607-cd45-4406-830f-95732daf01ed"/></a></figure><figure id="1bfaecc5-ba04-806c-9aed-c0f98ae8b2b3" class="image"><a href="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/588dbd32-0d1b-41fe-9a00-d2b6607daa71"><img src="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/588dbd32-0d1b-41fe-9a00-d2b6607daa71"/></a></figure><ul id="1bfaecc5-ba04-8057-8db2-fbc5f7bec617" class="bulleted-list"><li style="list-style-type:circle">Give a unique name for your RPi hostname.</li></ul><ul id="1bfaecc5-ba04-800d-a2f7-e5783fcc59b0" class="bulleted-list"><li style="list-style-type:circle">Pick an appropriate username and password</li></ul><ul id="1bfaecc5-ba04-80c0-a541-e724ce44a198" class="bulleted-list"><li style="list-style-type:circle">Configure the WiFi to your hotspot (SIT WiFi doesnt work well with RPi)</li></ul><ul id="1bfaecc5-ba04-80da-82d9-d6d1dd6c7036" class="bulleted-list"><li style="list-style-type:circle">Also enable SSH in the services tab (to SSH to Pi later)</li></ul><ul id="1bfaecc5-ba04-80bb-8417-e8c754d2911d" class="bulleted-list"><li style="list-style-type:circle">Click on &quot;Save&quot;</li></ul><ul id="1bfaecc5-ba04-80e3-a7d1-c537c7c4f4f7" class="bulleted-list"><li style="list-style-type:circle">Click on &quot;Yes&quot; and &quot;Yes&quot; again.</li></ul></li></ul><h3 id="1bfaecc5-ba04-80c4-8df3-dad7e045729d" class="">Section 2: Configuring Raspberry Pi</h3><ol type="1" id="1bfaecc5-ba04-80bf-81df-e1cfca6ffeca" class="numbered-list" start="1"><li><strong>Assembling Hardware</strong>:</li></ol><ul id="1bfaecc5-ba04-805f-8e8a-e9a79952327f" class="bulleted-list"><li style="list-style-type:disc">Insert the MicroSD card, power adapter and USB Webcam into the Raspberry Pi 400.</li></ul><ul id="1bfaecc5-ba04-807f-b26e-d222e2c6c060" class="bulleted-list"><li style="list-style-type:disc">Power on the Raspberry Pi.<figure id="1bfaecc5-ba04-8077-bab1-de286b30b22a" class="image"><a href="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/9fd5dfa1-ddee-4cb5-a557-0e4283a513a7"><img src="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/9fd5dfa1-ddee-4cb5-a557-0e4283a513a7"/></a></figure></li></ul><ol type="1" id="1bfaecc5-ba04-80cc-8ef3-f4acabb8c0b7" class="numbered-list" start="1"><li><strong>Enabling VNC</strong>:</li></ol><ul id="1bfaecc5-ba04-80a4-b57d-d02a8e43b994" class="bulleted-list"><li style="list-style-type:disc">Connect to RPi 400 via SSH (e.g. use Putty).</li></ul><ul id="1bfaecc5-ba04-80e2-b043-ede73db6c473" class="bulleted-list"><li style="list-style-type:disc">You can find the RPi400&#x27;s IP via your mobile hotspot or router.<ul id="1bfaecc5-ba04-8058-9597-c8fd41ed0751" class="bulleted-list"><li style="list-style-type:circle">Alternatively, you may use the following: <code>raspberrypi.local</code> (assuming you named your device as raspberrypi)</li></ul></li></ul><ul id="1bfaecc5-ba04-80e0-a7c7-d6b541a88f7b" class="bulleted-list"><li style="list-style-type:disc">[Optional] If you are using a Rasperry PI without keyboard, you need to enable VNC option without keyboard by the below command<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80f3-8afe-e820ca2ec198" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">sudo apt install haveged</code></pre></li></ul><ul id="1bfaecc5-ba04-80cd-8276-c5cb779c47e5" class="bulleted-list"><li style="list-style-type:disc">Enable the VNC using the following command and selecting &quot;Interfacing Options&quot;, navigate to &quot;VNC&quot;, choose &quot;Yes&quot;, select &quot;Ok&quot; and select &quot;Finish&quot;.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8036-9492-c433d40f9e89" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">sudo raspi-config</code></pre></li></ul><figure id="1bfaecc5-ba04-80f1-a0c3-e51aab205699" class="image"><a href="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/8ea119c9-8b27-48b8-80b2-4f32821ffd51"><img src="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/8ea119c9-8b27-48b8-80b2-4f32821ffd51"/></a></figure><ol type="1" id="1bfaecc5-ba04-80b6-8775-eea085a714a2" class="numbered-list" start="1"><li><strong>Setting Up a Static IP (Optional)</strong>:<ul id="1bfaecc5-ba04-80b2-bea7-c1780bcf92e5" class="bulleted-list"><li style="list-style-type:disc">For easier connection, assign a static IP to your RPi 400.</li></ul><ul id="1bfaecc5-ba04-8079-ad98-ebc2b2d1d94f" class="bulleted-list"><li style="list-style-type:disc">Open Terminal and edit the <code>dhcpcd.conf</code> file:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8064-9eb8-dbd2e608941f" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">sudo nano /etc/dhcpcd.conf</code></pre></li></ul><ul id="1bfaecc5-ba04-802e-89eb-d95e0ff0ccea" class="bulleted-list"><li style="list-style-type:disc">Add the following lines at the end of the file (replace with your network details):<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8081-b4f3-c740a4a9eb7b" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">interface eth0
static ip_address=192.168.x.xxx/24
static routers=192.168.x.x
static domain_name_servers=192.168.x.x</code></pre></li></ul><ul id="1bfaecc5-ba04-8029-a3d9-f3f4c291498c" class="bulleted-list"><li style="list-style-type:disc">Save and exit the editor (CTRL+X, then Y, then Enter).</li></ul><ul id="1bfaecc5-ba04-80fb-ae31-f5e982e95c5a" class="bulleted-list"><li style="list-style-type:disc">Reboot the RPi 400.</li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-8037-9ce0-f57dbbb72096" class="numbered-list" start="2"><li><strong>Installing VNC Viewer on Your Laptop</strong>:<ul id="1bfaecc5-ba04-80be-b316-c5fa5963e60e" class="bulleted-list"><li style="list-style-type:disc">Download and install VNC Viewer on your laptop from the official <a href="https://www.realvnc.com/en/connect/download/viewer/">VNC Viewer website</a>.</li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-803e-85c4-f1a6e00f9b89" class="numbered-list" start="3"><li><strong>Connecting to the Raspberry Pi</strong>:<ul id="1bfaecc5-ba04-804b-9a5f-e0f5bdea9add" class="bulleted-list"><li style="list-style-type:disc">Open VNC Viewer on your laptop.</li></ul><ul id="1bfaecc5-ba04-8040-a287-d3f198743cfe" class="bulleted-list"><li style="list-style-type:disc">Enter the IP address of your Raspberry Pi (same IP address as used with SSH) and press Enter.</li></ul><ul id="1bfaecc5-ba04-8058-917f-d2fe5e0f3c83" class="bulleted-list"><li style="list-style-type:disc">When prompted, enter the Raspberry Pi&#x27;s username and password.</li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-808b-b951-dc46e4537a00" class="numbered-list" start="4"><li><strong>Using Raspberry Pi Desktop Remotely</strong>:<ul id="1bfaecc5-ba04-8049-8f2c-cb474238de68" class="bulleted-list"><li style="list-style-type:disc">Once connected, you should see the Raspberry Pi&#x27;s desktop interface on your laptop.</li></ul><ul id="1bfaecc5-ba04-8075-ac12-f43a18a3ebc0" class="bulleted-list"><li style="list-style-type:disc">You can now use the Raspberry Pi as if you were working directly on it.</li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-801c-a1be-cc39aaf7102b" class="numbered-list" start="5"><li><strong>Update the RPi 400 System</strong>:<ul id="1bfaecc5-ba04-80ac-86aa-e8adde0d5269" class="bulleted-list"><li style="list-style-type:disc">Open Terminal.</li></ul><ul id="1bfaecc5-ba04-80cb-8354-eb89dd900c85" class="bulleted-list"><li style="list-style-type:disc">Update the system with the following commands:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8034-b005-dd5d776d59fd" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">sudo apt update
sudo apt upgrade</code></pre></li></ul></li></ol><h3 id="1bfaecc5-ba04-808b-8622-d8b577cd2316" class="">Section 3: Setting Up and Testing the Webcam</h3><ol type="1" id="1bfaecc5-ba04-8017-8ff5-e8bc1a54eed6" class="numbered-list" start="1"><li><strong>Connecting the Webcam</strong>:<ul id="1bfaecc5-ba04-801c-96a5-f0e94ba89edc" class="bulleted-list"><li style="list-style-type:disc">Plug the webcam into a USB port on the RPi 400.</li></ul><ul id="1bfaecc5-ba04-80df-b916-f3028a8b15f3" class="bulleted-list"><li style="list-style-type:disc">The drivers for the webcam should be automatically installed.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8018-a612-d796307170bb" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">lsusb</code></pre></li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-809c-8e05-c326ad06987c" class="numbered-list" start="2"><li><strong>Install Software to capture image</strong>:<ul id="1bfaecc5-ba04-8012-8ca2-e7e6e72fdfdc" class="bulleted-list"><li style="list-style-type:disc">Ensure that you have internet access.</li></ul><ul id="1bfaecc5-ba04-802d-8109-eafe3d2b0803" class="bulleted-list"><li style="list-style-type:disc">Install &#x27;fswebcam&#x27; for capturing images:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8079-8c2e-d8f74b006755" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">sudo apt install fswebcam</code></pre></li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-805d-a892-c039642df1f4" class="numbered-list" start="3"><li><strong>Capture Image with Webcam</strong>:<ul id="1bfaecc5-ba04-8053-be33-f7389ba59f6b" class="bulleted-list"><li style="list-style-type:disc">Use <code>fswebcam</code> to capture an image:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-800e-88d0-eb42dd279d18" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">fswebcam -r 1280x720 --no-banner image.jpg</code></pre></li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-803c-b61a-cf07fe8bac27" class="numbered-list" start="4"><li><strong>View and Edit Images (Optional)</strong>:<ul id="1bfaecc5-ba04-80ff-87ae-c71be64bd534" class="bulleted-list"><li style="list-style-type:disc">Install an image editor like GIMP:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-802d-968c-fbbed683d6de" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">sudo apt install gimp</code></pre></li></ul><ul id="1bfaecc5-ba04-8042-95d4-fcae90e4e868" class="bulleted-list"><li style="list-style-type:disc">Open the captured image in GIMP for editing.</li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-80ab-963a-d5bfac05474c" class="numbered-list" start="5"><li><strong>Recording Video</strong>:<ul id="1bfaecc5-ba04-8004-8d68-e6c9565ddda7" class="bulleted-list"><li style="list-style-type:disc">Install <code>ffmpeg</code> to record video from the webcam:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8020-8ad3-c446f37c125f" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">sudo apt install ffmpeg</code></pre></li></ul><ul id="1bfaecc5-ba04-8015-bce2-cdf0da02225d" class="bulleted-list"><li style="list-style-type:disc">Record a video from the webcam:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80cf-9cc2-fbcfb74ac907" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">ffmpeg -f v4l2 -framerate 25 -video_size 640x480 -i /dev/video0 output.mp4</code></pre></li></ul><ul id="1bfaecc5-ba04-8086-96c7-d3d07e7a8af5" class="bulleted-list"><li style="list-style-type:disc">this command tells ffmpeg to capture video from the first connected webcam at a resolution of 640x480 pixels and a frame rate of 25 fps, and save it to a file named output.mp4. In addition, &quot;-f&quot; specifies the format to be used while &quot;v4l2&quot; stands for Video4Linux2, which is the standard video driver model for Linux for video capture.</li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-80d4-8d82-df2ce0e2e079" class="numbered-list" start="6"><li><strong>Play Video (Optional)</strong>:<ul id="1bfaecc5-ba04-802c-b387-f3eec1b1832d" class="bulleted-list"><li style="list-style-type:disc">Install VLC media player:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8031-ada9-f61313fc87cd" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">sudo apt install vlc</code></pre></li></ul><ul id="1bfaecc5-ba04-802f-a4d6-fbfa75eb3d8d" class="bulleted-list"><li style="list-style-type:disc">Play the recorded video using VLC:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80b4-86cd-ef38db230b9e" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">vlc output.mp4</code></pre></li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-808a-8fca-f9629e92afe1" class="numbered-list" start="7"><li><strong>Checking for Audio Device</strong>:<ul id="1bfaecc5-ba04-80df-a8d8-f71def273e7f" class="bulleted-list"><li style="list-style-type:disc">First, check if your system has recognized the webcam&#x27;s microphone.</li></ul><ul id="1bfaecc5-ba04-8025-aac8-e2d37160ccbc" class="bulleted-list"><li style="list-style-type:disc">Open Terminal and type the following command to list all audio input devices:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8059-a59f-da5089a0a450" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">arecord -l</code></pre></li></ul><ul id="1bfaecc5-ba04-8003-bb96-c55189e91e71" class="bulleted-list"><li style="list-style-type:disc">Look for your webcam in the list. The following image shows what you might see.<figure id="1bfaecc5-ba04-8060-a9f2-e3436ecec3cb" class="image"><a href="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/ce795948-b92a-4486-a933-eab2fe27b0b5"><img src="https://github.com/drfuzzi/INF2009_Setup/assets/108112390/ce795948-b92a-4486-a933-eab2fe27b0b5"/></a></figure></li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-8089-be41-e0d15dee518f" class="numbered-list" start="8"><li><strong>Use the arecord Command</strong>:<ul id="1bfaecc5-ba04-8017-a6b8-db9d51c1df34" class="bulleted-list"><li style="list-style-type:disc">To test if the microphone is working, record a short audio clip:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80a8-a3e0-fa970df3cdbb" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">arecord -D plughw:&lt;CardNumber&gt;,&lt;DeviceNumber&gt; -d 10 test.wav</code></pre></li></ul><ul id="1bfaecc5-ba04-80f4-a3d0-f6b449966783" class="bulleted-list"><li style="list-style-type:disc">Replace <code>&lt;CardNumber&gt;</code> and <code>&lt;DeviceNumber&gt;</code> with the numbers you found in the previous step. The <code>d 10</code> flag tells <code>arecord</code> to record for 10 seconds.</li></ul><ul id="1bfaecc5-ba04-803b-808c-f13f0e6516c3" class="bulleted-list"><li style="list-style-type:disc">Based on the above screenshot, the USB audio device is listed as <code>card 2, device 0</code>, I specify these in the command.</li></ul><ul id="1bfaecc5-ba04-80f1-ae3e-d5cd72469016" class="bulleted-list"><li style="list-style-type:disc">To record a short audio clip, use the following command in the terminal:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8087-b980-d791173041b8" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">arecord -D plughw:2,0 -d 10 test.wav</code></pre></li></ul><ul id="1bfaecc5-ba04-8001-9957-ef535198b76b" class="bulleted-list"><li style="list-style-type:disc">Here&#x27;s what each part of the command means:<ul id="1bfaecc5-ba04-8076-bfa3-f153eddcd793" class="bulleted-list"><li style="list-style-type:circle"><code>arecord</code>: This is the command to start recording audio.</li></ul><ul id="1bfaecc5-ba04-80e3-9073-dccc70b2537e" class="bulleted-list"><li style="list-style-type:circle"><code>D plughw:2,0</code>: The <code>D</code> option specifies the audio device. <code>plughw:2,0</code> refers to <code>card 2, device 0</code>, which is your USB audio device.</li></ul><ul id="1bfaecc5-ba04-801d-a888-d3686531e3a7" class="bulleted-list"><li style="list-style-type:circle"><code>d 10</code>: This option specifies the duration of the recording. <code>d 10</code> means the recording will last for 10 seconds.</li></ul><ul id="1bfaecc5-ba04-806b-bbce-dfef18c7a1f3" class="bulleted-list"><li style="list-style-type:circle"><code>test.wav</code>: This is the name of the file where the audio will be saved. It will be saved in the current directory.</li></ul></li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-8086-a19b-c3af1cfffa86" class="numbered-list" start="9"><li><strong>Playing the Recorded Audio</strong>:<ul id="1bfaecc5-ba04-8083-9d70-c82c34a2c111" class="bulleted-list"><li style="list-style-type:disc">Play the recorded audio to check the quality:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8050-b7c8-de3834df8983" class="code"><code class="language-Bash" style="white-space:pre-wrap;word-break:break-all">aplay test.wav</code></pre></li></ul><ul id="1bfaecc5-ba04-80b3-9b90-fda76630e5f2" class="bulleted-list"><li style="list-style-type:disc">If you donâ€™t hear anything, adjust the microphone levels using the sound settings or <code>alsamixer</code> in the terminal.</li></ul></li></ol><h3 id="1bfaecc5-ba04-806d-b7ba-c68b5401c5cd" class="">Section 4: Advanced Applications (Optional)</h3><p id="1bfaecc5-ba04-8043-be08-c0161901d42d" class="">a. Setting up and using a Virtual Environment<br/>Using a Python virtual environment allows you to manage Python packages independently of the system packages. This is a recommended approach to avoid conflicts between packages installed via <br/><code>apt</code> and <code>pip</code>.</p><ol type="1" id="1bfaecc5-ba04-800b-917a-fb10252cfe34" class="numbered-list" start="1"><li><strong>Install Virtual Environment Package</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8066-a338-d85d07b8215b" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt install python3-venv</code></pre></li></ol><ol type="1" id="1bfaecc5-ba04-80e2-aef5-dec78f1ea8d6" class="numbered-list" start="2"><li><strong>Create a Virtual Environment</strong>:<br/>Navigate to the directory where you want to create your virtual environment and run:<br/><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8034-b2d7-de32f2674f5e" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">python3 -m venv myenv</code></pre></li></ol><ol type="1" id="1bfaecc5-ba04-80f4-9f49-df51453b011a" class="numbered-list" start="3"><li><strong>Activate the Virtual Environment</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8039-960f-f58940f9ce23" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">source myenv/bin/activate</code></pre></li></ol><ol type="1" id="1bfaecc5-ba04-804b-a354-c78b9d6cbd94" class="numbered-list" start="4"><li><strong>Install Packages Using pip</strong>:<br/>Now you can install packages using pip without encountering the <br/><code>externally-managed-environment</code> error:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-806e-b8e1-d04778f59684" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install opencv-python</code></pre></li></ol><ol type="1" id="1bfaecc5-ba04-80da-89ab-d47564a5b85c" class="numbered-list" start="5"><li><strong>Upgrade pip</strong>:<br/>Ensure you are using the latest version of pip:<br/><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8080-9a08-c4d219205981" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install --upgrade pip</code></pre></li></ol><p id="1bfaecc5-ba04-805c-be61-d445c4626693" class="">Remember, after using the virtual environment, you can deactivate it by simply running <code>deactivate</code> in your terminal. Using virtual environments is a good practice as it keeps your global Python environment clean and prevents version conflicts between different Python projects.</p><p id="1bfaecc5-ba04-802b-976a-c8042f712a2c" class="">b. <strong>Python Scripting for Webcam</strong>: <strong>(CODE 1)</strong></p><ul id="1bfaecc5-ba04-8047-8456-f274a3c4cf0c" class="bulleted-list"><li style="list-style-type:disc">Introduction to using Python for controlling the webcam.</li></ul><ul id="1bfaecc5-ba04-8008-b570-c031c7828889" class="bulleted-list"><li style="list-style-type:disc">A simple script to capture a single video frame from the webcam and save it.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80cd-bbd9-fe68561ea69e" class="code"><code class="language-Python">#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import cv2

# Initialize the webcam
cap = cv2.VideoCapture(0)

# Check if the webcam is opened correctly
if not cap.isOpened():
    raise IOError(&quot;Cannot open webcam&quot;)

# Capture one frame
ret, frame = cap.read()

# Save the captured image
cv2.imwrite(&#x27;captured_image.jpg&#x27;, frame)

# Release the webcam
cap.release()

print(&quot;Image captured and saved!&quot;)</code></pre></li></ul><p id="1bfaecc5-ba04-8031-b93a-e45c8826df95" class="">c. <strong>Mock Exercise - Creating a Surveillance System</strong>: (<strong>CODE 2)</strong></p><ul id="1bfaecc5-ba04-802e-8e07-ec02fae0029c" class="bulleted-list"><li style="list-style-type:disc">Setting up motion detection using the webcam.</li></ul><ul id="1bfaecc5-ba04-8080-b837-ccc4f2de728b" class="bulleted-list"><li style="list-style-type:disc">Another simple script to stream a video from the webcam and notify on screen if you detect motion using simple algorithm.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-804b-be98-c9f44a8b174f" class="code"><code class="language-Python">#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import cv2
import numpy as np

# Initialize the webcam
cap = cv2.VideoCapture(0)

# Read the first frame
_, frame1 = cap.read()
_, frame2 = cap.read()

while True:
    # Calculate the absolute difference between frames
    diff = cv2.absdiff(frame1, frame2)
    gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    _, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)
    dilated = cv2.dilate(thresh, None, iterations=3)
    contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    # Draw rectangles around detected contours
    for contour in contours:
        (x, y, w, h) = cv2.boundingRect(contour)

        if cv2.contourArea(contour) &lt; 900:
            continue
        cv2.rectangle(frame1, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(frame1, &quot;Status: {}&quot;.format(&#x27;Movement&#x27;), (10, 20), cv2.FONT_HERSHEY_SIMPLEX,
                    1, (0, 0, 255), 3)

    cv2.imshow(&quot;feed&quot;, frame1)
    frame1 = frame2
    _, frame2 = cap.read()

    # Break the loop if &#x27;q&#x27; is pressed
    if cv2.waitKey(40) == ord(&#x27;q&#x27;):
        break

cap.release()
cv2.destroyAllWindows()</code></pre></li></ul><h3 id="1bfaecc5-ba04-80ac-95e7-c64cc2f06c5c" class="">Section 5: Questions to think about</h3><ul id="1bfaecc5-ba04-80e8-9f20-f8df0d48d466" class="toggle"><li><details open=""><summary>1. Identify and explain the additional functionalities introduced in Code #2. How do these changes transform the program from a simple image capture to a movement detection system?</summary><p id="1bfaecc5-ba04-80f4-8043-e94dca41ee99" class=""><strong>Code #1</strong> simply captures and saves a single image using the webcam:</p><ul id="1bfaecc5-ba04-801a-b17a-cf125518be48" class="bulleted-list"><li style="list-style-type:disc">It opens the camera</li></ul><ul id="1bfaecc5-ba04-80f0-9309-f4c1eeae5d81" class="bulleted-list"><li style="list-style-type:disc">Takes one frame</li></ul><ul id="1bfaecc5-ba04-80d8-ab94-eef23eabdc58" class="bulleted-list"><li style="list-style-type:disc">Saves it as <code>captured_image.jpg</code></li></ul><p id="1bfaecc5-ba04-80af-a2e1-ef240ec41cb7" class=""><strong>Code #2</strong> transforms this into a <strong>real-time motion detection system</strong>:</p><ul id="1bfaecc5-ba04-80fc-8c9a-f3cc39534eed" class="bulleted-list"><li style="list-style-type:disc">Continuously reads video frames in a loop</li></ul><ul id="1bfaecc5-ba04-802b-bd42-f7156f91bdf7" class="bulleted-list"><li style="list-style-type:disc">Compares frames to detect changes (i.e., movement)</li></ul><ul id="1bfaecc5-ba04-8056-ba10-e190d69c9506" class="bulleted-list"><li style="list-style-type:disc">Uses image processing to isolate areas of motion</li></ul><ul id="1bfaecc5-ba04-8035-855d-ecd479d8889b" class="bulleted-list"><li style="list-style-type:disc">Highlights movement with rectangles and labels</li></ul><p id="1bfaecc5-ba04-808d-a03d-db917527acd2" class="">These additions turn the script from a <strong>static image capture</strong> tool into a <strong>dynamic surveillance-style motion detector</strong>.</p><hr id="1bfaecc5-ba04-804f-9d8a-eb30f3da2f44"/></details></li></ul><ul id="1bfaecc5-ba04-80ca-929b-d0a551a69ef5" class="toggle"><li><details open=""><summary>2. Several new OpenCV functions are used (like cv2.absdiff, cv2.cvtColor, cv2.GaussianBlur, cv2.threshold, cv2.dilate, and cv2.findContours). Research each of these functions and understand their role in processing the video frames for movement detection.</summary><p id="1bfaecc5-ba04-8092-8eb1-dae6d75de0da" class="">ðŸ”§ <strong>2. OpenCV Functions for Movement Detection</strong></p><h3 id="1bfaecc5-ba04-80b8-a178-ecab0aa08975" class="">ðŸŸ¡ <code>cv2.absdiff(frame1, frame2)</code></h3><ul id="1bfaecc5-ba04-80fa-898f-eebfaf810d0e" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: Highlights changes between two consecutive frames.</li></ul><ul id="1bfaecc5-ba04-80f6-b05b-c438d6f338ba" class="bulleted-list"><li style="list-style-type:disc"><strong>Effect</strong>: Detects motion by showing only <em>what has changed</em>.</li></ul><ul id="1bfaecc5-ba04-8029-b8bd-c867698a12b0" class="bulleted-list"><li style="list-style-type:disc"><strong>Snippet</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80bb-a78f-d3a4f6ab2b53" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">diff = cv2.absdiff(frame1, frame2)</code></pre></li></ul><ul id="1bfaecc5-ba04-80e8-b3bf-cf4e4b0d8de4" class="bulleted-list"><li style="list-style-type:disc"><strong>Key Point</strong>: Without this, we can&#x27;t detect <em>what</em> changed between frames.</li></ul><hr id="1bfaecc5-ba04-807a-84bb-e15cccbbee2d"/><h3 id="1bfaecc5-ba04-80be-9cff-c7d8d7b4d878" class="">âš« <code>cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)</code></h3><ul id="1bfaecc5-ba04-807e-8b64-d9d2a7f8a766" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: Converts the diff image to grayscale.</li></ul><ul id="1bfaecc5-ba04-80bd-abdb-fddd57601a89" class="bulleted-list"><li style="list-style-type:disc"><strong>Effect</strong>: Simplifies the image for faster processing.</li></ul><ul id="1bfaecc5-ba04-8074-b616-dcafe6133d59" class="bulleted-list"><li style="list-style-type:disc"><strong>Snippet</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8045-8125-ca976633d9b3" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)</code></pre></li></ul><ul id="1bfaecc5-ba04-80b5-9e95-e123437099db" class="bulleted-list"><li style="list-style-type:disc"><strong>Key Point</strong>: Grayscale reduces the complexity of image data (from 3 color channels to 1).</li></ul><hr id="1bfaecc5-ba04-80d8-b20c-c4c80817b188"/><h3 id="1bfaecc5-ba04-80ee-9c53-c86d58204dbb" class="">âšª <code>cv2.GaussianBlur(gray, (5, 5), 0)</code></h3><ul id="1bfaecc5-ba04-8064-b998-c9c85837c617" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: Smooths the image and reduces noise.</li></ul><ul id="1bfaecc5-ba04-80d3-832b-f5d1f93fc719" class="bulleted-list"><li style="list-style-type:disc"><strong>Effect</strong>: Helps avoid false detections from small variations (like flickering lights).</li></ul><ul id="1bfaecc5-ba04-80f9-acfc-d67cf6abb0bd" class="bulleted-list"><li style="list-style-type:disc"><strong>Snippet</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8090-aa48-c79bfe0cd044" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">blur = cv2.GaussianBlur(gray, (5, 5), 0)</code></pre></li></ul><ul id="1bfaecc5-ba04-80ab-806a-ea3a21484d79" class="bulleted-list"><li style="list-style-type:disc"><strong>Parameter</strong>: <strong>Kernel size (5,5)</strong> â€” <strong>larger = more smoothing, less sensitivity</strong></li></ul><blockquote id="1bfaecc5-ba04-80ad-8aa9-feb81a5d92d0" class="">ðŸ” Higher kernel size â†’ less sensitive to tiny motion<p id="1bfaecc5-ba04-80fb-b192-d2c56dcb3343" class="">ðŸ” <strong>Lower kernel size</strong> â†’ more sensitive, but may detect noise</p></blockquote><hr id="1bfaecc5-ba04-80be-9011-e442cd6bce56"/><h3 id="1bfaecc5-ba04-80c2-b399-df8b50a4e6dc" class="">âšª <code>cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)</code></h3><ul id="1bfaecc5-ba04-802b-b192-dd567025018e" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: Converts the blurred image to a binary image (black/white).</li></ul><ul id="1bfaecc5-ba04-803b-9eb7-f4fe2002dde5" class="bulleted-list"><li style="list-style-type:disc"><strong>Effect</strong>: Isolates motion areas as white, background as black.</li></ul><ul id="1bfaecc5-ba04-80b2-8556-e64bef186d41" class="bulleted-list"><li style="list-style-type:disc"><strong>Snippet</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8091-9193-ff82da55b593" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">_, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)</code></pre></li></ul><ul id="1bfaecc5-ba04-8098-8885-facf15e903dc" class="bulleted-list"><li style="list-style-type:disc"><strong>Parameter</strong>: <strong>Threshold value (20)</strong> â€” sets the cutoff for motion detection</li></ul><blockquote id="1bfaecc5-ba04-804c-82db-e6dcecfbe661" class="">ðŸ” Higher threshold â†’ ignores small differences (less sensitive)<p id="1bfaecc5-ba04-806d-a6ed-eedb58d2573c" class="">ðŸ” <strong>Lower threshold</strong> â†’ picks up even minor motion (more sensitive)</p></blockquote><hr id="1bfaecc5-ba04-80ac-a844-c546a6b099ad"/><h3 id="1bfaecc5-ba04-80c0-9080-cb34a4a47496" class="">âšª <code>cv2.dilate(thresh, None, iterations=3)</code></h3><ul id="1bfaecc5-ba04-807f-a16c-cdbd897b7432" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: Expands the white regions (motion areas).</li></ul><ul id="1bfaecc5-ba04-80c7-9531-e1f516a9537b" class="bulleted-list"><li style="list-style-type:disc"><strong>Effect</strong>: Fills in holes, makes contours more complete.</li></ul><ul id="1bfaecc5-ba04-80a7-8729-e7d01c0e93e7" class="bulleted-list"><li style="list-style-type:disc"><strong>Snippet</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8046-b905-d978958f24de" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">dilated = cv2.dilate(thresh, None, iterations=3)
</code></pre></li></ul><ul id="1bfaecc5-ba04-80f5-9e20-f2844a96365e" class="bulleted-list"><li style="list-style-type:disc"><strong>Parameter</strong>: <strong>Iterations = 3</strong> â€” how much to expand the white area</li></ul><blockquote id="1bfaecc5-ba04-8014-8e2d-d1b1e962ec9d" class="">ðŸ” More iterations â†’ more robust detection, but might merge small areas<p id="1bfaecc5-ba04-8059-b21a-c9161a0e48a4" class="">ðŸ” <strong>Fewer iterations</strong> â†’ sharper regions, but may miss weak contours</p></blockquote><hr id="1bfaecc5-ba04-80ba-93e3-c6b71c783d85"/><h3 id="1bfaecc5-ba04-80d4-ac97-f18a4840c98f" class="">âš« <code>cv2.findContours(...)</code></h3><ul id="1bfaecc5-ba04-809d-9db6-eb05b89775a2" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: Detects outlines (contours) of white regions.</li></ul><ul id="1bfaecc5-ba04-80ad-83b4-c32ddf40f5fc" class="bulleted-list"><li style="list-style-type:disc"><strong>Effect</strong>: Allows us to locate and draw rectangles around motion.</li></ul><ul id="1bfaecc5-ba04-80a6-b9ca-c1abaa540f79" class="bulleted-list"><li style="list-style-type:disc"><strong>Snippet</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8078-a2f9-f9ea1583226a" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
</code></pre></li></ul><ul id="1bfaecc5-ba04-80eb-b958-d4e855a97ba3" class="bulleted-list"><li style="list-style-type:disc"><strong>Post-process</strong>: Each contourâ€™s area is checked:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-800b-a97e-c5c4bc89a818" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">if cv2.contourArea(contour) &lt; 900:
    continue
</code></pre></li></ul><blockquote id="1bfaecc5-ba04-8022-932b-c56d038ba97d" class="">ðŸ” Higher contour area threshold (e.g., 900) â†’ ignores small objects like bugs or light changes<p id="1bfaecc5-ba04-8027-be79-d475278bb8b8" class="">ðŸ” <strong>Lower value</strong> â†’ more sensitivity to small movements</p></blockquote><hr id="1bfaecc5-ba04-8072-9d58-d7e163da1de0"/></details></li></ul><ul id="1bfaecc5-ba04-8085-97b0-f302029226ab" class="toggle"><li><details open=""><summary>3. The program uses specific conditions (such as contour area) to decide when to draw rectangles and indicate movement. Experiment with these parameters to see how they affect the accuracy and sensitivity of movement detection.</summary><p id="1bfaecc5-ba04-8026-9efa-cbdd1cde0767" class="">ðŸŽ¯ <strong>What does this do?</strong></p><p id="1bfaecc5-ba04-8046-9ff1-d3d68cec756d" class="">This condition filters out small contours (moving areas) by checking their <strong>area in pixels</strong>.</p><ul id="1bfaecc5-ba04-80e1-9de3-f6a3623a2ee5" class="bulleted-list"><li style="list-style-type:disc">If the area is <strong>less than 900</strong>, the program <strong>ignores</strong> that contour â€” assuming it&#x27;s just noise, a shadow, or very minor movement.</li></ul><ul id="1bfaecc5-ba04-80b3-be2d-cea72e544363" class="bulleted-list"><li style="list-style-type:disc">Only <strong>larger movements</strong> trigger a rectangle and status message.</li></ul><hr id="1bfaecc5-ba04-8068-af5c-dde05bdf20eb"/><h3 id="1bfaecc5-ba04-80a8-9c47-eac55e0a82d7" class="">ðŸ“Š <strong>How does changing this number affect accuracy and sensitivity?</strong></h3><table id="1bfaecc5-ba04-80fa-8ca2-c61dbba7bf41" class="simple-table"><thead class="simple-table-header"><tr id="1bfaecc5-ba04-806f-9901-f570b900aecb"><th id="m|z~" class="simple-table-header-color simple-table-header"><strong>Area Threshold</strong></th><th id="?XEd" class="simple-table-header-color simple-table-header"><strong>Effect on Detection</strong></th><th id="PJYY" class="simple-table-header-color simple-table-header"><strong>Sensitivity</strong></th><th id="B[_}" class="simple-table-header-color simple-table-header"><strong>Use Case Example</strong></th></tr></thead><tbody><tr id="1bfaecc5-ba04-8076-89ef-ff3bd8f0b705"><td id="m|z~" class=""><code>LOW</code> (e.g., <code>300</code>)</td><td id="?XEd" class="">Detects even small movements (like hand gestures or small objects)</td><td id="PJYY" class="">ðŸ”¼ High</td><td id="B[_}" class="">Indoor automation, gesture control</td></tr><tr id="1bfaecc5-ba04-805d-be6c-dd154659f208"><td id="m|z~" class=""><code>MEDIUM</code> (e.g., <code>900</code>)</td><td id="?XEd" class="">Ignores minor background noise, focuses on moderate movement</td><td id="PJYY" class="">âš–ï¸ Balanced</td><td id="B[_}" class="">Home security, general motion detection</td></tr><tr id="1bfaecc5-ba04-80f9-bab9-d162a2411b8e"><td id="m|z~" class=""><code>HIGH</code> (e.g., <code>2000</code>)</td><td id="?XEd" class="">Only detects large movements (e.g., a person walking by)</td><td id="PJYY" class="">ðŸ”½ Low</td><td id="B[_}" class="">Outdoor use, animal/bird filtering</td></tr></tbody></table><hr id="1bfaecc5-ba04-80bc-84fe-d9a808654547"/></details></li></ul><ul id="1bfaecc5-ba04-801c-b0bb-d30408117d23" class="toggle"><li><details open=""><summary>4. Loop Mechanics and Video Processing: Analyze the role of the while loop in the 2nd Code for continuous video capture and processing. How does this looping mechanism differ from the single capture approach in the 1st Code, especially in terms of real-time processing and movement detection?</summary><h3 id="1bfaecc5-ba04-8081-be89-e6208d7fd906" class="">ðŸ”„ <strong>4. Loop Mechanics and Real-time Processing</strong></h3><p id="1bfaecc5-ba04-80e1-a342-c22366b2e2e1" class="">In <strong>Code #2</strong>, the <code>while True:</code> loop enables <strong>continuous capture and processing</strong> of frames from the webcam:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-800f-a8b9-d2e11386d531" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">while True:
    ret, frame = cap.read()
    # Process the frame
</code></pre><p id="1bfaecc5-ba04-807f-ba85-d278609d7478" class="">This is critical for:</p><ul id="1bfaecc5-ba04-8014-8ca2-c9302ee364f5" class="bulleted-list"><li style="list-style-type:disc">Detecting <strong>movement over time</strong></li></ul><ul id="1bfaecc5-ba04-8077-b352-e9813604f8f0" class="bulleted-list"><li style="list-style-type:disc">Keeping the system responsive</li></ul><ul id="1bfaecc5-ba04-8010-b018-eb7d83f69b38" class="bulleted-list"><li style="list-style-type:disc">Providing <strong>live feedback</strong> to the user</li></ul><p id="1bfaecc5-ba04-805b-a595-fe74ab42ce00" class=""><strong>Code #1</strong>, in contrast, just runs once to take a snapshot. Itâ€™s not suitable for dynamic tasks like surveillance or gesture detection.</p></details></li></ul><ul id="1bfaecc5-ba04-8016-838a-e163f246f854" class="toggle"><li><details open=""><summary>5. Consider aspects like improving the accuracy of movement detection, optimizing performance, or adding new features (like recording video when movement is detected).</summary><h3 id="1bfaecc5-ba04-80bf-8276-d8eb16f7c0b2" class="">ðŸš€ <strong>5. Ideas for Improvement</strong></h3><p id="1bfaecc5-ba04-801d-a8d2-d40f4c72622d" class="">You can level up the system with these features:</p><ul id="1bfaecc5-ba04-802e-a154-fca48a7f78a6" class="bulleted-list"><li style="list-style-type:disc"><strong>Record video only when motion is detected</strong><p id="1bfaecc5-ba04-801f-b71d-e94337a5177d" class="">Use <code>cv2.VideoWriter</code> to start recording when movement is first detected and stop when it ceases.</p></li></ul><ul id="1bfaecc5-ba04-80e2-9cfa-d2ce03c0a74d" class="bulleted-list"><li style="list-style-type:disc"><strong>Add timestamp or logging</strong><p id="1bfaecc5-ba04-8011-b04a-cf651e9da25c" class="">Print/log timestamps when movement is detected â€” useful for review.</p></li></ul><ul id="1bfaecc5-ba04-8098-a251-f355fca45acd" class="bulleted-list"><li style="list-style-type:disc"><strong>Sensitivity slider or config file</strong><p id="1bfaecc5-ba04-80a1-bc1c-fa3efe040cf3" class="">Let users adjust the contour area threshold dynamically (e.g., using a config file or UI slider).</p></li></ul><ul id="1bfaecc5-ba04-8098-ab68-e52e88dbb51d" class="bulleted-list"><li style="list-style-type:disc"><strong>Email or push notification on motion</strong><p id="1bfaecc5-ba04-8047-8797-fadcc0b04f7c" class="">Integrate with MQTT, email (like <code>smtplib</code>), or Telegram bots for real alerts.</p></li></ul><ul id="1bfaecc5-ba04-80c3-a45b-c849539e8014" class="bulleted-list"><li style="list-style-type:disc"><strong>Optimize for performance</strong><ul id="1bfaecc5-ba04-8080-8713-e55ca805aa10" class="bulleted-list"><li style="list-style-type:circle">Resize frames to smaller dimensions</li></ul><ul id="1bfaecc5-ba04-8058-bc93-c22dc0078a04" class="bulleted-list"><li style="list-style-type:circle">Reduce frame rate (e.g., skip every other frame)</li></ul></li></ul><h3 id="1bfaecc5-ba04-8009-a81e-e68a1dec0eba" class="">âœ… <strong>Accuracy</strong></h3><ul id="1bfaecc5-ba04-80d6-ab2b-ec2f7d54d4ec" class="bulleted-list"><li style="list-style-type:disc">Tune blur kernel size and threshold for best sensitivity</li></ul><ul id="1bfaecc5-ba04-8081-9237-f2f1dd90a3a7" class="bulleted-list"><li style="list-style-type:disc">Use <code>cv2.accumulateWeighted()</code> for background subtraction (better long-term tracking)</li></ul><h3 id="1bfaecc5-ba04-8060-aa32-ee69fad0e1b4" class="">âœ… <strong>Performance</strong></h3><ul id="1bfaecc5-ba04-80b2-8b7e-f2153578eadd" class="bulleted-list"><li style="list-style-type:disc">Resize frame: <code>frame = cv2.resize(frame, (640, 480))</code> â€” reduces processing load</li></ul><ul id="1bfaecc5-ba04-8043-b3dc-e71cf3828792" class="bulleted-list"><li style="list-style-type:disc">Skip alternate frames for speed</li></ul><h3 id="1bfaecc5-ba04-8092-9ccc-d1b4ec7a16f0" class="">âœ… <strong>Features</strong></h3><ul id="1bfaecc5-ba04-8021-9b42-daf19992600f" class="bulleted-list"><li style="list-style-type:disc">Add video recording when motion is detected (<code>cv2.VideoWriter</code>)</li></ul><ul id="1bfaecc5-ba04-800e-b83d-c8e78c743ff0" class="bulleted-list"><li style="list-style-type:disc">Add a log/timestamp (<code>datetime.now()</code>)</li></ul><ul id="1bfaecc5-ba04-8069-b356-ee23d206d44f" class="bulleted-list"><li style="list-style-type:disc">Email/SMS notifications using <code>smtplib</code> or <code>requests</code></li></ul></details></li></ul><h3 id="1bfaecc5-ba04-8064-94d3-c02dc93c729b" class="">âœ… Summary Table</h3><table id="1bfaecc5-ba04-8025-8c8d-ef4828cab612" class="simple-table"><tbody><tr id="1bfaecc5-ba04-80a1-a5f3-fab3e23955dd"><td id="@xQ{" class="">Feature</td><td id="uX;_" class="">Code #1</td><td id="YzlC" class="">Code #2</td></tr><tr id="1bfaecc5-ba04-80c1-a802-ccf2965ee111"><td id="@xQ{" class="">Image Capture</td><td id="uX;_" class="">âœ…</td><td id="YzlC" class="">âœ…</td></tr><tr id="1bfaecc5-ba04-8020-9611-c0ae97e92c46"><td id="@xQ{" class="">Continuous Video</td><td id="uX;_" class="">âŒ</td><td id="YzlC" class="">âœ…</td></tr><tr id="1bfaecc5-ba04-8031-a8ea-ef7fafc75d13"><td id="@xQ{" class="">Movement Detection</td><td id="uX;_" class="">âŒ</td><td id="YzlC" class="">âœ…</td></tr><tr id="1bfaecc5-ba04-80c8-8149-db4cda27c28e"><td id="@xQ{" class="">OpenCV Processing</td><td id="uX;_" class="">âŒ</td><td id="YzlC" class="">âœ…</td></tr><tr id="1bfaecc5-ba04-8032-a648-c28d49ccd538"><td id="@xQ{" class="">Real-time Feedback</td><td id="uX;_" class="">âŒ</td><td id="YzlC" class="">âœ…</td></tr><tr id="1bfaecc5-ba04-80f9-8103-faec07f786c1"><td id="@xQ{" class="">Potential for Alerts &amp; Recording</td><td id="uX;_" class="">âŒ</td><td id="YzlC" class="">âœ…</td></tr></tbody></table><h3 id="1bfaecc5-ba04-80a0-bfb9-ea2cb2cb3080" class="">Additional Resources</h3><ul id="1bfaecc5-ba04-80a7-b14e-e5a5a4a3d182" class="bulleted-list"><li style="list-style-type:disc">Raspberry Pi Documentation: <a href="https://www.raspberrypi.org/documentation/">Official Raspberry Pi Documentation</a></li></ul><ul id="1bfaecc5-ba04-8029-b04b-e55b2fdf95f0" class="bulleted-list"><li style="list-style-type:disc">Python Programming for Raspberry Pi: <a href="https://www.raspberrypi.org/documentation/usage/python/">Python Programming on Raspberry Pi</a></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Week 2 Sound Analytics</summary><div class="indented"><p id="b20d0e85-69b1-41bd-8f34-35b0d6f56e7a" class=""><strong>Sound Analytics with Raspberry Pi 4/3 using Microphone</strong></p><p id="e934a6ca-6a09-4659-9c51-ad6a0c93098e" class=""><strong>Objective:</strong> By the end of this session, participants will understand how to set up a microphone with the Raspberry Pi, capture audio, and conduct basic sound analytics.</p><hr id="78649b5d-e2d4-42bd-a66d-106e8ef60de3"/><p id="687783b2-af7f-400e-826a-610048cd1a25" class=""><strong>Prerequisites:</strong></p><ol type="1" id="5ac55031-c8cd-4abf-9220-624c61567f7b" class="numbered-list" start="1"><li>Raspberry Pi with Raspbian OS installed.</li></ol><ol type="1" id="12c7740f-7a3f-4bce-9464-d7cb1c5b11c2" class="numbered-list" start="2"><li>MicroSD card (16GB or more recommended).</li></ol><ol type="1" id="07bd6254-5047-40d7-9556-4486a7903e14" class="numbered-list" start="3"><li>USB keyboard, mouse, and monitor. You can also use Real VNC to access the Rasperry Pi (Need to enable VNC configuration in your Pi)</li></ol><ol type="1" id="99a444e0-50a7-47e4-9a2e-91f77794523b" class="numbered-list" start="4"><li>USB microphone compatible with Raspberry Pi.</li></ol><ol type="1" id="d2c8b4f9-a5d8-4978-9574-4f94492003e0" class="numbered-list" start="5"><li>(Optional) A Speaker/USB headset to hear the playback</li></ol><ol type="1" id="f8830db0-1fc7-4d9e-9a5d-c832956436e1" class="numbered-list" start="6"><li>Internet connectivity (Wi-Fi or Ethernet).</li></ol><ol type="1" id="95a2d8f8-8c19-4190-9bc4-24148b7148dc" class="numbered-list" start="7"><li>Basic knowledge of Python and Linux commands.</li></ol><hr id="e857edbc-e930-43d4-b3e6-2412aa090136"/><p id="e532af8f-735d-4492-a0c3-6a1b8cd7a9ad" class=""><strong>1. Introduction (10 minutes)</strong></p><p id="cc26973b-2a79-46fd-aecb-9b0f9b33bf01" class="">Developing a computer (ideally embedded) aided audio listening system similar to the human hearing to make sense of sounds is one of the growing areas of research. There are various application to the same not limited to 1) voice enabled services (e.g. Alexa Dot), 2) healthcare applications (e.g. lung sound analysis /  digital stethoscope) and 3) audio chatbots (e.g. IVR services). In this lab, we will be working on ways to capture audio data, analyze it and visualize the same on Raspberry Pi. A basic approach for the same is as shown below:</p><figure id="e79c2321-1f6a-467e-ba4c-c8aefbdcf948" class="image"><a href="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/bb9a2d8a-b4ae-4207-8272-21162987c821"><img style="width:480px" src="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/bb9a2d8a-b4ae-4207-8272-21162987c821"/></a></figure><p id="13d4d9ad-fc52-4927-b8e4-cf31b75d06c1" class=""><strong>2. Setting up the Raspberry Pi (10 minutes)</strong></p><ul id="f21c1c84-f76a-418e-a387-a72ee5d0e233" class="bulleted-list"><li style="list-style-type:disc">Booting up the Raspberry Pi.</li></ul><ul id="f27e74c2-fb30-4535-9112-40c9e0c4a6f6" class="bulleted-list"><li style="list-style-type:disc">Setting up Wi-Fi/Ethernet.</li></ul><ul id="b0b44079-0b0e-4ef1-a789-47e5fa5e94bb" class="bulleted-list"><li style="list-style-type:disc">System updates:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="845bf667-ff73-434c-b851-a5df30a6a005" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt update
sudo apt upgrade</code></pre></li></ul><ul id="159bcb78-58d2-446f-bd83-923c4f7b995c" class="bulleted-list"><li style="list-style-type:disc"><strong>[Important!] Set up and activate a virtual environment named &quot;audio&quot; for this experiment (to avoid conflicts in libraries) as below</strong><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="9e781fec-259e-4465-acb7-e12b870dab62" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt install python3-venv
python3 -m venv audio
source audio/bin/activate</code></pre></li></ul><p id="f12eebd8-adba-41b4-910e-4d7ae2c1e2f1" class=""><strong>3. Connecting and Testing the Microphone (15 minutes)</strong></p><ul id="e1ac632e-cdb3-4e9c-99e2-e70ce3464e20" class="bulleted-list"><li style="list-style-type:disc">Physically connecting the microphone to the Raspberry Pi.</li></ul><ul id="8429b30b-4518-4e31-ad7c-5bbdec90ef73" class="bulleted-list"><li style="list-style-type:disc">Testing the microphone:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="47c7089e-78bc-4816-82d7-8bccf6f355e8" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">arecord --duration=10 test.wav
aplay test.wav</code></pre></li></ul><ul id="406822c6-2ab3-47b1-bed7-768f4f9e2c6b" class="bulleted-list"><li style="list-style-type:disc">Don&#x27;t delete the .wav file as that will be used later for feature extraction</li></ul><p id="b7777657-c0f0-4797-87bf-45f976771359" class=""><strong>4. Introduction to Sound Processing with Python (20 minutes)</strong></p><ul id="b9cd0332-1fad-40b0-967b-16942fd9f4b1" class="bulleted-list"><li style="list-style-type:disc">Installing necessary Python libraries (may need additional libraries if not preinstalled with default python):<strong>Note that you need either pyaudio or sounddevice to record the audio stream from microphone</strong><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="17ff0f52-0ee9-4306-89d1-a9d0dfc33fa3" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt install portaudio19-dev
pip3 install pyaudio
pip3 install sounddevice
pip3 install scipy matplotlib</code></pre></li></ul><ul id="9d7ae40a-512b-4283-97dd-80eaf4244989" class="bulleted-list"><li style="list-style-type:disc">Capturing audio in Python.</li></ul><ul id="c7a834e4-2167-4a4c-b182-f18a2f4621b8" class="bulleted-list"><li style="list-style-type:disc">Fourier Transform: Understanding frequency components of sound.</li></ul><ul id="cfcae4e6-7de4-446c-a074-701cfbda0a45" class="bulleted-list"><li style="list-style-type:disc">Visualizing sound waves (both the wave itself and the audio spectrum).<ul id="b861363e-16de-431b-b494-7d388ac819a7" class="bulleted-list"><li style="list-style-type:circle">For above tasks, you can use the<ul id="f7a58d26-9298-432a-a63a-ece6e0444488" class="bulleted-list"><li style="list-style-type:square">sample code below if you are using <em>pyaudio</em>.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80c8-b930-e220365e3025" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script captures live audio from the microphone, processes it in real-time, and visualizes both the time-domain waveform and its frequency-domain spectrum.
It uses PyAudio for audio capture, performs a Fast Fourier Transform (FFT) to analyze the audio frequencies, and updates plots using Matplotlib.
This type of code is useful for audio signal analysis, debugging audio hardware, or as a foundation for more advanced audio processing tasks.
&quot;&quot;&quot;

#%% Import the required libraries
import pyaudio  # For interfacing with the audio hardware (microphone input and audio output).
                # Q: Why use PyAudio? 
                # A: It provides a convenient interface to capture and play back audio data in Python.
import struct   # For converting raw audio bytes into numerical 16-bit integers.
                # Q: What is the purpose of the struct module?
                # A: It helps convert binary data (from the microphone) into a format that can be processed (integers).
import numpy as np  # For efficient numerical and array operations.
import matplotlib.pyplot as plt  # For plotting the audio waveform and spectrum.
from scipy.fftpack import fft, fftfreq  # For computing the FFT (transforms time-domain data to frequency-domain).
                # Q: What does FFT do?
                # A: It converts a signal from the time domain to the frequency domain, revealing its frequency components.
import time  # For measuring the execution time of operations (like the FFT computation).

#%% Parameters
# Define constants for audio processing and visualization
BUFFER = 1024 * 16           # Number of samples per frame. A larger buffer can provide higher frequency resolution.
                             # Q: Why is the buffer size important?
                             # A: It determines the resolution of both the time and frequency analysis.
FORMAT = pyaudio.paInt16     # Audio format, indicating that audio samples are 16-bit integers.
                             # Q: What does paInt16 indicate?
                             # A: It specifies that each audio sample is a 16-bit number.
CHANNELS = 1                 # Number of audio channels; 1 means mono audio.
                             # Q: What would be the effect of using 2 channels?
                             # A: It would capture stereo audio instead of mono.
RATE = 44100                 # Sampling rate in samples per second (44.1 kHz is standard for high-quality audio).
                             # Q: Why choose a sampling rate of 44100 Hz?
                             # A: It is a standard that provides a good balance between audio quality and processing requirements.
RECORD_SECONDS = 30          # Total duration (in seconds) to record audio from the microphone.

#%% Create matplotlib figure and axes with initial random plots as placeholders
# Set up a figure with two subplots: one for the waveform (time domain) and one for the spectrum (frequency domain).
fig, (ax1, ax2) = plt.subplots(2, figsize=(7, 7))

# Generate x-axis data for the waveform plot.
x = np.arange(0, 2 * BUFFER, 2)  # Sample indices for the waveform.
                # Q: Why is the x-axis generated with a step of 2?
                # A: It matches the length of the unpacked data array from the binary stream.
                
# Generate frequency bins for the FFT output.
xf = fftfreq(BUFFER, (1 / RATE))[:BUFFER // 2]
                # Q: What is fftfreq used for?
                # A: It computes the frequency values corresponding to the FFT bins.
                # Note: Only the first half of the FFT is used because the FFT of a real-valued signal is symmetric.

# Create initial line objects for the waveform and FFT plots using random data as placeholders.
line, = ax1.plot(x, np.random.rand(BUFFER), &#x27;-&#x27;, lw=2)
line_fft, = ax2.plot(xf, np.random.rand(BUFFER // 2), &#x27;-&#x27;, lw=2)

# Format the waveform (time-domain) plot.
ax1.set_title(&#x27;AUDIO WAVEFORM&#x27;)
ax1.set_xlabel(&#x27;Samples&#x27;)
ax1.set_ylabel(&#x27;Amplitude (volume)&#x27;)
ax1.set_ylim(-5000, 5000)  # Adjust y-axis limits to view the typical range of microphone signal amplitudes.
ax1.set_xlim(0, BUFFER)

# Format the frequency spectrum plot.
ax2.set_title(&#x27;SPECTRUM&#x27;)
ax2.set_xlabel(&#x27;Frequency (Hz)&#x27;)
ax2.set_ylabel(&#x27;Magnitude (Log Scale)&#x27;)
ax2.set_ylim(0, 1000) 
ax2.set_xlim(0, RATE / 2)  # x-axis goes up to the Nyquist frequency (half the sampling rate).

# Display the plot in non-blocking mode so that the script continues to run.
plt.show(block=False)

#%% Initialize the pyaudio class instance
# Create a PyAudio object to interface with the audio hardware.
audio = pyaudio.PyAudio()

# Open an audio stream to capture data from the microphone.
stream = audio.open(
    format=FORMAT,           # Audio sample format.
    channels=CHANNELS,       # Number of channels (mono in this case).
    rate=RATE,               # Sampling rate.
    input=True,              # Enable input (recording).
    output=True,             # Enable output (if needed).
    frames_per_buffer=BUFFER # Number of samples per frame.
)
print(&#x27;stream started&#x27;)  # Indicate that audio capturing has begun.

#%% Main loop: Capture audio, compute FFT, and update the plots in real-time.
exec_time = []  # List to store the execution time of each FFT computation.

# Calculate the total number of frames to capture based on the recording duration and buffer size.
for _ in range(0, RATE // BUFFER * RECORD_SECONDS):
    
    # Read a chunk of binary audio data from the microphone.
    data = stream.read(BUFFER)
    
    # Convert the binary data to a tuple of 16-bit integers.
    # The format string (e.g., &#x27;16384h&#x27; for a BUFFER of 16384 samples) tells struct.unpack how many integers to extract.
    data_int = struct.unpack(str(BUFFER) + &#x27;h&#x27;, data)
                # Q: Why do we need to convert binary data to integers?
                # A: To be able to perform numerical analysis (such as FFT) on the audio data.
    
    # Compute the FFT of the audio data.
    start_time = time.time()  # Start timing the FFT computation.
    yf = fft(data_int)        # Compute the Fast Fourier Transform.
    exec_time.append(time.time() - start_time)  # Record the time taken for the FFT.
    
    # Update the waveform plot with the new audio data.
    line.set_ydata(data_int)
    
    # Update the FFT plot:
    # Normalize the FFT data by scaling with 2.0/BUFFER and take the absolute value (magnitude).
    # Only the first half of the FFT is used since the second half is symmetric.
    line_fft.set_ydata(2.0 / BUFFER * np.abs(yf[0:BUFFER // 2]))
                # Q: Why normalize the FFT output?
                # A: Normalization scales the FFT data to reflect the actual amplitude of the frequency components.
    
    # Refresh the plots with the updated data.
    fig.canvas.draw()
    fig.canvas.flush_events()

#%% Cleanup: Terminate the audio stream and report execution time.
audio.terminate()  # Close the audio stream and release resources.
   
print(&#x27;stream stopped&#x27;)
# Print the average execution time for the FFT computation, in milliseconds.
print(&#x27;average execution time = {:.0f} milli seconds&#x27;.format(np.mean(exec_time) * 1000))
                # Q: Why measure the FFT execution time?
                # A: It helps determine if the processing is fast enough for real</code></pre><ul id="81d1e228-38e5-45fc-8b1b-62ae3fbdcbe1" class="bulleted-list"><li style="list-style-type:square">sample code if you are using <em>sounddevice</em>.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-808e-a5c0-d26eb0210cb3" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#%% Import the required libraries
import sounddevice as sd # Refer to https://python-sounddevice.readthedocs.io/en/0.4.6/
import numpy as np 
import matplotlib.pyplot as plt
import time

#%% Parameters
BUFFER = 1024 * 16           # samples per frame (you can change the same to acquire more or less samples)
CHANNELS = 1                 # single channel for microphone
RATE = 44100                 # samples per second
RECORD_SECONDS = 30          # Specify the time to record from the microphone in seconds

#%% create matplotlib figure and axes with initial random plots as placeholder
fig, (ax1, ax2) = plt.subplots(2, figsize=(4, 4))
# create a line object with random data
x = np.arange(0, 2*BUFFER, 2)       # samples (waveform)
xf = np.fft.fftfreq(BUFFER,1/RATE)[:BUFFER//2]
line, = ax1.plot(x,np.random.rand(BUFFER), &#x27;-&#x27;, lw=2)
line_fft, = ax2.plot(xf,np.random.rand(BUFFER//2), &#x27;-&#x27;, lw=2)

# basic formatting for the axes
ax1.set_title(&#x27;AUDIO WAVEFORM&#x27;)
ax1.set_xlabel(&#x27;samples&#x27;)
ax1.set_ylabel(&#x27;volume&#x27;)
ax1.set_ylim(-5000, 5000) # change this to see more amplitude values (when we speak)
ax1.set_xlim(0, BUFFER)

ax2.set_title(&#x27;SPECTRUM&#x27;)
ax2.set_xlabel(&#x27;Frequency&#x27;)
ax2.set_ylabel(&#x27;Log Magnitude&#x27;)
ax2.set_ylim(0, 1000) 
ax2.set_xlim(0, RATE/2)

# Do not show the plot yet
plt.show(block=False)

#%% Reconrding the sound and constructing the spectrum
exec_time = []
for _ in range(0, RATE // BUFFER * RECORD_SECONDS):   
       
    # Record the sound in int16 format and wait till recording is done
    data = sd.rec(frames=BUFFER,samplerate=RATE,channels=CHANNELS,dtype=&#x27;int16&#x27;,blocking=True)
    data = np.squeeze(data)  
    
    # compute FFT    
    start_time=time.time()  # for measuring frame rate
    fft_data = np.fft.fft(data)
    fft_data = np.abs(fft_data[:BUFFER//2])
    
    # calculate time of execution of FFT
    exec_time.append(time.time() - start_time)
    
    #update line plots for both axes
    line.set_ydata(data)
    line_fft.set_ydata(fft_data)
    line_fft.set_ydata(2.0/BUFFER * fft_data)
    fig.canvas.draw()
    fig.canvas.flush_events()

  
print(&#x27;stream stopped&#x27;)
print(&#x27;average execution time = {:.0f} milli seconds&#x27;.format(np.mean(exec_time)*1000))</code></pre></li></ul><ul id="2773210a-828e-4c59-889d-e72cb27fa632" class="bulleted-list"><li style="list-style-type:circle">A sample captured speech and its spectrum are shown below<figure id="82422517-a370-4af3-9eaf-c0e1177de0e8" class="image"><a href="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/26449854-8770-46a7-ac2d-de94f8f2bc7a"><img src="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/26449854-8770-46a7-ac2d-de94f8f2bc7a"/></a></figure></li></ul><ul id="3e4123ef-de70-4020-b784-416674d6176c" class="bulleted-list"><li style="list-style-type:circle">The top plot shows a sample time series of the captured audio and the bottom plot shows the frequency components present in the time series. It is easier to intepret audio by its spectrum.</li></ul><ul id="07260b34-8d3c-44a5-bdaf-5dd646886b8a" class="bulleted-list"><li style="list-style-type:circle">Try speaking, making different sounds and observe how the spectrum changes.</li></ul></li></ul><p id="f008a2e7-693f-4a6c-afae-1eb07b97df5d" class=""><strong>5. Basic Sound Analytics (40 minutes)</strong></p><ul id="960817c7-cada-47dd-a8eb-9219ed2e5e79" class="bulleted-list"><li style="list-style-type:disc">Filtering: Removing noise or specific frequencies. The below code illustrates a bandpass filter (only passes audio within a certain frequencies as decided by the user are kept).</li></ul><ul id="36f919db-8794-46ab-b7c0-08f637a38e4d" class="bulleted-list"><li style="list-style-type:disc">For the filtering task, you can use the<ul id="4ae79278-f375-424a-8af7-1cd7704912ea" class="bulleted-list"><li style="list-style-type:circle">sample code if you are using <em>pyaudio</em>.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80ac-addb-dda15a077fac" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script captures live audio from a microphone, applies a bandpass filter to the audio signal,
and displays both the original and filtered waveforms in real-time using Matplotlib.
It uses PyAudio for audio capture, SciPyâ€™s signal processing functions (butter and sosfilt)
for designing and applying a bandpass filter, and NumPy for numerical operations.
This demo is useful for understanding real-time audio processing, filtering, and visualization,
and it can help you prepare for lab questions on digital signal processing and Python audio handling.
&quot;&quot;&quot;

#%% Import the required libraries
import pyaudio  # Interfaces with audio hardware for capturing microphone input.
              # Q: Why use PyAudio? 
              # A: It allows easy real-time audio capture and playback in Python.
import struct   # Used to convert binary audio data (bytes) into 16-bit integers.
              # Q: What is the role of the struct module?
              # A: It unpacks the raw byte data from the microphone into numerical values.
import numpy as np  # Provides efficient numerical operations and array manipulation.
import matplotlib.pyplot as plt  # For plotting waveforms and updating plots in real-time.
from scipy.signal import butter, sosfilt  # Used for designing and applying a bandpass filter.
              # Q: What do butter and sosfilt do?
              # A: &#x27;butter&#x27; designs a Butterworth filter, and &#x27;sosfilt&#x27; applies the filter in second-order sections.
import time  # For measuring execution time of operations (e.g., filtering frame rate).

#%% Parameters
# Set the parameters for audio capture and processing.
BUFFER = 1024 * 16          # Number of audio samples per frame.
                           # Q: Why is the buffer size important?
                           # A: It affects the resolution and latency of the real-time processing.
FORMAT = pyaudio.paInt16    # Audio format: 16-bit integer samples.
                           # Q: What does paInt16 represent?
                           # A: Each audio sample is stored as a 16-bit integer.
CHANNELS = 1                # Use a single channel (mono audio).
                           # Q: How would this change for stereo?
                           # A: CHANNELS would be set to 2.
RATE = 44100                # Sampling rate in samples per second (standard high-quality audio).
                           # Q: Why choose 44100 Hz?
                           # A: It&#x27;s the CD-quality sampling rate providing a good balance between quality and processing load.
RECORD_SECONDS = 20         # Duration (in seconds) for which audio is recorded.

#%% Create matplotlib figure and axes with initial placeholder plots
# Create a figure with two subplots: one for the original audio waveform and one for the filtered output.
fig, (ax1, ax2) = plt.subplots(2, figsize=(7, 7))

# Generate x-axis data for the waveform plots.
x = np.arange(0, 2 * BUFFER, 2)  # Sample indices for the waveform.
                                # Q: Why use np.arange with step=2?
                                # A: It corresponds to the number of 16-bit samples after unpacking the byte stream.

# Create initial line objects with random data for both plots.
line, = ax1.plot(x, np.random.rand(BUFFER), &#x27;-&#x27;, lw=2)         # Original waveform plot.
line_filter, = ax2.plot(x, np.random.rand(BUFFER), &#x27;-&#x27;, lw=2)    # Filtered waveform plot.

# Basic formatting for the original waveform plot.
ax1.set_title(&#x27;AUDIO WAVEFORM&#x27;)
ax1.set_xlabel(&#x27;Samples&#x27;)
ax1.set_ylabel(&#x27;Amplitude&#x27;)
ax1.set_ylim(-5000, 5000)  # Adjust y-axis limits based on expected microphone signal amplitude.
ax1.set_xlim(0, BUFFER)

# Basic formatting for the filtered waveform plot.
ax2.set_title(&#x27;FILTERED&#x27;)
ax2.set_xlabel(&#x27;Samples&#x27;)
ax2.set_ylabel(&#x27;Amplitude&#x27;)
ax2.set_ylim(-5000, 5000)
ax2.set_xlim(0, BUFFER)

# Display the plot in non-blocking mode so the script continues execution.
plt.show(block=False)

#%% Function for design of filter
def design_filter(lowfreq, highfreq, fs, order=3):
    &quot;&quot;&quot;
    Designs a Butterworth bandpass filter.
    
    Parameters:
        lowfreq (float): Lower cutoff frequency in Hz.
        highfreq (float): Higher cutoff frequency in Hz.
        fs (float): Sampling frequency in Hz.
        order (int): The order of the filter (default is 3).
    
    Returns:
        sos (ndarray): Second-order sections representation of the filter.
        
    Q: Why use second-order sections (sos)?
    A: They provide improved numerical stability for high-order filters.
    &quot;&quot;&quot;
    nyq = 0.5 * fs  # Calculate the Nyquist frequency.
    low = lowfreq / nyq  # Normalize the low cutoff frequency.
    high = highfreq / nyq  # Normalize the high cutoff frequency.
    sos = butter(order, [low, high], btype=&#x27;band&#x27;, output=&#x27;sos&#x27;)  # Design the bandpass filter.
    return sos

# Design the filter with chosen cutoff frequencies.
# Note: Adjust the cutoff frequencies as needed. Here, it&#x27;s set between 19400 Hz and 19600 Hz.
sos = design_filter(19400, 19600, 48000, 3)
              # Q: Why is the sampling frequency set to 48000 here?
              # A: This value is used for filter design; ensure it matches or is appropriately related to your audio RATE.

#%% Initialize the pyaudio class instance
# Create a PyAudio object to interact with the audio hardware.
audio = pyaudio.PyAudio()

# Open an audio stream to capture data from the microphone.
stream = audio.open(
    format=FORMAT,           # Audio format (16-bit integer).
    channels=CHANNELS,       # Mono audio.
    rate=RATE,               # Sampling rate.
    input=True,              # Enable audio input.
    output=True,             # Enable audio output (if needed).
    frames_per_buffer=BUFFER # Number of samples per frame.
)

print(&#x27;stream started&#x27;)  # Indicate that the audio stream has started.

#%% Main loop: Capture audio, apply filter, and update plots in real-time.
exec_time = []  # List to store the execution time for each filtering operation.

# Loop for the total duration calculated by dividing total samples (RATE * RECORD_SECONDS) by BUFFER.
for _ in range(0, RATE // BUFFER * RECORD_SECONDS):
    
    # Read a chunk of binary audio data from the microphone.
    data = stream.read(BUFFER)
    
    # Convert the binary data to a tuple of 16-bit integers.
    # The format string (e.g., &#x27;16384h&#x27; for BUFFER = 16384) tells struct.unpack the number of samples.
    data_int = struct.unpack(str(BUFFER) + &#x27;h&#x27;, data)
                # Q: Why convert the byte data to integers?
                # A: It allows numerical processing (such as filtering and plotting) of the audio signal.
    
    # Apply bandpass filtering to the audio data.
    start_time = time.time()  # Start timing the filtering operation.
    yf = sosfilt(sos, data_int)  # Apply the designed bandpass filter.
    exec_time.append(time.time() - start_time)  # Record the time taken for filtering.
    
    # Update the original waveform plot with the new audio data.
    line.set_ydata(data_int)
    
    # Update the filtered waveform plot with the filtered data.
    line_filter.set_ydata(yf)
    
    # Redraw the updated plots.
    fig.canvas.draw()
    fig.canvas.flush_events()

#%% Cleanup: Terminate the audio stream and report the average execution time.
audio.terminate()  # Close the audio stream and free resources.

print(&#x27;stream stopped&#x27;)
# Calculate and print the average execution time for the filtering operation in milliseconds.
print(&#x27;average execution time = {:.0f} milli seconds&#x27;.format(np.mean(exec_time) * 1000))
                # Q: Why is measuring execution time important?
                # A: It helps determine if the filtering and plotting can be done in real-time.</code></pre><ul id="b51e1707-0222-4866-8a70-04d2db09167f" class="bulleted-list"><li style="list-style-type:circle">sample code if you are using <em>sounddevice</em>.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8054-badc-d4f2768d0c58" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#%% Import the required libraries
#%% Import the required libraries
import sounddevice as sd # Refer to https://python-sounddevice.readthedocs.io/en/0.4.6/
import numpy as np 
import matplotlib.pyplot as plt
import time
from scipy.signal import butter, sosfilt # Refer to https://docs.scipy.org/doc/scipy/reference/signal.html (Used for Bandpass filtering)
import time # In case time of execution is required  

#%% Parameters
BUFFER = 1024 * 16          # samples per frame (you can change the same to acquire more or less samples)
CHANNELS = 1                # single channel for microphone
RATE = 44100                # samples per second
RECORD_SECONDS = 20         # Specify the time to record from the microphone in seconds

#%% create matplotlib figure and axes with initial random plots as placeholder
fig, (ax1, ax2) = plt.subplots(2, figsize=(4, 4))
# create a line object with random data
x = np.arange(0, 2*BUFFER, 2)       # samples (waveform)

line, = ax1.plot(x,np.random.rand(BUFFER), &#x27;-&#x27;, lw=2)
line_filter, = ax2.plot(x,np.random.rand(BUFFER), &#x27;-&#x27;, lw=2)

# basic formatting for the axes
ax1.set_title(&#x27;AUDIO WAVEFORM&#x27;)
ax1.set_xlabel(&#x27;samples&#x27;)
ax1.set_ylabel(&#x27;amplitude&#x27;)
ax1.set_ylim(-5000, 5000) # change this to see more amplitude values (when we speak)
ax1.set_xlim(0, BUFFER)

ax2.set_title(&#x27;FILTERED&#x27;)
ax2.set_xlabel(&#x27;samples&#x27;)
ax2.set_ylabel(&#x27;amplitude&#x27;)
ax2.set_ylim(-5000, 5000) 
ax2.set_xlim(0, BUFFER)

# show the plot
plt.show(block=False)

#%% Function for design of filter
def design_filter(lowfreq, highfreq, fs, order=3):
    nyq = 0.5*fs
    low = lowfreq/nyq
    high = highfreq/nyq
    sos = butter(order, [low,high], btype=&#x27;band&#x27;,output=&#x27;sos&#x27;)
    return sos

# design the filter
sos = design_filter(19400, 19600, 48000, 3) #change the lower and higher freqcies according to choice


exec_time = []
for _ in range(0, RATE // BUFFER * RECORD_SECONDS):   
       
    # Record the sound in int16 format and wait till recording is done
    data = sd.rec(frames=BUFFER,samplerate=RATE,channels=CHANNELS,dtype=&#x27;int16&#x27;,blocking=True)
    data = np.squeeze(data)      
    
    # Bandpass filtering
    start_time=time.time()  # for measuring frame rate
    yf = sosfilt(sos, data)
    
    # calculate average frame rate
    exec_time.append(time.time() - start_time)
    
    #update line plots for both axes
    line.set_ydata(data)
    line_filter.set_ydata(yf)
    fig.canvas.draw()
    fig.canvas.flush_events()
    
print(&#x27;stream stopped&#x27;)
print(&#x27;average execution time = {:.0f} milli seconds&#x27;.format(np.mean(exec_time)*1000))</code></pre><ul id="17768074-b286-4fca-aa98-80d8f08dce45" class="bulleted-list"><li style="list-style-type:circle">Using the audio spectrum visualization, identify the frequency to be kept (e.g. tap sound or some particular sound) and change the above code accordingly.</li></ul></li></ul><ul id="2f1781ee-aab9-4ae2-9b8a-18e058f3ebb8" class="bulleted-list"><li style="list-style-type:disc">Feature extraction: Spectrogram, Chromogram, Mel-Spectrogram and MFFC.<ul id="4883840c-675b-403d-ba6c-2382d1d56610" class="bulleted-list"><li style="list-style-type:circle">Install the <a href="https://librosa.org/doc/latest/index.html">librosa library</a> using the command<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e5f036f0-03d7-499c-9eee-588aa8046a43" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install librosa</code></pre></li></ul><ul id="2f4aa630-5ddf-42df-80e2-ad7e8bcd8920" class="bulleted-list"><li style="list-style-type:circle">In this section, we will explore various features which can be extracted from speech/audio time series employing the librosa library. A <a href="https://github.com/drfuzzi/INF2009_SoundAnalytics/blob/main/Codes/audio_features.py">sample code</a> which shows how to extract the above features is available for testing.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80a3-b6fe-e6adc158af3b" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script demonstrates various audio feature extraction techniques using the Librosa library.
It loads an audio file (&quot;test.wav&quot;), computes its short-time Fourier transform (STFT) to generate a spectrogram,
estimates chroma features, computes a Mel-spectrogram, and extracts MFCC (Mel-frequency cepstral coefficients).
Each of these features is visualized using Matplotlib.
This code is useful for understanding how to extract and analyze different 
audio features, which is a common task
in speech and audio processing, and could be helpful for your lab test.
&quot;&quot;&quot;

#%% Import the required libraries
import numpy as np                      # Provides efficient numerical operations on arrays.
import matplotlib.pyplot as plt         # For plotting graphs and visualizations.
import librosa                         # Library for audio processing and feature extraction.
                                        # Q: Why use Librosa?
                                        # A: It offers powerful tools for audio analysis, including loading audio, computing spectrograms, and extracting features.
# Note: librosa.display is used for visualizing spectrograms; make sure it is available in your Librosa version.
import librosa.display

#%% Load Audio File
# Load the audio file &quot;test.wav&quot; with its original sampling rate (sr=None preserves the file&#x27;s native rate).
y, sr = librosa.load(&quot;test.wav&quot;, sr=None)
# Q: What do &#x27;y&#x27; and &#x27;sr&#x27; represent?
# A: &#x27;y&#x27; is the audio time series (a NumPy array of amplitude values), and &#x27;sr&#x27; is the sampling rate (samples per second).

#%% Compute the Spectrogram Magnitude and Phase
# Compute the short-time Fourier transform (STFT) of the audio signal.
# Then, separate the magnitude and phase components.
S_full, phase = librosa.magphase(librosa.stft(y))
# Q: What is the purpose of computing the STFT?
# A: STFT converts the time-domain signal into the frequency domain, allowing analysis of how frequency content evolves over time.

#%% Plot the Time Series and the Frequency-Time Plot (Spectrogram)
fig, (ax1, ax2) = plt.subplots(2, figsize=(7, 7))
# Plot the time series (raw audio signal).
ax1.plot(y)
ax1.set_xlabel(&#x27;Samples&#x27;)
ax1.set_ylabel(&#x27;Volume&#x27;)
ax1.set(title=&#x27;Time Series&#x27;)
# Q: Why visualize the time series?
# A: It shows the raw amplitude variation over time and helps in understanding the overall dynamics of the audio.

# Convert the amplitude spectrogram to decibel (dB) units for better visualization.
img = librosa.display.specshow(librosa.amplitude_to_db(S_full, ref=np.max),
                               y_axis=&#x27;log&#x27;, x_axis=&#x27;time&#x27;, sr=sr, ax=ax2)
fig.colorbar(img, ax=ax2)
ax2.set(title=&#x27;Spectrogram&#x27;)
plt.show()

#%% Chroma Estimation
# Compute the power spectrogram (magnitude squared) from the STFT.
S = np.abs(librosa.stft(y, n_fft=4096))**2
# Extract chroma features from the power spectrogram.
chroma = librosa.feature.chroma_stft(S=S, sr=sr)
# Q: What are chroma features?
# A: Chroma features represent the intensity of each of the 12 different pitch classes (semitones) in the audio.

# Plot the power spectrogram and the chromagram.
fig, ax = plt.subplots(nrows=2, sharex=True)
img = librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),
                               y_axis=&#x27;log&#x27;, x_axis=&#x27;time&#x27;, ax=ax[0])
fig.colorbar(img, ax=[ax[0]])
ax[0].set(title=&#x27;Power Spectrogram&#x27;)
ax[0].label_outer()

img = librosa.display.specshow(chroma, y_axis=&#x27;chroma&#x27;, x_axis=&#x27;time&#x27;, ax=ax[1])
fig.colorbar(img, ax=[ax[1]])
ax[1].set(title=&#x27;Chromagram&#x27;)
plt.show()

#%% Compute Mel-Spectrogram
# Compute a Mel-scaled spectrogram with 128 Mel bands and a maximum frequency of 8000 Hz.
S_mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
# Convert the Mel spectrogram power to decibel (dB) units.
S_mel_dB = librosa.power_to_db(S_mel, ref=np.max)
# Plot the Mel-frequency spectrogram.
fig, ax = plt.subplots()
img = librosa.display.specshow(S_mel_dB, x_axis=&#x27;time&#x27;, y_axis=&#x27;mel&#x27;,
                               sr=sr, fmax=8000, ax=ax)
fig.colorbar(img, ax=ax, format=&#x27;%+2.0f dB&#x27;)
ax.set(title=&#x27;Mel-Frequency Spectrogram&#x27;)
plt.show()

#%% Compute MFCC (Mel-Frequency Cepstral Coefficients)
# Extract 40 MFCCs from the audio signal.
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
# Recompute the Mel spectrogram for visualization.
S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
# Plot both the Mel spectrogram and the MFCCs.
fig, ax = plt.subplots(nrows=2, sharex=True)
img = librosa.display.specshow(librosa.power_to_db(S, ref=np.max),
                               x_axis=&#x27;time&#x27;, y_axis=&#x27;mel&#x27;, fmax=8000, ax=ax[0])
fig.colorbar(img, ax=[ax[0]])
ax[0].set(title=&#x27;Mel Spectrogram&#x27;)
ax[0].label_outer()

img = librosa.display.specshow(mfccs, x_axis=&#x27;time&#x27;, ax=ax[1])
fig.colorbar(img, ax=[ax[1]])
ax[1].set(title=&#x27;MFCC&#x27;)
plt.show()
# Q: What are MFCCs?
# A: MFCCs (Mel-frequency cepstral coefficients) are features that capture the timbral aspects of audio, commonly used in speech and music analysis.</code></pre></li></ul><ul id="3faac269-f67e-493a-91fb-463b03959673" class="bulleted-list"><li style="list-style-type:circle">The time series recorded through microphone and saved as test.wav (mostly speech with some background noise) and its spectrogram are shown below <br/>The <br/><a href="https://en.wikipedia.org/wiki/Spectrogram">spectrogram</a> is a visual representation of the spectrum of frequencies of a signal as it varies with time. <figure id="5bf2cfa8-bcfe-4c4b-be9e-bacb4eceac9d" class="image"><a href="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/0ff75402-20c6-492f-9ee8-1f20c954c0a3"><img style="width:384px" src="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/0ff75402-20c6-492f-9ee8-1f20c954c0a3"/></a></figure></li></ul><ul id="b3d0bc64-fdf3-4bcd-8e65-fe83c448444a" class="bulleted-list"><li style="list-style-type:circle">The spectrogram and the chromogram are shown below <br/>In Western music, the term <br/><a href="https://en.wikipedia.org/wiki/Chroma_feature">chroma feature or chromagram</a> closely relates to twelve different pitch classes. Chroma-based features, which are also referred to as &quot;pitch class profiles&quot;, are a powerful tool for analyzing music whose pitches can be meaningfully categorized (often into twelve categories) and whose tuning approximates to the equal-tempered scale.<figure id="1a6167d4-75c8-46bd-91f0-9e9e5976a0ce" class="image"><a href="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/7be397f9-9b7e-4c98-a1ea-38dab4b2caba"><img style="width:384px" src="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/7be397f9-9b7e-4c98-a1ea-38dab4b2caba"/></a></figure></li></ul><ul id="5d98ae93-d1fd-4deb-acc8-da7e9a266d72" class="bulleted-list"><li style="list-style-type:circle">The Mel-frequency spectrogram is shown below <br/>A Mel Spectrogram makes two important changes relative to a regular Spectrogram that plots Frequency vs Time. It uses the Mel Scale instead of Frequency on the y-axis. It uses the Decibel Scale instead of Amplitude to indicate colors. The <br/><a href="https://en.wikipedia.org/wiki/Mel_scale">Mel Scale</a> is a perceptual scale of pitches judged by listeners to be equal in distance from one another.<figure id="1e14b797-9cbf-498f-9fbf-2f9063e767b4" class="image"><a href="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/4663a522-8c0e-416f-95e3-eefe42a3696b"><img style="width:480px" src="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/4663a522-8c0e-416f-95e3-eefe42a3696b"/></a></figure></li></ul><ul id="563f717d-893a-44d8-b8cf-b9886877704a" class="bulleted-list"><li style="list-style-type:circle">Finally the MFCC (Mel Frequency Cepstral Coefficients) and the original Mel Spectrogram are as shown below \<br/> \<br/>The [mel-frequency cepstrum (MFC)] (<br/><a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">https://en.wikipedia.org/wiki/Mel-frequency_cepstrum</a>) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.<figure id="0838e34d-c7ca-4f72-bd00-5bfc0cff224d" class="image"><a href="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/d2746cc2-54a3-4eff-beb5-664813a2fcd0"><img style="width:480px" src="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/d2746cc2-54a3-4eff-beb5-664813a2fcd0"/></a></figure></li></ul></li></ul><p id="b515253d-2a14-428e-bcb7-941b0159bc4a" class=""><strong>6. Advanced Sound Analytics (20 minutes)</strong></p><ul id="55a20678-a1e2-47a9-a9b6-77b16999f388" class="bulleted-list"><li style="list-style-type:disc">Introduction to machine learning with sound through speech recognition task (through <a href="https://cmusphinx.github.io/wiki/">CMUSphinx</a> and <a href="https://github.com/Uberi/speech_recognition/tree/master/third-party/Source%20code%20for%20Google%20API%20Client%20Library%20for%20Python%20and%20its%20dependencies">Google Speech Recognition</a>.</li></ul><ul id="10e7a311-62c9-4b4c-aaad-3e1f09b27f2e" class="bulleted-list"><li style="list-style-type:disc">Installing the <a href="https://github.com/Uberi/speech_recognition#readme">speech_recognition library</a> through following commands<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c0ff889e-0e67-43b2-ac43-4e9f9c9a13d7" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt-get install flac
pip install pocketsphinx
pip install SpeechRecognition</code></pre><ul id="62f9e61c-244b-4608-b2ea-b39e3869318d" class="bulleted-list"><li style="list-style-type:circle">A FLAC encoder (installed through above command) is required to encode the audio data to send to the API. Similarly CM Sphinx is also installed through above commands</li></ul><ul id="ce3e46aa-bdf4-4e9a-b7c7-1cfadbb75bc3" class="bulleted-list"><li style="list-style-type:circle">The <a href="https://www.notion.so/Codes/microphone_recognition.py">sample code</a> illustrates to record an audio, then use the CMUSphinx API and Google Speech Recognition APIs to predict the spoken text<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-805b-90b6-e465bbeaacb6" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script is a demonstration of speech recognition using the SpeechRecognition library.
It captures audio input from the microphone, processes the audio to recognize spoken words,
and then prints the recognized text using two different recognition engines:
Google Speech Recognition (an online service) and CMU Sphinx (an offline engine).
This demo is useful for learning how to integrate speech recognition into Python projects,
handle ambient noise, and manage potential errors during the recognition process.
Refer to the library&#x27;s GitHub page for more details:
https://github.com/Uberi/speech_recognition?tab=readme-ov-file#readme
&quot;&quot;&quot;

#%% Import all necessary libraries
import speech_recognition as sr  # Library to access various speech recognition engines.
                                # Q: Why use the SpeechRecognition library?
                                # A: It provides a simple interface for capturing audio and converting speech to text.
import time                    # Used for timing operations, such as measuring recognition duration.
import os                      # Used to execute operating system commands (e.g., clearing the terminal).

#%% Recording from microphone
# Obtain audio input from the microphone.

# Initialize the Recognizer instance; this object handles the audio processing.
r = sr.Recognizer()  # Q: What is the purpose of the Recognizer class?
                     # A: It encapsulates the functionality to capture and process audio for speech recognition.

# Use the microphone as the source for audio input.
with sr.Microphone() as source:
    # Adjust the recognizer sensitivity to ambient noise.
    # This step calibrates the recognizer to the background noise level.
    r.adjust_for_ambient_noise(source)
    # Q: Why is adjusting for ambient noise important?
    # A: It helps reduce the impact of background sounds, making the speech recognition more accurate.
    
    # Clear the terminal screen for a cleaner output.
    os.system(&#x27;clear&#x27;)
    print(&quot;Say something!&quot;)
    
    # Listen for the first phrase and capture it as audio.
    audio = r.listen(source)
    # Q: What does r.listen(source) do?
    # A: It records audio from the microphone until a pause is detected.

#%% Recognize speech using Google Speech Recognition
# Measure the time taken for Google Speech Recognition to process the audio.
start_time = time.time()  # Start timing the recognition process.

try:
    # Recognize the captured audio using Google&#x27;s speech recognition service.
    # The default API key is used for testing purposes.
    recognized_text = r.recognize_google(audio)
    print(&quot;Google Speech Recognition thinks you said &quot; + recognized_text)
    # Q: How does r.recognize_google(audio) work?
    # A: It sends the audio data to Google&#x27;s online service and returns the recognized text.
except sr.UnknownValueError:
    # This error is raised when the audio is not understood.
    print(&quot;Google Speech Recognition could not understand audio&quot;)
except sr.RequestError as e:
    # This error is raised when there is a problem with the request (e.g., network issues).
    print(&quot;Could not request results from Google Speech Recognition service; {0}&quot;.format(e))

# Print the time taken for Google Speech Recognition.
print(&#x27;Time for Google Speech Recognition recognition = {:.0f} seconds&#x27;.format(time.time() - start_time))

#%% Recognize speech using Sphinx
# Measure the time taken for CMU Sphinx (an offline engine) to process the audio.
start_time = time.time()  # Restart timing for Sphinx recognition.

try:
    # Recognize the captured audio using CMU Sphinx.
    recognized_text = r.recognize_sphinx(audio)
    print(&quot;Sphinx thinks you said &quot; + recognized_text)
    # Q: What is the advantage of using Sphinx over Google Speech Recognition?
    # A: Sphinx is an offline engine, which means it doesn&#x27;t require an internet connection.
except sr.UnknownValueError:
    # This error is raised when Sphinx cannot understand the audio.
    print(&quot;Sphinx could not understand audio&quot;)
except sr.RequestError as e:
    # This error is raised if there is an issue with Sphinx (e.g., missing data files).
    print(&quot;Sphinx error; {0}&quot;.format(e))

# Print the time taken for Sphinx recognition.
print(&#x27;Time for Sphinx recognition = {:.0f} seconds&#x27;.format(time.time() - start_time))</code></pre></li></ul><ul id="fb8c673e-1835-4bec-8078-af4ab213207b" class="bulleted-list"><li style="list-style-type:circle">Its important to see the &#x27;Say Something&#x27; before you start speaking as in the initial few seconds the ambient noise is being captured.</li></ul><ul id="822ee803-f900-43ad-b95f-3cbd670bbab2" class="bulleted-list"><li style="list-style-type:circle">A sample display screen will look like as in below:<br/><br/>It is very clear from the screenshot that an offline (inference done on the edge device) model like Sphinx is not as effective as a Google Speech Recognition API where inference is done on the cloud.<br/><figure id="26998df7-4619-430c-b6b0-acba88a00cd2" class="image"><a href="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/bc5b4ccc-f06e-422e-b0f0-8a403e14cc65"><img src="https://github.com/drfuzzi/INF2009_SoundAnalytics/assets/52023898/bc5b4ccc-f06e-422e-b0f0-8a403e14cc65"/></a></figure></li></ul></li></ul><ul id="9ba2d3b5-fa21-4ac6-9e6f-6908dcb77896" class="bulleted-list"><li style="list-style-type:disc">Employ other speech recognition APIs provided in the <a href="https://github.com/Uberi/speech_recognition#readme">speech_recognition library</a> and compare the performance on Rasperry Pi</li></ul><ul id="b2bdbe25-6b9f-4191-b50a-0165a6321fcc" class="bulleted-list"><li style="list-style-type:disc">Modify the code to identify certain words in the generated (predicted text) which can form the basis for &#x27;wake word&#x27; based system control (e.g. Ok Google, Alexa or Siri)</li></ul><hr id="1bfaecc5-ba04-80cc-966a-d8d8e2287293"/><h3 id="f71d1ed1-b32d-4e09-befb-a8b20ef8d0ff" class="">ðŸŽ™ï¸ Audio Capture + Visualization</h3><h3 id="7e94da6b-2212-4f1f-a43d-ea3c4e4f8de4" class="">Required Libraries:</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="b652b256-523e-4451-83a9-c18b8a9f77e6" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt install portaudio19-dev
pip install pyaudio sounddevice scipy matplotlib</code></pre><ul id="14a225ac-db38-4241-ab75-3d73bd5d3f75" class="bulleted-list"><li style="list-style-type:disc"><strong>Two options</strong>:<ul id="8e069797-5cc1-4b24-8b50-491ebbb8e0c1" class="bulleted-list"><li style="list-style-type:circle"><code>pyaudio</code> (uses <code>struct</code>)</li></ul><ul id="5785987b-778c-434d-b903-f02dc7221375" class="bulleted-list"><li style="list-style-type:circle"><code>sounddevice</code> (uses <code>numpy.squeeze</code>)</li></ul></li></ul><ul id="7dbc28c5-3d9b-4b57-a3a6-92255fcb4bd7" class="bulleted-list"><li style="list-style-type:disc"><strong>Plots</strong>:<ul id="f9effc6d-5eba-446f-a7e8-2d995f110a61" class="bulleted-list"><li style="list-style-type:circle">Top: Raw waveform</li></ul><ul id="1b0efea6-a2aa-4831-a5dc-ffe2e097aacf" class="bulleted-list"><li style="list-style-type:circle">Bottom: FFT spectrum (frequency domain)</li></ul></li></ul><hr id="1bfaecc5-ba04-806f-8fdd-fb378f2835be"/><h3 id="574dd123-24b6-4825-9836-69b72eef1929" class="">ðŸ”Š Basic Sound Analytics</h3><h3 id="cf35de98-f4b5-4c25-8b51-453f549907c3" class="">ðŸŽ› Bandpass Filter</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="6d2a4c5d-999a-4ea8-98bc-d48d55cdb2ba" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from scipy.signal import butter, sosfilt
sos = butter(order, [low/high], btype=&#x27;band&#x27;, output=&#x27;sos&#x27;)
filtered_data = sosfilt(sos, data)</code></pre><p id="5acd0b79-a2ea-43d9-9e0b-94c791240488" class="">Use plots to tune filter cutoff for isolating desired frequency (e.g., tap sounds).</p><h3 id="4bd88725-7180-48ba-b20c-d144cae5eda2" class="">ðŸ“Š Feature Extraction (via <code>librosa</code>)</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="18d22ebe-9845-4b3c-bccf-f1157d6d8a7b" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install librosa</code></pre><p id="c21a865e-a306-4261-807d-c6f7e62c4153" class="">Features:</p><ul id="2bea187d-4325-4548-b28e-a5acd88bf3ce" class="bulleted-list"><li style="list-style-type:disc"><strong>Spectrogram</strong> â€“ frequency over time</li></ul><ul id="a682bc23-a7e8-4c5f-8f42-d0740f15259a" class="bulleted-list"><li style="list-style-type:disc"><strong>Chromagram</strong> â€“ pitch class</li></ul><ul id="78f3de53-8ba7-429b-975a-4490aa67d9a9" class="bulleted-list"><li style="list-style-type:disc"><strong>Mel-spectrogram</strong> â€“ perceptual frequency scaling</li></ul><ul id="07dfd64d-80b8-403a-b906-9e39d8d9306b" class="bulleted-list"><li style="list-style-type:disc"><strong>MFCCs</strong> â€“ speech features (common in ML models)</li></ul><hr id="1bfaecc5-ba04-8055-90ae-f4c754a47680"/><h3 id="fcd74d12-3712-412d-88a3-4d7c6c814b8f" class="">ðŸ§  Advanced Sound Analytics (Speech Recognition)</h3><h3 id="644223cd-2e8d-4ac6-8885-1e6dd5aa8456" class="">Libraries:</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="179283fa-5aac-452b-a6d3-ec79e6767544" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt install flac
pip install pocketsphinx SpeechRecognition</code></pre><h3 id="03867fa4-0322-4a22-b6d5-d49ed8556391" class="">Concepts:</h3><ul id="8965fbe3-bbfd-4765-aa61-4908e72b9579" class="bulleted-list"><li style="list-style-type:disc"><code>recognize_google(audio)</code> â†’ cloud-based</li></ul><ul id="795f6776-1814-4e8a-a307-4f3942e25684" class="bulleted-list"><li style="list-style-type:disc"><code>recognize_sphinx(audio)</code> â†’ offline/edge</li></ul><p id="d57a74ed-1c50-4f61-b24a-979cad3fa4e0" class="">Compare their accuracy and response time.</p><hr id="f9f2a1a0-ef76-4d4c-9c5a-ad73ef0c49fd"/><p id="7b7e536b-bb14-45f7-bcd8-8cfe4195c763" class=""><strong>[Optional] Homework/Extended Activities:</strong></p><ol type="1" id="95da5312-1492-46d9-aaa3-e93cc1642631" class="numbered-list" start="1"><li>Build a voice-activated command system.<p id="7dd16386-6fc3-4232-bac9-f272a2df2485" class="">Goal: Detect words like &quot;start&quot;, &quot;stop&quot;, etc., and trigger actions.</p><ul id="c4d3214d-7237-4e0c-821d-9936c1cd3260" class="bulleted-list"><li style="list-style-type:disc">Use <code>speech_recognition.Recognizer().recognize_google()</code></li></ul><ul id="3cde586e-bf35-44e6-8524-22df5bd55ca5" class="bulleted-list"><li style="list-style-type:disc">Trigger GPIO pin outputs or print messages:</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c73ea743-0c6b-47ac-bb34-5552af790135" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">text = r.recognize_google(audio)
if &quot;start&quot; in text.lower():
    print(&quot;Command: START SYSTEM&quot;)</code></pre><ul id="f8bcc73e-3f11-43c1-9ef2-5fd2be3a4912" class="bulleted-list"><li style="list-style-type:disc">Use keywords like:<ul id="87721c42-2ff9-4bab-a4c1-ee82341adf46" class="bulleted-list"><li style="list-style-type:circle">&quot;light on&quot; â†’ GPIO high</li></ul><ul id="ca034c84-eb32-4607-a62d-143cbf525e71" class="bulleted-list"><li style="list-style-type:circle">&quot;shutdown&quot; â†’ system shutdown</li></ul></li></ul></li></ol><ol type="1" id="28245189-6976-4516-bdab-6e8f8e3de11c" class="numbered-list" start="2"><li>Create a basic sound classifier using a dataset of various sounds.<p id="1804d4e2-1341-414b-a806-05a097d047c6" class="">Goal: Train a model to recognize sounds like clap, snap, bell.</p><ul id="7a69af25-4636-4a02-b9d5-2ad483778768" class="bulleted-list"><li style="list-style-type:disc">Dataset: Collect <code>.wav</code> recordings labeled per class</li></ul><ul id="ea082e9c-5d53-41c3-b553-7f49dd84bc89" class="bulleted-list"><li style="list-style-type:disc">Extract <strong>MFCCs</strong> with <code>librosa</code></li></ul><ul id="c5efa759-5132-4b81-9f07-4051743a58a6" class="bulleted-list"><li style="list-style-type:disc">Use <strong>sklearn</strong> or <strong>tensorflow</strong> for classification:</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="20ae10c9-7d5c-44d1-96ad-4ed8e2d6f977" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()
clf.fit(features, label</code></pre><ul id="54c155e6-2e74-4cf3-86d3-c1b5c83e972c" class="bulleted-list"><li style="list-style-type:disc">Classify real-time audio from microphone.</li></ul></li></ol><ol type="1" id="436a4d05-8c3f-47cb-9947-b5914194f46a" class="numbered-list" start="3"><li>Experiment with sound effects: reverb, echo, and pitch alteration.<ul id="99fe590a-67b4-4e29-80f7-6284d8990885" class="bulleted-list"><li style="list-style-type:disc"><strong>Pitch shifting</strong>:</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="3a39eda1-cb73-4cbb-b24c-effa69dc493e" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import librosa
y_shifted = librosa.effects.pitch_shift(y, sr, n_steps=4)</code></pre><ul id="3ad87d92-c655-4d01-9ec9-582e50f17868" class="bulleted-list"><li style="list-style-type:disc"><strong>Add echo</strong>:</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="77c48990-b9db-45b2-b821-30fef3abfce0" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">echo = np.concatenate((np.zeros(delay_samples), 0.6 * y[:-delay_samples]))
y_echo = y + echo</code></pre><ul id="6e07e24d-851b-4a67-aeaf-c4c85be564d5" class="bulleted-list"><li style="list-style-type:disc"><strong>Reverb</strong>: Use <code>pydub</code> or <code>scipy.signal.convolve</code> with an impulse response file.</li></ul></li></ol><hr id="4f376e9d-526c-43e3-9305-fd99a12f5664"/><p id="22fe57fb-4b2d-4fbe-aa5e-f77ecdaf67ff" class=""><strong>Resources:</strong></p><ol type="1" id="da446c0d-f215-47d3-bc72-0e38aaa745b0" class="numbered-list" start="1"><li>Raspberry Pi official documentation.</li></ol><ol type="1" id="3a550167-ca22-4b89-a774-30c48973ca90" class="numbered-list" start="2"><li>Python sound processing library documentation (e.g., <code>librosa</code>, <code>pyaudio</code>, <code>speech_recognition</code>).</li></ol><ol type="1" id="ab025245-8795-4761-b83d-5fdd74fb3004" class="numbered-list" start="3"><li>Online communities and forums related to Raspberry Pi and sound analytics.</li></ol><hr id="29972e3b-145b-4b0c-9f8a-4b0969e1e5e6"/></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Week 3 Image Analytics</summary><div class="indented"><p id="1bfaecc5-ba04-80db-b0f5-c9b572d2bce5" class=""><strong>Image Analytics with Raspberry Pi using Web Camera</strong></p><p id="1bfaecc5-ba04-80b3-b3e3-c8e14d75a872" class=""><strong>Objective:</strong> By the end of this session, participants will understand how to set up a web camera with the Raspberry Pi, capture images, and perform basic and advanced image analytics.</p><hr id="1bfaecc5-ba04-8034-b4e1-ea5c46101df4"/><p id="1bfaecc5-ba04-80ac-a98a-fa0691b3700c" class=""><strong>Prerequisites:</strong></p><ol type="1" id="1bfaecc5-ba04-80a2-bff7-c39211259b7c" class="numbered-list" start="1"><li>Raspberry Pi with Raspbian OS installed.</li></ol><ol type="1" id="1bfaecc5-ba04-8070-9ca0-c06ff257a01d" class="numbered-list" start="2"><li>MicroSD card (16GB or more recommended).</li></ol><ol type="1" id="1bfaecc5-ba04-8037-940b-ed6ec2c6e7e7" class="numbered-list" start="3"><li>Web camera compatible with Raspberry Pi (Will be using USB Webcam for this experiment).</li></ol><ol type="1" id="1bfaecc5-ba04-8053-8557-cf0bcfd8e9cd" class="numbered-list" start="4"><li>Internet connectivity (Wi-Fi).</li></ol><ol type="1" id="1bfaecc5-ba04-80ea-963a-c007053eb8b8" class="numbered-list" start="5"><li>Basic knowledge of Python and Linux commands.</li></ol><hr id="1bfaecc5-ba04-80a6-b1f7-cd427e9cde91"/><p id="1bfaecc5-ba04-80f2-a1cc-d972eddc7349" class=""><strong>1. Introduction (10 minutes)</strong></p><p id="1bfaecc5-ba04-80e9-82b6-edfe395dedb7" class="">Computer vision has been a very popular field since the advent of digital systems. However computer vision on the edge devices such as Raspberry Pi is challenging due to resource contraints. Edge Computer Vision (ECV) has emerged as a transformative technology, with</p><p id="1bfaecc5-ba04-8026-b2ac-c4355080635a" class=""><a href="https://www.linkedin.com/pulse/what-edge-computer-vision-how-get-started-deep-block-net">Gartner</a></p><p id="1bfaecc5-ba04-8037-8ab6-ce336b1e424e" class="">recognizing it as one of the top emerging technologies of 2023. ECV offers several benefits such as 1) they can operate in real-time or near-real-time, providing instant insights and enabling immediate actions, 2) they offer enhanced privacy and security and 3) It reduces dependency on network connectivity or relaxes the bandwidth requirements as some processing will be done within.<br/>In this lab, few basic and advanced image processing tasks on edge devices is introduced. An overview of the experiments/setup is as follows:<br/></p><figure id="1bfaecc5-ba04-8026-a044-de188fdee0d3" class="image"><a href="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/882c84dc-1989-4039-807d-554a079e3776"><img src="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/882c84dc-1989-4039-807d-554a079e3776"/></a></figure><p id="1bfaecc5-ba04-8056-8f5b-c53c5fd94289" class=""><strong>2. Setting up the Raspberry Pi (15 minutes)</strong></p><ul id="1bfaecc5-ba04-80e5-92d2-ef65e467300c" class="bulleted-list"><li style="list-style-type:disc">Booting up the Raspberry Pi.</li></ul><ul id="1bfaecc5-ba04-8037-be66-f7caa20c9991" class="bulleted-list"><li style="list-style-type:disc">Setting up Wi-Fi/Ethernet.</li></ul><ul id="1bfaecc5-ba04-80aa-8a5e-fb67aa48ac57" class="bulleted-list"><li style="list-style-type:disc">System updates:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80c2-ae6b-d90965f4ea3e" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt update
sudo apt upgrade</code></pre></li></ul><ul id="1bfaecc5-ba04-8042-b418-f80d96f5a874" class="bulleted-list"><li style="list-style-type:disc"><strong>[Important!] Set up and activate a virtual environment named &quot;image&quot; for this experiment (to avoid conflicts in libraries) as below</strong><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80fd-93a2-d699ffe6914a" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt install python3-venv
python3 -m venv image
source image/bin/activate</code></pre></li></ul><p id="1bfaecc5-ba04-8009-b3d2-d25efee8ba57" class=""><strong>3. Connecting and Testing the Web Camera (5 minutes)</strong></p><ul id="1bfaecc5-ba04-808d-aa95-c15139995d17" class="bulleted-list"><li style="list-style-type:disc">Physically connect the web camera to the Raspberry Pi.</li></ul><p id="1bfaecc5-ba04-805d-8c67-dff97bf7ccca" class=""><strong>. Introduction to Real-time Image Processing with Python (25 minutes)</strong></p><ul id="1bfaecc5-ba04-806f-a9bd-d56c8d1d5f9c" class="bulleted-list"><li style="list-style-type:disc">Installing OpenCV:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-801f-a60a-fa3bba197dd5" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install opencv-python</code></pre></li></ul><ul id="1bfaecc5-ba04-80b9-a288-f92673b5f5cc" class="bulleted-list"><li style="list-style-type:disc">The <a href="https://www.notion.so/Codes/image_capture_display.py">s</a>ample codeshows the code to read frames from a webcam and then based on the intensity range for each colour channel (RGB), how to segment the image into red green and blue images. A sample image and the colour segmentation is as shown below<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8035-b893-c654611d1d9b" class="code"><code class="language-Python">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script performs real-time color detection using OpenCV. It captures video from the webcam,
segments the image based on predefined BGR color boundaries (for red, blue, and green),
normalizes the segmented images, and displays the original frame alongside the segmented color outputs.
This demo is useful for understanding color space manipulation, masking, and image normalization in OpenCV,
and it could be useful for lab tests on image processing and computer vision.
Reference: https://pyimagesearch.com/2014/08/04/opencv-python-color-detection/
&quot;&quot;&quot;

#%% Import necessary libraries
import cv2              # OpenCV library for image and video processing.
                        # Q: Why use OpenCV?
                        # A: It provides powerful tools for real-time computer vision tasks.
import numpy as np      # For numerical operations and array manipulation.

#%% Define color boundaries in the BGR color space
# OpenCV uses BGR ordering for images (not RGB), so our boundaries are specified accordingly.
# Each boundary is defined as a tuple of two lists: [lower_bound, upper_bound].
# These boundaries are used to detect colors in the image via thresholding.
boundaries = [
    ([17, 15, 100], [50, 56, 200]),  # For Red
    ([86, 31, 4], [220, 88, 50]),    # For Blue
    ([25, 90, 4], [62, 200, 50]),    # For Green
    ([0, 200, 200], [50, 255, 255])     # Yellow (new)
]
# Q: Why are the boundaries specified in BGR order?
# A: Because OpenCV represents images in BGR, so the channels are ordered Blue, Green, Red.

#%% Normalize the Image for Display (Optional)
def normalizeImg(Img):
    &quot;&quot;&quot;
    Normalize an image to the 0-255 range.

    Converts the image to float to perform division without errors, then scales
    the pixel values to span from 0 to 255, and finally converts the result back to uint8.
    
    Q: Why is normalization necessary?
    A: It improves image visualization by adjusting the intensity values to the full 0-255 range.
    &quot;&quot;&quot;
    Img = np.float64(Img)  # Convert to float to avoid integer division issues.
    norm_img = (Img - np.min(Img)) / (np.max(Img) - np.min(Img))  # Normalize to [0, 1].
    norm_img = np.uint8(norm_img * 255.0)  # Scale to [0, 255] and convert to 8-bit unsigned integer.
    return norm_img

#%% OpenCV Video Capture and Frame Analysis
cap = cv2.VideoCapture(0)  # Open the default webcam (index 0).
# Q: What does cv2.VideoCapture(0) do?
# A: It initializes video capturing from the default camera.

# Check if the webcam is opened correctly; if not, raise an error.
if not cap.isOpened():
    raise IOError(&quot;Cannot open webcam&quot;)

#%% Process video frames in a loop until &#x27;q&#x27; is pressed or interrupted.
while True:
    try:
        # Capture one frame from the webcam.
        ret, frame = cap.read()    
        # Q: What does cap.read() return?
        # A: It returns a boolean indicating success and the captured frame.

        output = []  # Initialize a list to store segmented images for each color boundary.
        
        # Loop over each defined color boundary.
        for (lower, upper) in boundaries:
            # Create NumPy arrays for the lower and upper bounds.
            lower = np.array(lower, dtype=&quot;uint8&quot;)
            upper = np.array(upper, dtype=&quot;uint8&quot;)
            
            # Create a mask that identifies pixels within the specified boundary.
            mask = cv2.inRange(frame, lower, upper)
            # Q: What does cv2.inRange() do?
            # A: It thresholds the image, creating a binary mask where pixels falling within the range are set to 255 (white) and others to 0 (black).
            
            # Apply the mask to the frame to segment out the desired color.
            segmented = cv2.bitwise_and(frame, frame, mask=mask)
            output.append(segmented)  # Append the segmented image to the output list.

        # Normalize the segmented images for display.
        # Note: The order in the output list corresponds to the boundaries order.
        red_img = normalizeImg(output[0])
        green_img = normalizeImg(output[1])
        blue_img = normalizeImg(output[2])
        yellow_img = normalizeImg(output[3])  # New addition
        # Q: Why normalize the segmented images?
        # A: Normalization improves visualization by scaling the pixel intensities consistently.
       
        # Concatenate the original frame and the segmented images horizontally for display.
        catImg = cv2.hconcat([frame, red_img, green_img, blue_img, yellow_img])
        
        # Display the concatenated image.
        cv2.imshow(&quot;Images with Colours&quot;, catImg)
        
        # Wait for 1 ms for a key press; if &#x27;q&#x27; is pressed, break the loop.
        if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;):
            break
   
    except KeyboardInterrupt:
        # Allow graceful exit if the user interrupts the program.
        break

# Release the webcam and close all OpenCV windows.
cap.release()
cv2.destroyAllWindows()
# Q: Why release the video capture and destroy windows?
# A: To free system resources and close display windows properly after the program ends.</code></pre></li></ul><ul id="1bfaecc5-ba04-80ea-9067-e4886a3cca21" class="bulleted-list"><li style="list-style-type:disc">Expand the code to segment another colour (say yellow) <p id="1bfaecc5-ba04-802a-aa26-fe13c288fdb3" class="">To <strong>expand the code to segment yellow</strong>, you just need to:</p><ol type="1" id="1bfaecc5-ba04-8000-a5de-fe652a847b43" class="numbered-list" start="1"><li><strong>Add the BGR lower and upper boundary values for yellow</strong> to the <code>boundaries</code> list.</li></ol><ol type="1" id="1bfaecc5-ba04-8044-bb3f-c20c5241b9d4" class="numbered-list" start="2"><li>Ensure the segmented yellow image is normalized and added to the final concatenated display.</li></ol></li></ul><p id="1bfaecc5-ba04-8016-93e4-f76b7e7b1d60" class=""><strong>5. Real-time Image Analysis (25 minutes)</strong></p><ul id="1bfaecc5-ba04-806d-acf5-eb5fd9a0e6e5" class="bulleted-list"><li style="list-style-type:disc">Installing scikit-image:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8016-b1bf-ecae934ef1d0" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install scikit-image</code></pre></li></ul><ul id="1bfaecc5-ba04-80b4-9810-f3f2d5e935eb" class="bulleted-list"><li style="list-style-type:disc">Computer vision employs feature extraction from images. Some important image features include edges and textures. In this section we will employ a feature named histogram of gradients (HoG) which is widely employed for face recognition and other tasks. HoG involves gradient operation (basically extracting edges) on various image patches (by dividing the image into blocks). </li></ul><ul id="1bfaecc5-ba04-805b-9485-c3949608e53c" class="bulleted-list"><li style="list-style-type:disc">A sample code involving scikit-image is employed for the same. <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8026-9c0f-d275615fa6da" class="code"><code class="language-Python">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script captures real-time video from a webcam, extracts Histogram of Oriented Gradients (HOG) features
from each frame using the scikit-image library, and displays both the original frame and its corresponding
HOG visualization side by side. HOG features are useful for object detection and image analysis tasks.
This demo is useful for understanding feature extraction in computer vision and can help answer questions
related to image processing techniques, performance considerations, and working with video streams.
Reference: https://scikit-image.org/
&quot;&quot;&quot;

import cv2                     # OpenCV library for video capture and image processing.
import numpy as np             # NumPy for efficient array operations.
from skimage import feature   # Provides the HOG function for feature extraction.
from skimage import exposure  # Used for image intensity rescaling (contrast stretching).

#%% OpenCV Video Capture and Frame Analysis
cap = cv2.VideoCapture(0)  # Open the default webcam (index 0).
# Q: What does cv2.VideoCapture(0) do?
# A: It initializes video capture from the default camera.

# Check if the webcam is opened correctly
if not cap.isOpened():
    raise IOError(&quot;Cannot open webcam&quot;)  # Raise an error if the webcam cannot be accessed.

# The loop will break on pressing the &#x27;q&#x27; key
while True:
    try:
        # Capture one frame from the webcam.
        ret, frame = cap.read()  
        # Q: What do ret and frame represent?
        # A: &#x27;ret&#x27; is a boolean indicating success, and &#x27;frame&#x27; is the captured image.

        # Optional: Resize the frame for faster processing.
        # Uncomment the following line to resize the frame to 256x256 pixels.
        # frame = cv2.resize(frame, (256, 256))
        # Q: How does resizing affect processing speed?
        # A: Smaller images require less computation, leading to faster processing times.

        # Convert the captured frame to grayscale.
        # HOG feature extraction in scikit-image works only on grayscale images.
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # Q: Why convert the image to grayscale?
        # A: To reduce computational complexity and because HOG typically operates on single channel images.

        # Extract the HOG features from the grayscale image.
        # &#x27;H&#x27; contains the HOG feature vector; &#x27;hogImage&#x27; is a visualization of the HOG.
        (H, hogImage) = feature.hog(image,
                                    orientations=9,         # Number of orientation bins.
                                    pixels_per_cell=(8, 8),   # Size (in pixels) of a cell.
                                    cells_per_block=(2, 2),   # Number of cells in each block.
                                    transform_sqrt=True,      # Apply power law compression.
                                    block_norm=&quot;L1&quot;,          # Block normalization method.
                                    visualize=True)           # Return the HOG image for visualization.
        # Q: What is the purpose of the &#x27;visualize&#x27; parameter?
        # A: It returns an image representation of the HOG, which helps in understanding the gradient distribution.

        # Rescale the intensity of the HOG visualization to the range 0-255.
        # This is done to improve the contrast of the HOG image.
        hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255))
        hogImage = hogImage.astype(&quot;uint8&quot;)  # Convert the image to 8-bit unsigned integers.
        # Q: Why rescale intensity?
        # A: To adjust the image contrast for better visual interpretation.

        # Convert the grayscale HOG image back to RGB for concatenation with the original frame.
        hogImg = cv2.cvtColor(hogImage, cv2.COLOR_GRAY2RGB)
        # Q: Why convert from grayscale to RGB?
        # A: To match the color channels of the original frame when concatenating for display.

        # Concatenate the original frame and the HOG visualization horizontally.
        catImg = cv2.hconcat([frame, hogImg])
        # Q: What does cv2.hconcat() do?
        # A: It combines two images horizontally (side-by-side).

        # Display the concatenated image in a window titled &quot;HOG Image&quot;.
        cv2.imshow(&quot;HOG Image&quot;, catImg)
        
        # Exit the loop when the &#x27;q&#x27; key is pressed.
        if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;):
            break
        
    except KeyboardInterrupt:
        # Allow graceful exit if a keyboard interrupt (Ctrl+C) occurs.
        break

# Release the video capture object and close all OpenCV windows.
cap.release()
cv2.destroyAllWindows()
# Q: Why is it important to release the capture and destroy windows?
# A: To free system resources and ensure the program terminates cleanly.</code></pre></li></ul><ul id="1bfaecc5-ba04-8030-9266-e65c36c2f825" class="bulleted-list"><li style="list-style-type:disc">The code displays the dominant HoG image for each image patch overlaid on the actual image. It has to be noted that OpenCV can also be employed for the same task, but the visualization using scikit-image is better compared to that from OpenCV. A sample image for the HoG feature is as shown below:<figure id="1bfaecc5-ba04-80f6-a965-ff730aed42a3" class="image"><a href="https://github.com/drfuzzi/INF2009_ImageAnalytics/assets/52023898/94e7d597-c259-4634-a3dc-433c79e8533b"><img src="https://github.com/drfuzzi/INF2009_ImageAnalytics/assets/52023898/94e7d597-c259-4634-a3dc-433c79e8533b"/></a></figure><ul id="1bfaecc5-ba04-8071-91a8-fefdab1e8beb" class="bulleted-list"><li style="list-style-type:circle">Note the usage of colour (RGB) to gray scale conversion employed before HoG feature extraction.</li></ul><ul id="1bfaecc5-ba04-805c-a1d8-fa3c18621fec" class="bulleted-list"><li style="list-style-type:circle">Run the code with and without resizing the image and observe the resultant frame rate. It is important to note that for edge computing, downsizing the image will speed up the compute and many such informed decisions are critical.</li></ul><ul id="1bfaecc5-ba04-808b-989b-e24881856b0d" class="bulleted-list"><li style="list-style-type:circle">Change the patch size in line 25 (feature.hog) and observe the changes in the results.</li></ul></li></ul><ul id="1bfaecc5-ba04-80e3-85af-d90ee13b61d2" class="bulleted-list"><li style="list-style-type:disc">The HoG features can be employed to identify the presence of face. An <a href="https://www.notion.so/Codes/image_human_capture.py">example using OpenCV</a> is available for experimenting with. A multiscale HoG feature extraction is employed in this case. This involves extracting HoG features at multiple scales (resolutions) of the given image.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8032-87d3-da28e5bfcccc" class="code"><code class="language-Python">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script performs real-time person detection using OpenCV&#x27;s HOG (Histogram of Oriented Gradients)
descriptor combined with a pre-trained SVM detector for people. It captures video from the webcam,
detects people in each frame, calculates the horizontal distance of each detected personâ€™s bounding box
from a fixed center, and then prints commands (e.g., &quot;left&quot;, &quot;right&quot;, &quot;center&quot;) based on that distance.
This demo can be used to learn about object detection, bounding box processing, and basic decision logic
in computer vision applications, which may be relevant for your lab test.
&quot;&quot;&quot;

import cv2  # OpenCV for video capture and image processing.
import numpy as np  # NumPy for efficient numerical and array operations.

#%% Initialize the HOG descriptor/person detector
# Create an instance of the HOGDescriptor.
hog = cv2.HOGDescriptor()
# Set the SVM detector with a pre-trained people detector.
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
# Q: Why do we use HOG for person detection?
# A: HOG features are effective for capturing the shape and appearance of people in images.

# Define a tolerance for center alignment.
# This value determines how many pixels away from the center a person can be
# before the system decides the person is off-center.
center_tolerance = 5

#%% OpenCV Video Capture and Frame Analysis
cap = cv2.VideoCapture(0)  # Open the default webcam (index 0).
# Q: What does cv2.VideoCapture(0) do?
# A: It initializes video capture from the default camera.

# Check if the webcam is opened correctly.
if not cap.isOpened():
    raise IOError(&quot;Cannot open webcam&quot;)

# Process frames continuously until &#x27;q&#x27; is pressed.
while True:
    try:
        # Capture one frame from the webcam.
        ret, frame = cap.read()
        # Q: What are &#x27;ret&#x27; and &#x27;frame&#x27;?
        # A: &#x27;ret&#x27; is a boolean indicating if the frame was successfully captured, and &#x27;frame&#x27; is the image.

        # Resize frame for faster detection.
        frame = cv2.resize(frame, (256, 256))
        # Q: How does resizing affect performance?
        # A: Smaller frames require less computation, speeding up detection at the cost of resolution.

        # Detect people in the image.
        # &#x27;detectMultiScale&#x27; returns bounding boxes and weights for detected objects.
        boxes, weights = hog.detectMultiScale(frame, winStride=(1, 1), scale=1.05)
        # Convert bounding boxes to format: [x1, y1, x2, y2].
        boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])
        centers = []  # List to store each box and its distance from a defined center.

        # Loop over detected bounding boxes to compute horizontal center position.
        for box in boxes:
            # Calculate center x-coordinate of the bounding box.
            center_x = ((box[2] - box[0]) / 2) + box[0]
            # Compute horizontal distance from a fixed center (70 in this case).
            x_pos_rel_center = center_x - 70
            # Get absolute distance.
            dist_to_center_x = abs(x_pos_rel_center)
            # Append the box information and calculated values.
            centers.append({
                &#x27;box&#x27;: box,
                &#x27;x_pos_rel_center&#x27;: x_pos_rel_center,
                &#x27;dist_to_center_x&#x27;: dist_to_center_x
            })
        # Q: What is the purpose of computing the distance from the center?
        # A: It determines whether the detected person is centered, to the left, or right relative to a reference point.

        if len(centers) &gt; 0:
            # Sort the detected boxes by distance to the center.
            sorted_boxes = sorted(centers, key=lambda i: i[&#x27;dist_to_center_x&#x27;])
            # Draw the boxes: the most centered person is highlighted differently.
            for idx in range(len(sorted_boxes)):
                box = sorted_boxes[idx][&#x27;box&#x27;]
                # Draw a green rectangle for the most centered box.
                if idx == 0:
                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)
                else:
                    # Draw red rectangles for the other boxes.
                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 2)
            # Retrieve the horizontal offset of the most centered box.
            Center_box_pos_x = sorted_boxes[0][&#x27;x_pos_rel_center&#x27;]
            # Determine position relative to center and print corresponding message.
            if -center_tolerance &lt;= Center_box_pos_x &lt;= center_tolerance:
                # Person is centered; simulate turning on eye light.
                print(&quot;center&quot;)
            elif Center_box_pos_x &gt;= center_tolerance:
                # Person is to the right; instruct head turn to the right.
                print(&quot;right&quot;)
            elif Center_box_pos_x &lt;= -center_tolerance:
                # Person is to the left; instruct head turn to the left.
                print(&quot;left&quot;)
            # Print the calculated offset value.
            print(str(Center_box_pos_x))
        else:
            # No person detected in the frame.
            print(&quot;nothing detected&quot;)

        # Resize the frame for display purposes (easier to view on screen).
        frame = cv2.resize(frame, (720, 720))
        # Display the resulting frame.
        cv2.imshow(&quot;frame&quot;, frame)

        # Exit the loop if &#x27;q&#x27; key is pressed.
        if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;):
            break

    except KeyboardInterrupt:
        # Gracefully exit on keyboard interrupt (e.g., Ctrl+C).
        break

# Release the video capture object and close all OpenCV windows.
cap.release()
cv2.destroyAllWindows()
# Q: Why is it important to release the video capture and destroy windows?
# A: To free system resources and ensure that the program terminates cleanly.</code></pre></li></ul><p id="1bfaecc5-ba04-807b-be7e-ca7b5b2eebd3" class=""><strong>6. Real-time Image Feature Analysis for Face Capture and Facial Landmark Extraction (20 minutes)</strong></p><ul id="1bfaecc5-ba04-805c-a453-dddba0820f11" class="bulleted-list"><li style="list-style-type:disc">In this work, a light weight opensource library named <em>&quot;Mediapipe&quot;</em> for tasks such as face landmark detection, pose estimation, hand landmark detection, hand gesture recognition and object detection using pretrained neural network models.</li></ul><ul id="1bfaecc5-ba04-8055-bc4c-c778815c283f" class="bulleted-list"><li style="list-style-type:disc"><a href="https://developers.google.com/mediapipe">MediaPipe</a> is a on-device (<em>embedded machine learning</em>) framework for building cross platform multimodal applied ML pipelines that consist of fast ML inference, classic computer vision, and media processing (e.g. video decoding). MediaPipe was open sourced at CVPR in June 2019 as v0.5.0 and has various lightweight models developed with Tensorflow lite available for usage.</li></ul><ul id="1bfaecc5-ba04-806d-a006-c1f712e91f97" class="bulleted-list"><li style="list-style-type:disc">Installing media pipe:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8065-bac3-e6378324eaba" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install mediapipe</code></pre></li></ul><ul id="1bfaecc5-ba04-80ce-a8b1-ccab84a2426c" class="bulleted-list"><li style="list-style-type:disc">Try the <a href="https://www.notion.so/Codes/image_face_capture.py">sample code</a> to detect the face based on Mediapipe&#x27;s approach which is very light weight when compared to the approach employed in above section. <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8063-911f-c9ef30399144" class="code"><code class="language-Python">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script uses MediaPipe&#x27;s Face Mesh solution to detect and draw facial landmarks in real-time from a webcam feed.
It captures video frames, processes them with MediaPipe to identify facial landmarks, and overlays the mesh and contours on the face.
The demo is useful for understanding real-time face detection, landmark extraction, and drawing utilities in computer vision applications.
&quot;&quot;&quot;

import cv2                     # OpenCV for accessing the webcam and image processing.
import mediapipe as mp         # MediaPipe for efficient real-time face landmark detection.

#%% Initialize MediaPipe Face Mesh
mp_face_mesh = mp.solutions.face_mesh  # Access the Face Mesh module from MediaPipe.
# Create a FaceMesh object with specified parameters.
face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False,          # False for video stream mode (processes consecutive frames).
    max_num_faces=1,                  # Detect at most one face.
    min_detection_confidence=0.5,     # Minimum confidence value for face detection.
    min_tracking_confidence=0.5       # Minimum confidence value for tracking the face landmarks.
)
# Q: Why set static_image_mode to False?
# A: Because we&#x27;re processing a video stream, so we want the model to track faces across frames rather than detecting each frame independently.

# Initialize MediaPipe drawing utilities for visualization.
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

#%% Open the camera feed using OpenCV
cap = cv2.VideoCapture(0)  # Open the default webcam (index 0).
# Q: What happens if the camera isn&#x27;t available?
# A: The program will print an error message and exit.

if not cap.isOpened():
    print(&quot;Error: Could not access the camera.&quot;)
    exit()

#%% Process each frame from the webcam
while cap.isOpened():
    ret, frame = cap.read()  # Capture a frame from the webcam.
    if not ret:
        print(&quot;Failed to grab frame.&quot;)
        break

    # Convert the captured frame from BGR (OpenCV default) to RGB (MediaPipe requires RGB).
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    # Q: Why convert BGR to RGB?
    # A: MediaPipe models are trained on RGB images, so conversion is needed for proper processing.

    # Process the RGB frame to detect face landmarks.
    results = face_mesh.process(rgb_frame)

    # Draw facial landmarks on the original frame if any are detected.
    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            # Draw the tesselation (mesh) connections.
            mp_drawing.draw_landmarks(
                image=frame,
                landmark_list=face_landmarks,
                connections=mp_face_mesh.FACEMESH_TESSELATION,
                landmark_drawing_spec=None,
                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()
            )
            # Draw the contour connections (outline of the face).
            mp_drawing.draw_landmarks(
                image=frame,
                landmark_list=face_landmarks,
                connections=mp_face_mesh.FACEMESH_CONTOURS,
                landmark_drawing_spec=None,
                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style()
            )
            # Q: What do FACEMESH_TESSELATION and FACEMESH_CONTOURS represent?
            # A: Tesselation shows the full mesh of facial landmarks, while contours represent the outer shape of the face.

    # Display the frame with the drawn facial landmarks.
    cv2.imshow(&#x27;Mediapipe Face Mesh&#x27;, frame)

    # Exit the loop when &#x27;q&#x27; key is pressed.
    if cv2.waitKey(5) &amp; 0xFF == ord(&#x27;q&#x27;):
        break

# Release the camera and close any OpenCV windows.
cap.release()
cv2.destroyAllWindows()
# Q: Why is it important to release the camera and destroy windows?
# A: To free system resources and ensure that the application closes gracefully.</code></pre></li></ul><ul id="1bfaecc5-ba04-80f9-a7df-d1ab404f4edd" class="bulleted-list"><li style="list-style-type:disc">Observe the speed up. - A sample image with face landmarks is as shown below:<figure id="1bfaecc5-ba04-806e-9831-f0c0f560967f" class="image"><a href="https://github.com/user-attachments/assets/3e952cbb-72df-4258-9d96-83f05c741096"><img src="https://github.com/user-attachments/assets/3e952cbb-72df-4258-9d96-83f05c741096"/></a></figure></li></ul><ul id="1bfaecc5-ba04-8011-993d-cc9f0a996042" class="bulleted-list"><li style="list-style-type:disc">[Optional] An opencv alternative (no dependence on mediapipe) of the face detection is available in the <a href="https://www.notion.so/Codes/image_human_capture_opencv.py">sample code</a>. If you are using this code, make sure you download the <a href="https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt2.xml">Haar cascade model</a> manually and save it as &#x27;haarcascade_frontalface_alt2.xml&#x27; in the same folder as the code.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-804b-a954-d97dc05f3dae" class="code"><code class="language-Python">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script uses OpenCV&#x27;s Haar Cascade classifier to perform real-time face detection from a webcam feed.
It captures video frames, converts them to grayscale, and applies a pre-trained Haar Cascade model to detect faces.
Detected faces are highlighted by drawing white rectangles around them. This demo is useful for understanding
object detection, the use of pre-trained models (Haar Cascades), and real-time video processing in computer vision.
Before running the code, ensure that the Haar Cascade XML file (haarcascade_frontalface_alt2.xml) is in the same folder.
Download it from:
https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt2.xml
&quot;&quot;&quot;

import cv2  # OpenCV for image and video processing.
           # Q: Why use OpenCV for face detection?
           # A: OpenCV provides efficient and easy-to-use computer vision tools, including pre-trained Haar Cascade models.

#%% Initiate the Face Detection Cascade Classifier
haarcascade = &quot;haarcascade_frontalface_alt2.xml&quot;  # Path to the Haar Cascade model file.
detector = cv2.CascadeClassifier(haarcascade)      # Load the Haar Cascade classifier for face detection.
           # Q: What is a Haar Cascade?
           # A: It is a machine learning-based approach for detecting objects, like faces, based on Haar features.

#%% OpenCV Video Capture and Frame Analysis
cap = cv2.VideoCapture(0)  # Open the default webcam (index 0).
           # Q: What does cv2.VideoCapture(0) do?
           # A: It initializes the video capture from the system&#x27;s default camera.

# Check if the webcam is opened correctly
if not cap.isOpened():
    raise IOError(&quot;Cannot open webcam&quot;)  # Raise an error if the webcam cannot be accessed.

# Process video frames continuously until &#x27;q&#x27; is pressed
while True:
    try:
        # Capture one frame from the webcam.
        ret, frame = cap.read()
        # Q: What do ret and frame represent?
        # A: &#x27;ret&#x27; is a boolean indicating if the frame was successfully captured; &#x27;frame&#x27; is the captured image.

        # Resize the frame to 256x256 pixels for faster processing.
        frame = cv2.resize(frame, (256, 256))
        # Q: How does resizing help?
        # A: Smaller frames require less computation, increasing the speed of detection.

        # Convert the frame to grayscale, since Haar Cascade works on grayscale images.
        image_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # Q: Why convert to grayscale?
        # A: Feature extraction with Haar Cascades is based on intensity differences, which are more efficient on single-channel images.

        # Detect faces in the grayscale image using the Haar Cascade classifier.
        faces = detector.detectMultiScale(image_gray)
        # Q: What does detectMultiScale() return?
        # A: It returns a list of bounding boxes around detected faces in the format (x, y, width, height).

        # Loop over the detected faces and draw rectangles around them.
        for face in faces:
            (x, y, w, d) = face
            # Draw a white rectangle around the face on the original frame.
            cv2.rectangle(frame, (x, y), (x + w, y + d), (255, 255, 255), 2)
            # Q: What do the parameters of cv2.rectangle() represent?
            # A: They represent the image, top-left and bottom-right corners of the rectangle, the color (BGR), and the thickness.

        # Resize the frame for display (making it larger for easier viewing).
        frame = cv2.resize(frame, (720, 720))
        # Display the processed frame in a window titled &quot;frame&quot;.
        cv2.imshow(&quot;frame&quot;, frame)

        # Exit the loop if the &#x27;q&#x27; key is pressed.
        if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;):
            break

    except KeyboardInterrupt:
        # Allow graceful exit on keyboard interrupt (e.g., Ctrl+C).
        break

# Release the webcam and close all OpenCV windows.
cap.release()
cv2.destroyAllWindows()
# Q: Why is it important to release the capture and destroy windows?
# A: To free system resources and ensure that the program terminates cleanly.</code></pre></li></ul><hr id="1bfaecc5-ba04-801b-9e32-e256c208a568"/><p id="1bfaecc5-ba04-807a-b365-fa620863b023" class=""><strong>[Optional] Homework/Extended Activities:</strong></p><ol type="1" id="1bfaecc5-ba04-8085-a7c8-dad0f61025b9" class="numbered-list" start="1"><li>Explore more advanced OpenCV functionalities like SIFT, SURF, and ORB for feature detection. These features alongside HoG could be used for image matching (e.g. face recognition)</li></ol><ol type="1" id="1bfaecc5-ba04-80f6-b1b1-f91c63284444" class="numbered-list" start="2"><li>Build an eye blink detection system for drowsiness detection.</li></ol><hr id="1bfaecc5-ba04-80aa-9e8b-fb4366fb7240"/><p id="1bfaecc5-ba04-801a-9255-f70b065f7c69" class=""><strong>Resources:</strong></p><ol type="1" id="1bfaecc5-ba04-80ea-9a63-f08d7abe7639" class="numbered-list" start="1"><li>Raspberry Pi official documentation.</li></ol><ol type="1" id="1bfaecc5-ba04-8004-b952-c441b468e871" class="numbered-list" start="2"><li>OpenCV documentation and tutorials.</li></ol><ol type="1" id="1bfaecc5-ba04-80ba-b8ee-d6dc8de60190" class="numbered-list" start="3"><li>Relevant Python libraries documentation for image processing (e.g., <code>opencv</code>, <code>scikit-image</code>, <code>mediapipe</code>).</li></ol><hr id="1bfaecc5-ba04-8084-b57e-e98351261b54"/></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Week 4 Video Analytics</summary><div class="indented"><p id="1167aa9e-3ea7-4e56-989d-9120d817c1c1" class=""><strong>Video Analytics with Raspberry Pi using Web Camera</strong></p><p id="b0901a6b-65e4-47bb-8ff2-27228501ba94" class=""><strong>Objective:</strong> By the end of this session, participants will understand how to set up a web camera with the Raspberry Pi, capture video streams, and perform basic and advanced video analytics.</p><hr id="8e085e99-714f-48a3-96ef-ac14098ae8ec"/><p id="c85785a4-d3e6-4fd7-8560-81ff0a1abb76" class=""><strong>Prerequisites:</strong></p><ol type="1" id="8155e372-726d-4a52-9c92-ded7e316b6f3" class="numbered-list" start="1"><li>Raspberry Pi with Raspbian OS installed.</li></ol><ol type="1" id="118eb8fc-bdce-485b-9d3f-ce481ba1274f" class="numbered-list" start="2"><li>MicroSD card (16GB or more recommended).</li></ol><ol type="1" id="e651bc48-1bce-45f1-87a9-ea6ccbe334f9" class="numbered-list" start="3"><li>Web camera compatible with Raspberry Pi.</li></ol><ol type="1" id="2526044d-fb87-4e9f-9142-2d872fca9bf0" class="numbered-list" start="4"><li>Internet connectivity (Wi-Fi or Ethernet).</li></ol><ol type="1" id="51ddb7d0-b3c8-4e1b-b671-67ec0a0e7cb1" class="numbered-list" start="5"><li>Basic knowledge of Python and Linux commands.</li></ol><hr id="2a1ca22d-77da-472f-886c-c2f61f200fd0"/><p id="a16fb3f3-51a7-4555-8e7f-a693bda92042" class=""><strong>1. Introduction (10 minutes)</strong></p><ul id="c2a25169-c00c-481e-92fa-aae5359e6ff7" class="bulleted-list"><li style="list-style-type:disc">Video analytics is an emerging field employed to extract valuable insights from video data. Edge video analytics with real-time processing capabilities is chellenging but important and inevitable due to privacy/security concerns. Also, in many cases redundancy can be avoided to save on the bandwidth requirements (e.g. compress the video to have only key (important) frames). In this lab, few basic and advanced video processing tasks on edge devices is introduced. An overview of the experiments/setup is as follows:<figure id="3be151db-7e67-4b94-9e37-03aeacd52fb7" class="image"><a href="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/882c84dc-1989-4039-807d-554a079e3776"><img src="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/882c84dc-1989-4039-807d-554a079e3776"/></a></figure></li></ul><p id="7f21ab3e-c2e8-4180-b777-3c4643efe0e9" class=""><strong>2. Setting up the Raspberry Pi (10 minutes)</strong></p><ul id="ed6d470b-db97-4e78-b2d1-2256b77b12b9" class="bulleted-list"><li style="list-style-type:disc">Booting up the Raspberry Pi.</li></ul><ul id="c46c5785-89d6-4d7c-b70f-267aaee81f69" class="bulleted-list"><li style="list-style-type:disc">Setting up Wi-Fi/Ethernet.</li></ul><ul id="c2a7af57-96c7-4ec5-b89c-d8346232de83" class="bulleted-list"><li style="list-style-type:disc">System updates:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="43a678f0-0a7f-4866-bbb8-04925db6b1de" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt update
sudo apt upgrade</code></pre></li></ul><p id="9274aeb4-97ba-4f0c-962e-3f2307e6c3de" class=""><strong>3. Connecting and Testing the Web Camera (5 minutes)</strong></p><ul id="3fcfd023-7b24-4389-9440-00c3244be0b7" class="bulleted-list"><li style="list-style-type:disc">Please ensure the web camera is working and proceed to subsequent steps.</li></ul><p id="cab9bc59-37ea-4a5e-9199-c5e009681e93" class=""><strong>4. Introduction to real-time video processing on raspberry pi (20 minutes)</strong></p><ul id="3287084b-aeac-4855-bcfa-7031c6e1df68" class="bulleted-list"><li style="list-style-type:disc"><strong>[Important!] Set up and activate a virtual environment named &quot;video&quot; for this experiment (to avoid conflicts in libraries) as below. You can also reuse the virtual environment &quot;image&quot; as we are employing opencv and mediapipe libraries for video analytics</strong><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ae9ad8a7-a20a-4ac5-a8e5-d5ae84ab2db7" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt install python3-venv
python3 -m venv video
source video/bin/activate</code></pre></li></ul><ul id="05f5242f-e625-483a-99d3-cb87e6a5f978" class="bulleted-list"><li style="list-style-type:disc">Installing OpenCV:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="4156d903-044f-415c-afef-d2f0557f593e" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install opencv-python</code></pre></li></ul><ul id="621c63bf-8c33-4b83-985b-a9c49a3b682c" class="bulleted-list"><li style="list-style-type:disc"><a href="https://en.wikipedia.org/wiki/Optical_flow">Optical flow</a> estimation is employed to track moving objects in a video sequence. In this section, we will employ the purely opencv based <a href="https://www.notion.so/Codes/optical_flow.py">sample code</a> for estimaging the flow using Lucas Kanade Optical Flow approach and Flow Farneback approach. The displays are in the form of streamlines or directional arrows as shown below. <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-805d-82f6-e4724f910a84" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script performs real-time optical flow estimation and tracking using OpenCV.
It demonstrates two optical flow techniques:
1. Sparse optical flow using the Lucas-Kanade method, which tracks specific feature points.
2. Dense optical flow using Gunnar Farnebackâ€™s algorithm, which computes flow vectors for a grid of pixels.
The script captures video from a webcam, processes each frame to estimate motion between consecutive frames,
and visualizes the motion either as drawn tracks (for Lucas-Kanade) or as flow lines (for Farneback).
This code is useful for understanding motion analysis and tracking in video sequences, which are common topics in computer vision labs.
Reference: https://github.com/daisukelab/cv_opt_flow/tree/master
&quot;&quot;&quot;

import numpy as np            # NumPy for numerical operations and array manipulation.
import cv2                    # OpenCV for image processing, video capture, and optical flow estimation.

#%% Generic Parameters
# Generate an array of 100 random colors for visualizing different feature tracks.
color = np.random.randint(0, 255, (100, 3))  # Each row is a random BGR color.
# Q: Why use random colors?
# A: To easily distinguish different feature points or flow vectors when drawn on the frame.

#%% Parameters for Lucas-Kanade Optical Flow Approach
# Parameters for Shi-Tomasi corner detection (good features to track).
feature_params = dict(
    maxCorners=100,       # Maximum number of corners to return.
    qualityLevel=0.3,     # Minimal accepted quality of image corners.
    minDistance=7,        # Minimum possible Euclidean distance between returned corners.
    blockSize=7           # Size of an average block for computing a derivative covariance matrix.
)
# Parameters for Lucas-Kanade optical flow.
lk_params = dict(
    winSize=(15, 15),     # Size of the search window at each pyramid level.
    maxLevel=2,           # Maximum number of pyramid levels.
    criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)  # Termination criteria.
)
# Q: What is the Lucas-Kanade method used for?
# A: It estimates motion by tracking sparse feature points between consecutive frames.

#%% Function: Initialize First Frame for Optical Flow
def set1stFrame(frame):
    &quot;&quot;&quot;
    Prepares the first frame for optical flow estimation.
    Converts the frame to grayscale, detects good features to track (corners),
    and creates a mask for drawing the optical flow trajectories.

    Parameters:
      frame: The initial video frame (color image).

    Returns:
      frame_gray: The grayscale version of the frame.
      mask: An empty mask image with the same size as the frame for drawing.
      p0: Detected feature points (corners) for tracking.
    &quot;&quot;&quot;
    # Convert the input frame to grayscale.
    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    # Detect good features to track using Shi-Tomasi method.
    p0 = cv2.goodFeaturesToTrack(frame_gray, mask=None, **feature_params)
    # Create a blank mask image (same dimensions as frame) to draw optical flow paths.
    mask = np.zeros_like(frame)
    return frame_gray, mask, p0

#%% Function: Lucas-Kanade Optical Flow Tracking
def LucasKanadeOpticalFlow(frame, old_gray, mask, p0):
    &quot;&quot;&quot;
    Tracks feature points using the Lucas-Kanade optical flow method and draws their trajectories.

    Parameters:
      frame: The current video frame (color image).
      old_gray: The previous frame in grayscale.
      mask: A mask image used for drawing optical flow lines.
      p0: Feature points from the previous frame.

    Returns:
      img: The current frame with drawn optical flow paths.
      old_gray: The updated previous frame (current frame in grayscale).
      p0: Updated feature points for tracking in the next frame.
    &quot;&quot;&quot;
    # Convert the current frame to grayscale.
    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    # Check if there are valid feature points; if not, create a default set.
    if (p0 is None or len(p0) == 0):
        p0 = np.array([[50, 50], [100, 100]], dtype=np.float32).reshape(-1, 1, 2)
    
    # Calculate optical flow: find new positions of the previously tracked points.
    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)
    
    if p1 is not None:
        # Select good points that were successfully tracked.
        good_new = p1[st == 1]
        good_old = p0[st == 1]
    
        # Draw the optical flow tracks.
        for i, (new, old) in enumerate(zip(good_new, good_old)):
            a, b = new.ravel()  # New coordinates.
            c, d = old.ravel()  # Old coordinates.
            # Draw a line from the old position to the new position.
            mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)
            # Draw a circle at the new position.
            frame_gray = cv2.circle(frame_gray, (int(a), int(b)), 5, color[i].tolist(), -1)
        # Combine the original frame and the mask with drawn tracks.
        img = cv2.add(frame, mask)
    
        # Update old_gray to the current frame for the next iteration.
        old_gray = frame_gray.copy()
        # Update feature points with the new positions.
        p0 = good_new.reshape(-1, 1, 2)
    
    return img, old_gray, p0

#%% Function: Dense Optical Flow using Farneback&#x27;s Algorithm
step = 16  # Define the grid step for sampling flow vectors.

def DenseOpticalFlowByLines(frame, old_gray):
    &quot;&quot;&quot;
    Computes dense optical flow using Farneback&#x27;s algorithm and visualizes it as flow lines.

    Parameters:
      frame: The current video frame (color image).
      old_gray: The previous frame in grayscale.

    Returns:
      frame: The current frame with drawn flow vectors.
    &quot;&quot;&quot;
    # Convert the current frame to grayscale.
    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    # Get the height and width of the frame.
    h, w = frame_gray.shape[:2]
    # Create a grid of points at which to sample the optical flow.
    y, x = np.mgrid[step // 2:h:step, step // 2:w:step].reshape(2, -1)
    
    # Calculate dense optical flow between the previous and current grayscale frames.
    flow = cv2.calcOpticalFlowFarneback(old_gray, frame_gray, None,
                                          0.5, 3, 15, 3, 5, 1.2, 0)
    # Extract the flow vectors (fx, fy) at the grid points.
    fx, fy = flow[y, x].T
    
    # Prepare lines by combining the starting and ending points of flow vectors.
    lines = np.vstack([x, y, x + fx, y + fy]).T.reshape(-1, 2, 2)
    lines = np.int32(lines + 0.5)
    # Draw polylines on the original frame representing the flow.
    cv2.polylines(frame, lines, 0, (0, 255, 0))
    # Draw a small circle at the starting point of each flow vector.
    for (x1, y1), (x2, y2) in lines:
        cv2.circle(frame, (x1, y1), 1, (0, 255, 0), -1)
    return frame

#%% OpenCV Video Capture and Frame Analysis
cap = cv2.VideoCapture(0)  # Open the default webcam.

# Check if the webcam is opened correctly.
if not cap.isOpened():
    raise IOError(&quot;Cannot open webcam&quot;)

firstframeflag = 1  # Flag to indicate processing of the first frame.

# Process frames until the &#x27;q&#x27; key is pressed.
while True:
    try:
        # For the first frame, initialize the previous frame and feature points.
        if firstframeflag:
            ret, frame = cap.read()  # Capture one frame.
            old_gray, mask, p0 = set1stFrame(frame)
            firstframeflag = 0  # Reset flag after initialization.
        
        # Capture the current frame.
        ret, frame = cap.read()
        if not ret:
            print(&quot;Failed to grab frame.&quot;)
            break
        
        # Option to use Dense Optical Flow visualization.
        img = DenseOpticalFlowByLines(frame, old_gray)
        
        # Option to use Lucas-Kanade sparse optical flow (uncomment the following line to use it).
        # img, old_gray, p0 = LucasKanadeOpticalFlow(frame, old_gray, mask, p0)
        
        # Display the resulting frame with optical flow visualization.
        cv2.imshow(&quot;Optical Flow&quot;, img)
       
        # Exit the loop if the &#x27;q&#x27; key is pressed.
        if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;):
           break
   
    except KeyboardInterrupt:
        # Gracefully exit on keyboard interrupt.
        break

# Release the video capture object and close all OpenCV windows.
cap.release()
cv2.destroyAllWindows()
# Q: Why is it important to update the previous frame and feature points?
# A: Optical flow is computed relative to the previous frame; updating them ensures that motion is tracked accurately across frames.</code></pre><figure id="5082fef7-7b11-47f6-abc8-591ec629eb1c" class="image"><a href="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/c5987191-27ff-44f9-ac85-d1a673477dc8"><img style="width:384px" src="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/c5987191-27ff-44f9-ac85-d1a673477dc8"/></a></figure><figure id="a59be91f-cc40-4571-8291-0bbcd7f13e4c" class="image"><a href="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/f9a6d18e-4973-4af9-80f5-45901d090cc1"><img style="width:384px" src="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/f9a6d18e-4973-4af9-80f5-45901d090cc1"/></a></figure></li></ul><ul id="62dd95f9-1a22-42ea-9e2d-79998d3c47fb" class="bulleted-list"><li style="list-style-type:disc"><strong>[Important]</strong> You need to comment/uncomment respective lines (line 119/121) to activate the desired results. Modify the parameters (line 12/18) by looking into the OpenCV documentation and observe/note down the observations/conclusions.</li></ul><p id="a9b07bba-3a93-4e81-8517-19935893bb80" class=""><strong>5. Advanced Video Analytics (40 minutes)</strong></p><ul id="ce0e73a3-48e8-453b-836e-ef6ad9e5c1f3" class="bulleted-list"><li style="list-style-type:disc">We will employ a light weight opensource library named <em>&quot;Mediapipe&quot;</em> for tasks such as face landmark detection, pose estimation, hand landmark detection, hand gesture recognition and object detection using pretrained neural network models.</li></ul><ul id="91e0eb06-b0c1-4640-9f3c-40d4d8230742" class="bulleted-list"><li style="list-style-type:disc"><a href="https://developers.google.com/mediapipe">MediaPipe</a> is a on-device (<em>embedded machine learning</em>) framework for building cross platform multimodal applied ML pipelines that consist of fast ML inference, classic computer vision, and media processing (e.g. video decoding). MediaPipe was open sourced at CVPR in June 2019 as v0.5.0 and has various lightweight models developed with Tensorflow lite available for usage.</li></ul><ul id="95b6e226-d665-4222-adc0-e2ae9dd63c21" class="bulleted-list"><li style="list-style-type:disc">Installing media pipe:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a3e352e3-ffbf-4a93-b1d4-74dcb9e12239" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install mediapipe</code></pre></li></ul><ul id="5f77ad71-7077-4c61-b3bc-a16b35a8839e" class="bulleted-list"><li style="list-style-type:disc">Hand landmark detection<ul id="47532b6d-1c9c-4fbb-a233-762f4b0e80c1" class="bulleted-list"><li style="list-style-type:circle">Download the handlandmark detection model:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="43a33bb1-5a87-4af8-b13d-23da9bad6197" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">wget -q &lt;https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task&gt;</code></pre></li></ul><ul id="76c955db-ebc0-4ade-95f1-dd14ab4a5907" class="bulleted-list"><li style="list-style-type:circle">The <a href="https://www.notion.so/Codes/hand_landmark.py">sample code</a> employs opencv and mediapipe to detect the human hand and subsequently the finger locations (the tip of thumb and index finger as well as a simple logic to predict if the thumb is pointing up) based on the <a href="https://developers.google.com/mediapipe/solutions/vision/hand_landmarker">finger model</a> outlined below :<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80a6-8588-df4bd8283e90" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script implements real-time hand landmark detection using MediaPipeâ€™s Hand Landmarker.
It captures video from a webcam, processes each frame to detect hand landmarks, and then visualizes the detection by drawing annotations on the frame.
In this example, it specifically highlights the thumb tip and index finger tip, and checks if the thumb is raised (thumb up gesture).
This demo is useful for understanding how to integrate MediaPipe models into real-time applications and can help answer lab questions about model initialization, image processing, and gesture detection.
Reference: https://github.com/googlesamples/mediapipe/tree/main/examples/hand_landmarker/raspberry_pi
&quot;&quot;&quot;

import cv2  # OpenCV for video capture and image processing.
import mediapipe as mp  # MediaPipe framework for hand landmark detection.
from mediapipe.tasks import python  # Import the Python wrapper for MediaPipe tasks.
from mediapipe.tasks.python import vision  # Import the Vision API for the Hand Landmarker.
 
#%% Parameters
numHands = 2  # Number of hands to be detected.
model = &#x27;hand_landmarker.task&#x27;  # Path to the hand landmark detection model.
# Download using: wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task

minHandDetectionConfidence = 0.5  # Minimum confidence threshold for hand detection.
minHandPresenceConfidence = 0.5     # Minimum confidence threshold for hand presence.
minTrackingConfidence = 0.5         # Minimum confidence threshold for hand tracking.
frameWidth = 640   # Desired width of the video frame.
frameHeight = 480  # Desired height of the video frame.

# Visualization parameters for text display.
MARGIN = 10          # Margin for drawing text.
FONT_SIZE = 1        # Font size for text.
FONT_THICKNESS = 1   # Font thickness for text.
HANDEDNESS_TEXT_COLOR = (88, 205, 54)  # Color for text (vibrant green).

#%% Create a HandLandmarker object.
# Set up the base options with the model path.
base_options = python.BaseOptions(model_asset_path=model)
# Configure the Hand Landmarker options.
options = vision.HandLandmarkerOptions(
    base_options=base_options,
    num_hands=numHands,
    min_hand_detection_confidence=minHandDetectionConfidence,
    min_hand_presence_confidence=minHandPresenceConfidence,
    min_tracking_confidence=minTrackingConfidence
)
# Create the hand landmarker from the options.
detector = vision.HandLandmarker.create_from_options(options)
# Q: Why do we need to set multiple confidence thresholds?
# A: These thresholds help ensure that the model only processes hands with sufficient detection, presence, and tracking quality.

#%% OpenCV Video Capture and Frame Analysis
cap = cv2.VideoCapture(0)  # Open the default webcam.
cap.set(cv2.CAP_PROP_FRAME_WIDTH, frameWidth)   # Set the frame width.
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frameHeight) # Set the frame height.
# Q: What does setting frame width and height achieve?
# A: It ensures that the captured frames match the expected input size for the model.

# Check if the webcam is opened correctly.
if not cap.isOpened():
    raise IOError(&quot;Cannot open webcam&quot;)

# Process frames until the &#x27;q&#x27; key is pressed.
while True:
    try:
        # Capture one frame from the webcam.
        ret, frame = cap.read()
        if not ret:
            print(&quot;Failed to grab frame.&quot;)
            break

        # Flip the frame horizontally to mimic a mirror view.
        frame = cv2.flip(frame, 1)
        
        # Convert the image from BGR (OpenCV default) to RGB.
        # The model requires images in RGB format.
        rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Wrap the RGB image in a MediaPipe Image object.
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)
        
        # Run the hand landmarker on the image.
        detection_result = detector.detect(mp_image)
        # Q: What does detector.detect() return?
        # A: It returns a result object containing hand landmarks and other detection details.
        
        # Extract the list of hand landmarks from the detection result.
        hand_landmarks_list = detection_result.hand_landmarks
        # (Optional) handedness_list could be used to determine which hand is detected.
        # handedness_list = detection_result.handedness
        
        # Loop through each detected hand to visualize landmarks and gestures.
        for idx in range(len(hand_landmarks_list)):
            hand_landmarks = hand_landmarks_list[idx]
            
            # Detect the thumb tip by using index 4 (as per MediaPipe&#x27;s hand landmark model).
            x_thumb = int(hand_landmarks[4].x * frame.shape[1])
            y_thumb = int(hand_landmarks[4].y * frame.shape[0])
            cv2.circle(frame, (x_thumb, y_thumb), 5, (0, 255, 0), -1)  # Draw a green circle.
            
            # Detect the index finger tip using index 8.
            x_index = int(hand_landmarks[8].x * frame.shape[1])
            y_index = int(hand_landmarks[8].y * frame.shape[0])
            cv2.circle(frame, (x_index, y_index), 5, (0, 255, 0), -1)  # Draw a green circle.
            
            # Define a threshold for determining if the thumb is up.
            threshold = 0.1
            thumb_tip_y = hand_landmarks[4].y
            thumb_base_y = hand_landmarks[1].y  # Index 1 corresponds to the thumb base.
            thums_up = thumb_tip_y &lt; thumb_base_y - threshold  # Thumb up condition.
            
            # If the thumb is up, overlay text &quot;Thumb Up&quot; on the frame.
            if thums_up:
                cv2.putText(frame, &#x27;Thumb Up&#x27;, (10, 30),
                            cv2.FONT_HERSHEY_DUPLEX, FONT_SIZE,
                            HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)
                # Q: How is the thumb up gesture detected?
                # A: By comparing the y-coordinates of the thumb tip and thumb base, ensuring the tip is significantly higher.

        # Display the annotated frame.
        cv2.imshow(&#x27;Annotated Image&#x27;, frame)
        
        # Exit the loop when &#x27;q&#x27; is pressed.
        if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;):
            break
   
    except KeyboardInterrupt:
        # Gracefully exit on keyboard interrupt.
        break

# Release the video capture object and close all OpenCV windows.
cap.release()
cv2.destroyAllWindows()
# Q: Why is cleanup necessary?
# A: To free system resources and ensure the program terminates without leaving open windows or active hardware connections.</code></pre><figure id="709b5532-5a02-440f-a606-fd6039cde0bd" class="image"><a href="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/1090e213-7a56-4059-9386-50123bd6f8f8"><img src="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/1090e213-7a56-4059-9386-50123bd6f8f8"/></a></figure></li></ul><ul id="2b1738dd-c7d9-4783-953c-ab6a052bbca2" class="bulleted-list"><li style="list-style-type:circle">Modify the code to show all the 21 finger points and observe the same while moving the hand.</li></ul><ul id="a97db474-ab5e-4d89-8766-e117fafe2717" class="bulleted-list"><li style="list-style-type:circle">Modify the code to predict the number of fingers and display the same overlaid on the image as text (e.g. if four fingers are raised, display &#x27;4&#x27; on the screen and if three fingers on one hand and two on the other, the display should be &#x27;5&#x27;).</li></ul></li></ul><p id="09fbea65-9059-4fe2-875e-f2cf83040dbe" class=""><strong>6. Advanced Video Analytics (20 minutes)</strong></p><ul id="bf37dabe-4c50-443c-8991-de1e9dff096d" class="bulleted-list"><li style="list-style-type:disc">In this section, we will work on more advanced analytics tasks such as hand gesture recognition and object detection based on pretrianed light weight models.</li></ul><ul id="83d9354e-80aa-4913-8025-2be7b77a75a9" class="bulleted-list"><li style="list-style-type:disc">Hand gesture recognition<ul id="e5722377-48df-4768-a121-371e1ad3e568" class="bulleted-list"><li style="list-style-type:circle">Download the hand gesture recognition model:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="eac4356d-9f11-4aaa-9d5b-c91ada990e5e" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all"> wget -O gesture_recognizer.task -q &lt;https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task&gt;</code></pre></li></ul><ul id="bc4576c5-28ae-4dcb-8083-30cd78786983" class="bulleted-list"><li style="list-style-type:circle">The <a href="https://www.notion.so/Codes/hand_gesture.py">sample code</a> shows a real-time hand gesture recongition task. A sample snapshot of the code result for victory sign is shown below: <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80ee-92ea-e78bf47363ee" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script implements real-time hand gesture recognition using MediaPipeâ€™s gesture recognizer model.
It captures video from a webcam, processes each frame to detect hand gestures, draws hand landmarks,
and overlays gesture classification results on the video feed.
The code uses MediaPipe Tasksâ€™ Python API to run a pre-trained gesture recognizer model in live stream mode.
This demo is useful for understanding real-time gesture detection and can help answer questions related
to model initialization, asynchronous processing, landmark drawing, and result visualization in computer vision.
Reference: https://github.com/googlesamples/mediapipe/blob/main/examples/gesture_recognizer/raspberry_pi/
&quot;&quot;&quot;

import cv2                     # OpenCV for video capture and image processing.
import mediapipe as mp         # MediaPipe framework for hand gesture recognition.
import time                    # Time module for time stamps and timing operations.

from mediapipe.tasks import python            # Python wrapper for MediaPipe tasks.
from mediapipe.tasks.python import vision       # Vision API for gesture recognition and configuration.
from mediapipe.framework.formats import landmark_pb2  # For handling landmark data in protobuf format.

# Initialize MediaPipe hands drawing utilities for visualization.
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

#%% Parameters
numHands = 2  # Number of hands to be detected.
model = &#x27;gesture_recognizer.task&#x27;  
# Model for hand gesture detection.
# Download using: wget -O gesture_recognizer.task -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task

# Set thresholds for hand detection and tracking.
minHandDetectionConfidence = 0.5  
minHandPresenceConfidence = 0.5
minTrackingConfidence = 0.5

# Define the capture resolution as required by the model.
frameWidth = 640
frameHeight = 480

# Visualization parameters for drawing text on the frame.
row_size = 50        # Pixel height for row spacing.
left_margin = 24     # Pixel left margin.
text_color = (0, 0, 0) # Black color for text.
font_size = 1        # Font size.
font_thickness = 1   # Font thickness.

# Label box parameters (for gesture text).
label_text_color = (255, 255, 255)  # White text.
label_font_size = 1
label_thickness = 2

#%% Initialize results and a callback for appending recognition results.
recognition_frame = None         # Placeholder for the frame with drawn results.
recognition_result_list = []     # List to store recognition results from the gesture recognizer.

def save_result(result: vision.GestureRecognizerResult,
                unused_output_image: mp.Image, timestamp_ms: int):
    &quot;&quot;&quot;
    Callback function to save gesture recognition results.
    
    Parameters:
      result: The gesture recognition result from the model.
      unused_output_image: The processed image (not used here).
      timestamp_ms: Timestamp of the frame in milliseconds.
      
    Appends the result to the global recognition_result_list.
    &quot;&quot;&quot;
    recognition_result_list.append(result)

#%% Create a Hand Gesture Control object.
# Initialize the gesture recognizer model with specified options.
base_options = python.BaseOptions(model_asset_path=model)
options = vision.GestureRecognizerOptions(
    base_options=base_options,
    running_mode=vision.RunningMode.LIVE_STREAM,  # Live stream mode for real-time processing.
    num_hands=numHands,
    min_hand_detection_confidence=minHandDetectionConfidence,
    min_hand_presence_confidence=minHandPresenceConfidence,
    min_tracking_confidence=minTrackingConfidence,
    result_callback=save_result  # Callback function to handle asynchronous results.
)
recognizer = vision.GestureRecognizer.create_from_options(options)
# Q: Why use a callback for results?
# A: The asynchronous processing allows non-blocking gesture recognition, improving real-time performance.

#%% OpenCV Video Capture and Frame Analysis
cap = cv2.VideoCapture(0)  # Open the default webcam.
cap.set(cv2.CAP_PROP_FRAME_WIDTH, frameWidth)   # Set frame width.
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frameHeight) # Set frame height.

# Check if the webcam is opened correctly.
if not cap.isOpened():
    raise IOError(&quot;Cannot open webcam&quot;)

# Process frames continuously until &#x27;q&#x27; is pressed.
while True:
    try:
        # Capture one frame from the webcam.
        ret, frame = cap.read()
        if not ret:
            print(&quot;Failed to grab frame.&quot;)
            break

        # Flip the frame horizontally to match the typical mirror view.
        frame = cv2.flip(frame, 1)
        
        # Convert the image from BGR (OpenCV default) to RGB as required by the TFLite model.
        rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        # Wrap the image in a MediaPipe Image object.
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)
        
        # Keep a copy of the current frame for drawing results.
        current_frame = frame.copy()
        
        # Run hand gesture recognition asynchronously.
        # time.time_ns() // 1_000_000 converts current time to milliseconds.
        recognizer.recognize_async(mp_image, time.time_ns() // 1_000_000)
        
        # If any recognition result is available, process and draw them.
        if recognition_result_list:
            # Process results for each detected hand.
            for hand_index, hand_landmarks in enumerate(recognition_result_list[0].hand_landmarks):
                # Calculate the bounding box of the hand based on landmark coordinates.
                x_min = min([landmark.x for landmark in hand_landmarks])
                y_min = min([landmark.y for landmark in hand_landmarks])
                y_max = max([landmark.y for landmark in hand_landmarks])
    
                # Convert normalized coordinates (0-1) to pixel values.
                frame_height, frame_width = current_frame.shape[:2]
                x_min_px = int(x_min * frame_width)
                y_min_px = int(y_min * frame_height)
                y_max_px = int(y_max * frame_height)
    
                # Get gesture classification results.
                if recognition_result_list[0].gestures:
                    gesture = recognition_result_list[0].gestures[hand_index]
                    category_name = gesture[0].category_name  # Name of the recognized gesture.
                    score = round(gesture[0].score, 2)         # Confidence score.
                    result_text = f&#x27;{category_name} ({score})&#x27;
    
                    # Compute the size of the text for display.
                    text_size = cv2.getTextSize(result_text, cv2.FONT_HERSHEY_DUPLEX,
                                                label_font_size, label_thickness)[0]
                    text_width, text_height = text_size
    
                    # Calculate the text position (above the hand bounding box).
                    text_x = x_min_px
                    text_y = y_min_px - 10  # Adjust vertical position.
    
                    # Ensure the text is within frame boundaries.
                    if text_y &lt; 0:
                        text_y = y_max_px + text_height
    
                    # Draw the text (gesture name and score) on the frame.
                    cv2.putText(current_frame, result_text, (text_x, text_y),
                                cv2.FONT_HERSHEY_DUPLEX, label_font_size,
                                label_text_color, label_thickness, cv2.LINE_AA)
    
                # Draw hand landmarks on the frame.
                # Convert hand landmarks to a protobuf format required by MediaPipe drawing utilities.
                hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
                hand_landmarks_proto.landmark.extend([
                    landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z)
                    for landmark in hand_landmarks
                ])
                mp_drawing.draw_landmarks(
                    current_frame,
                    hand_landmarks_proto,
                    mp_hands.HAND_CONNECTIONS,
                    mp_drawing_styles.get_default_hand_landmarks_style(),
                    mp_drawing_styles.get_default_hand_connections_style()
                )
    
            # Update the frame to be displayed with recognition results.
            recognition_frame = current_frame
            # Clear the result list for the next frame.
            recognition_result_list.clear()
    
        # Display the frame with gesture recognition overlays.
        if recognition_frame is not None:
            cv2.imshow(&#x27;gesture_recognition&#x27;, recognition_frame)
        
        # Exit loop when &#x27;q&#x27; is pressed.
        if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;):
            break
   
    except KeyboardInterrupt:
        # Graceful exit on keyboard interrupt (e.g., Ctrl+C).
        break

# Release the webcam and destroy all OpenCV windows.
cap.release()
cv2.destroyAllWindows()
# Q: Why is cleanup important?
# A: It frees system resources and closes display windows, ensuring the application terminates properly.</code></pre><figure id="280374b9-8490-435f-975e-443b3464ac19" class="image"><a href="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/84bf1517-22c0-427a-9ca7-047551f1b50e"><img style="width:192px" src="https://github.com/drfuzzi/INF2009_VideoAnalytics/assets/52023898/84bf1517-22c0-427a-9ca7-047551f1b50e"/></a></figure></li></ul></li></ul><ul id="4e3d003f-6214-4f83-878e-0b05828323a0" class="bulleted-list"><li style="list-style-type:disc">Object detection<ul id="12f849c1-baa5-4688-bf1e-5c62adc507d6" class="bulleted-list"><li style="list-style-type:circle">Download the light weight EfficientDet object detection model:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="886e9d5f-7d5e-44c1-9679-bca6a35fc7b4" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all"> wget -q -O efficientdet.tflite -q &lt;https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/int8/1/efficientdet_lite0.tflite&gt;</code></pre></li></ul><ul id="b47bf759-a2f0-4566-bf28-2a6d68c59e47" class="bulleted-list"><li style="list-style-type:circle">The sample code shows a real-time object detection task.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80f0-83f1-f2dc387d4134" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script performs real-time object detection using MediaPipe&#x27;s lightweight EfficientDet model.
It captures video frames from a webcam, processes each frame with the MediaPipe object detector running
in live stream mode, and overlays bounding boxes, labels, and confidence scores on detected objects.
This demo is useful for learning about model initialization, asynchronous detection, and real-time
visualization of object detection results in computer vision applications.
Reference: https://github.com/googlesamples/mediapipe/blob/main/examples/object_detection/raspberry_pi
&quot;&quot;&quot;

import cv2  # OpenCV for video capture and image processing.
import mediapipe as mp  # MediaPipe framework for ML-based vision tasks.
import time  # Time module for timestamps and measuring time.

from mediapipe.tasks import python  # Import the Python wrapper for MediaPipe tasks.
from mediapipe.tasks.python import vision  # Import the Vision API for object detection tasks.

#%% Parameters
maxResults = 5  # Maximum number of detected objects per frame.
scoreThreshold = 0.25  # Minimum confidence score required to consider a detection valid.
frameWidth = 640  # Width of the video capture frame.
frameHeight = 480  # Height of the video capture frame.
model = &#x27;efficientdet.tflite&#x27;  # Path to the EfficientDet TFLite model.
# Q: Why choose EfficientDet model?
# A: EfficientDet is optimized for speed and accuracy in resource-constrained environments.

# Visualization parameters for drawing labels.
MARGIN = 10       # Margin in pixels for label placement.
ROW_SIZE = 30     # Vertical space in pixels between rows of text.
FONT_SIZE = 1     # Font size for the text.
FONT_THICKNESS = 1  # Font thickness for the text.
TEXT_COLOR = (0, 0, 0)  # Text color: black.

#%% Initialize results and define a callback function to save detection results.
detection_frame = None  # Placeholder for the frame with detection annotations.
detection_result_list = []  # List to store detection results asynchronously.

def save_result(result: vision.ObjectDetectorResult, unused_output_image: mp.Image, timestamp_ms: int):
    &quot;&quot;&quot;
    Callback function to save detection results.
    
    Parameters:
      result: The object detector result containing detection information.
      unused_output_image: The processed image (unused in this case).
      timestamp_ms: Timestamp of the frame in milliseconds.
      
    The result is appended to a global list for further processing.
    &quot;&quot;&quot;
    detection_result_list.append(result)

#%% Create an object detection model object.
# Set up base options with the model asset path.
base_options = python.BaseOptions(model_asset_path=model)
# Configure the object detector options.
options = vision.ObjectDetectorOptions(
    base_options=base_options,
    running_mode=vision.RunningMode.LIVE_STREAM,  # Use live stream mode for real-time processing.
    max_results=maxResults,
    score_threshold=scoreThreshold,
    result_callback=save_result  # Callback to handle results asynchronously.
)
# Create the object detector from the specified options.
detector = vision.ObjectDetector.create_from_options(options)
# Q: What does asynchronous detection provide?
# A: It allows non-blocking processing, improving real-time performance.

#%% OpenCV Video Capture and Frame Analysis
cap = cv2.VideoCapture(0)  # Open the default webcam.
cap.set(cv2.CAP_PROP_FRAME_WIDTH, frameWidth)    # Set the desired frame width.
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frameHeight)  # Set the desired frame height.
# Q: Why configure frame dimensions?
# A: The model might require specific dimensions for optimal performance.

# Check if the webcam is opened correctly.
if not cap.isOpened():
    raise IOError(&quot;Cannot open webcam&quot;)

# Process frames until the &#x27;q&#x27; key is pressed.
while True:
    try:
        # Capture one frame from the webcam.
        ret, frame = cap.read()
        if not ret:
            print(&quot;Failed to grab frame.&quot;)
            break

        # Flip the frame horizontally to mimic a mirror view.
        frame = cv2.flip(frame, 1)

        # Convert the frame from BGR (OpenCV format) to RGB (model requirement).
        rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        # Wrap the RGB image in a MediaPipe Image object.
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)

        # Keep a copy of the current frame for drawing detection results.
        current_frame = frame.copy()

        # Run object detection asynchronously using the model.
        # time.time_ns() // 1_000_000 converts the current time to milliseconds.
        detector.detect_async(mp_image, time.time_ns() // 1_000_000)

        # If detection results have been received, process them.
        if detection_result_list:
            # Loop through each detection in the first result.
            for detection in detection_result_list[0].detections:
                # Retrieve the bounding box from the detection.
                bbox = detection.bounding_box
                start_point = (bbox.origin_x, bbox.origin_y)
                end_point = (bbox.origin_x + bbox.width, bbox.origin_y + bbox.height)
                # Draw a rectangle around the detected object using an orange color.
                cv2.rectangle(current_frame, start_point, end_point, (0, 165, 255), 3)

                # Extract the category information from the detection.
                category = detection.categories[0]
                category_name = category.category_name  # Detected object&#x27;s label.
                probability = round(category.score, 2)    # Confidence score.
                result_text = f&#x27;{category_name} ({probability})&#x27;
                # Define the text location near the top-left of the bounding box.
                text_location = (MARGIN + bbox.origin_x, MARGIN + ROW_SIZE + bbox.origin_y)
                # Draw the label and score on the frame.
                cv2.putText(current_frame, result_text, text_location, cv2.FONT_HERSHEY_DUPLEX,
                            FONT_SIZE, TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)

            # Update the detection frame to be displayed.
            detection_frame = current_frame
            # Clear the detection results for the next frame.
            detection_result_list.clear()

        # Display the frame with detection overlays if available.
        if detection_frame is not None:
            cv2.imshow(&#x27;object_detection&#x27;, detection_frame)

        # Break the loop if the &#x27;q&#x27; key is pressed.
        if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;):
            break

    except KeyboardInterrupt:
        # Gracefully exit if a keyboard interrupt is detected.
        break

# Release the webcam and close all OpenCV windows.
cap.release()
cv2.destroyAllWindows()
# Q: Why is cleanup important in video capture applications?
# A: To free up system resources and ensure that hardware and windows are properly released upon exit.</code></pre></li></ul><ul id="fcfb77d8-4e38-4897-9ac0-0deab7c7a98f" class="bulleted-list"><li style="list-style-type:circle">Based on the above code, write a code to do object detection based video summarization (e.g. for a video with only frames having a cellphone)</li></ul></li></ul><hr id="1bfaecc5-ba04-8094-a8b7-f55d09dae472"/><h3 id="72cb17b1-491f-4eb5-904e-a3ba65de9fd2" class="">ðŸ“· Connecting the Web Camera</h3><ul id="160ee920-edbc-4fda-99f7-06b9fe6bdce9" class="bulleted-list"><li style="list-style-type:disc">Plug in USB camera â†’ test it using tools like <code>fswebcam</code> or OpenCV capture scripts.</li></ul><hr id="1bfaecc5-ba04-8044-a39c-dcd7901d63ca"/><h3 id="f4da5a6d-ae2c-458b-bbcf-07734649cdc9" class="">âœ‹ Hand Landmark Detection</h3><h3 id="456da122-bb64-43a7-9421-6460002fc0e9" class="">Download model</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="afa8092b-5530-4a1d-9a57-1725e4ccdbf7" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task</code></pre><h3 id="a6c2cd5d-cb71-4984-84a5-fb0192352088" class="">What to do:</h3><ul id="cf39790b-f8bd-4176-83e3-a92463dbf673" class="bulleted-list"><li style="list-style-type:disc">Show all <strong>21 hand landmarks</strong></li></ul><ul id="37bd035a-ec37-42c5-a852-c72e081d379e" class="bulleted-list"><li style="list-style-type:disc">Use coordinates to:<ul id="59519830-c4e7-43c8-a537-09339d9bd14a" class="bulleted-list"><li style="list-style-type:circle">Count fingers raised (e.g. detect if finger tip is above its lower joint)</li></ul><ul id="64da75a0-3230-4d75-a1d4-434fc5b78002" class="bulleted-list"><li style="list-style-type:circle">Display count as overlay: <code>&quot;Fingers: 4&quot;</code></li></ul></li></ul><blockquote id="fb5ed020-1c61-4845-93c0-ab85af790e79" class="">Use tip IDs (e.g. 4 for thumb, 8 for index) and compare y-coordinates with joints.</blockquote><hr id="0f1a1105-2429-4318-8f5a-14e6232cf356"/><h3 id="d76ed0d7-7742-40c5-a2f2-cfccb4d57e81" class="">âœŒï¸ Gesture Recognition</h3><h3 id="e2cfe849-3293-4ab4-9df3-5aa7aba98a83" class="">Download model</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="faf98f32-7899-4fff-a298-be4169cd4d26" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">wget -O gesture_recognizer.task -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task</code></pre><ul id="59b8744c-70d8-4bab-9d60-740168762a3b" class="bulleted-list"><li style="list-style-type:disc">Use model to recognize gestures like âœŒï¸, âœ‹, ðŸ‘Š etc.</li></ul><ul id="9c6acbbe-22bd-4d32-8069-39eac2787a56" class="bulleted-list"><li style="list-style-type:disc">Recognized text can be printed/overlaid live on screen.</li></ul><hr id="5c984799-651b-4cc6-a8a4-748c007bf8b1"/><h3 id="5d91fbca-69de-4ce5-b6e6-b89428aa6a11" class="">ðŸ“¦ Object Detection</h3><h3 id="3274478f-a2f2-413b-b565-a196846aa499" class="">Download EfficientDet Model</h3><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ddd323d7-4b4c-4168-b3d7-d8725b411115" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">wget -O efficientdet.tflite -q https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/int8/1/efficientdet_lite0.tflite</code></pre><ul id="31a67e95-9d88-43dc-96f0-40e0ed48ad16" class="bulleted-list"><li style="list-style-type:disc">Detect and label objects in real-time.</li></ul><ul id="2b5cf4d8-1d4c-4299-a8b0-af16afe9a29a" class="bulleted-list"><li style="list-style-type:disc">Efficient for edge devices like Pi.</li></ul><h3 id="da329fe4-4c05-48e7-899d-8065e885e334" class="">Extension Task:</h3><ul id="58d79f0c-8b8d-4bbd-8fab-5bd27566b320" class="bulleted-list"><li style="list-style-type:disc"><strong>Video summarization</strong>: Save/display only the frames containing a specific object (e.g., &#x27;cellphone&#x27;).</li></ul><hr id="06d360dc-b30c-4243-82dd-bc6bbe15d056"/><p id="c034a481-63f0-4096-b206-006b0e0815ef" class=""><strong>[Optional] Homework/Extended Activities:</strong></p><ol type="1" id="002f69de-0c4e-46b8-9b92-12f1fcc44ae4" class="numbered-list" start="1"><li>Experiment with more advanced tracking algorithms available in OpenCV.<ul id="3ce6c441-5b79-4231-9959-a9973eff46af" class="bulleted-list"><li style="list-style-type:disc"><strong>CSRT</strong>: High accuracy</li></ul><ul id="43b11c9a-19f7-49e1-8b26-a08ba1d6b3cf" class="bulleted-list"><li style="list-style-type:disc"><strong>KCF</strong>: Fast and decent performance</li></ul><ul id="803e958a-6719-4aa0-b3ee-84665e7232b5" class="bulleted-list"><li style="list-style-type:disc"><strong>MOSSE</strong>: Good for low-power Pi</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="6b4c63a8-dbe0-430c-bc53-2f7e123b465c" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">tracker = cv2.TrackerCSRT_create()
bbox = cv2.selectROI(frame)
tracker.init(frame, bbox)</code></pre><blockquote id="db3ea9bb-0686-4c85-86d4-7941e65ae693" class="">Track object of interest (e.g. a hand, face) across frames.</blockquote></li></ol><ol type="1" id="3c627963-0160-44fe-af5c-ab191387e5bd" class="numbered-list" start="2"><li>Build a gesture based video player control (e.g. could use libraries like <a href="https://pyautogui.readthedocs.io/en/latest/">Pyautogui</a> for the same)<p id="b3b559c3-b72a-4b6d-8dab-7dded9dc6e40" class="">Control media playback with hand gestures.</p><h3 id="fbff2f5e-5989-40ab-a586-ea35d21552e2" class="">Tools:</h3><ul id="1a95cf2e-d6ad-496b-a4b3-252ebf27e9ba" class="bulleted-list"><li style="list-style-type:disc"><code>pyautogui</code> for keyboard emulation</li></ul><ul id="a91a590c-f734-40a5-9aff-e8330924e898" class="bulleted-list"><li style="list-style-type:disc">MediaPipe for gesture detection</li></ul><h3 id="e979c7db-ea5b-4f1b-bcbf-f97a93e22ce6" class="">Example:</h3><ul id="95358285-aea7-4757-8a00-c3c0edd743a8" class="bulleted-list"><li style="list-style-type:disc">âœ‹ â†’ Pause/Play</li></ul><ul id="eda1deb8-b3f9-4101-bb53-1fac3431772a" class="bulleted-list"><li style="list-style-type:disc">ðŸ‘ â†’ Volume up</li></ul><ul id="0a09d851-bbf2-4135-84ab-c809f61cffc5" class="bulleted-list"><li style="list-style-type:disc">ðŸ‘Ž â†’ Volume down</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="79268eb0-c56a-4157-b694-fe0601a0c6f5" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import pyautogui
pyautogui.press(&quot;space&quot;)  # Toggle play/pause</code></pre></li></ol><ol type="1" id="dac1cf16-2d0d-4c2a-958e-763ab7fb8e35" class="numbered-list" start="3"><li>Build a surveillance system based on video based motion detection.<ul id="8b60463f-13cb-4357-8461-cf1cbdee84d5" class="bulleted-list"><li style="list-style-type:disc">Compare frame N and frame N+1</li></ul><ul id="6bc579a2-0ad0-44f1-b7b6-8b01dcd425ce" class="bulleted-list"><li style="list-style-type:disc">If pixel differences exceed a threshold, motion is detected</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="67a553a1-1da8-4460-9bd8-ed05d16d2d59" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">frameDelta = cv2.absdiff(gray1, gray2)
thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]</code></pre><p id="81d8cae6-bfe7-4b14-b8b9-4e25cac600bb" class="">Trigger:</p><ul id="85e48f31-66f8-4644-9ddf-98f33a78c38a" class="bulleted-list"><li style="list-style-type:disc">Video recording</li></ul><ul id="422f6da2-bda2-49b9-980e-b2af0d738b69" class="bulleted-list"><li style="list-style-type:disc">Email notification</li></ul><ul id="2148cce3-0fe5-4bf5-b877-24653e096e55" class="bulleted-list"><li style="list-style-type:disc">Image save with timestamp</li></ul></li></ol><hr id="78c1f915-cb0d-4b81-aebe-9e7bbafce654"/><p id="d9146b3d-f0bb-44be-9713-154bdaeabc71" class=""><strong>Resources:</strong></p><ol type="1" id="77ee4b78-7118-4af4-978a-dc98994d1e41" class="numbered-list" start="1"><li>Raspberry Pi official documentation.</li></ol><ol type="1" id="d482a2da-db8b-459b-a9e5-1f0d03f9b2a7" class="numbered-list" start="2"><li>OpenCV documentation and tutorials.</li></ol><ol type="1" id="6449ac74-17fc-4058-b92e-98e114495870" class="numbered-list" start="3"><li>Mediapipe documentation and tutorials.</li></ol><hr id="af408382-77bb-4794-93cb-2e9aa55ffaf6"/><ul id="5d6d1965-c8df-40a2-989f-6f62ec2af8d3" class="bulleted-list"><li style="list-style-type:disc"><strong>OpenCV</strong>: Know <code>cv2.VideoCapture()</code>, optical flow, trackers</li></ul><ul id="4a806b58-39f7-441b-b3cf-c04c0c4cc554" class="bulleted-list"><li style="list-style-type:disc"><strong>MediaPipe</strong>: Use pre-trained models for hand/object detection</li></ul><ul id="391471f2-b173-45d6-88d9-c70e6a963f26" class="bulleted-list"><li style="list-style-type:disc"><strong>Tracking vs Detection</strong>: Tracking = follow same object; Detection = locate per frame</li></ul><ul id="174f1933-b10c-447d-bb65-46ace21092ba" class="bulleted-list"><li style="list-style-type:disc"><strong>Gesture Recognition</strong>: Relies on hand landmark + classifier</li></ul><ul id="a18353f9-2a2a-415d-85be-4a3c706fe40a" class="bulleted-list"><li style="list-style-type:disc"><strong>Object Detection</strong>: EfficientDet â†’ Edge-optimized detection</li></ul><ul id="a8a48e49-1245-4585-bfbd-bc8ef897040f" class="bulleted-list"><li style="list-style-type:disc"><strong>Summarization</strong>: Filter/store frames by detection label</li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Week 5 Deep Learning on Edge</summary><div class="indented"><p id="f11242c8-da67-4916-9151-ec3c7700bc89" class=""><strong>Real-time Inference of Deep Learning models on Edge Device</strong></p><p id="90beeb90-92f4-46fd-9942-33a44792eac4" class=""><strong>Objective:</strong> By the end of this session, participants will understand</p><ol type="1" id="1634e072-799a-4686-98ea-1d3407ef8f53" class="numbered-list" start="1"><li>How to run a deep learning model on an edge device (aka raspberryPi)</li></ol><ol type="1" id="b0cb1080-9304-4013-8cb7-9b2afbb98bab" class="numbered-list" start="2"><li>How does quantization work and different quantization methods for deep learning models</li></ol><hr id="22d488c6-82a6-4903-9835-efc7ff861cfc"/><p id="6fcdb545-b04c-4470-99a5-30bea7d75726" class=""><strong>Prerequisites:</strong></p><ol type="1" id="db6056eb-c039-44e6-9448-b57abfc342f3" class="numbered-list" start="1"><li>Raspberry Pi with Raspbian OS installed.</li></ol><ol type="1" id="35d4f65c-9c64-45d4-a51b-77bebb1c8851" class="numbered-list" start="2"><li>MicroSD card (16GB or more recommended).</li></ol><ol type="1" id="04befc6b-6927-415f-ba93-0fbd773feb89" class="numbered-list" start="3"><li>Web camera compatible with Raspberry Pi.</li></ol><ol type="1" id="b990bf91-6494-431d-9f07-44bdb612f46e" class="numbered-list" start="4"><li>Internet connectivity (Wi-Fi or Ethernet).</li></ol><ol type="1" id="11ba7f82-20e6-4484-b5ab-eb1d15e300cb" class="numbered-list" start="5"><li>Basic knowledge of Python and Linux commands.</li></ol><hr id="3cbaa8b5-9ce4-4e9e-9e48-3e1d13919533"/><p id="45134e53-cefc-4744-a197-cec183451d09" class=""><strong>1. Introduction</strong></p><p id="56345337-3240-4c5b-b377-ee634126231e" class="">Edge analytics with real-time processing capabilities is chellenging but important and inevitable due to privacy/security concerns. However, edge devices like RaspberryPi are constrained with limited hardware resources, which at times are not sufficient to run complex deep learning models. These models require lot of computational resource and memory due to their size and complex architecture. Therefore, in such scenarios, we optimize the model such that it can run efficiently with reduced inference time critical for real-time analytics. Optimization can be achieved by combination of techniques like quantization and converting trained model into architecture specific lite model.</p><p id="18544268-bbd6-44ac-b994-ea10e317583b" class=""><strong>2. Running Deep Learning Model On RaspberryPi</strong></p><ul id="1e67ad14-0e1c-45c2-be95-f3b377f9cfe1" class="bulleted-list"><li style="list-style-type:disc"><strong>This section guide you on how to setup a Raspberry Pi for running PyTorch and deploy a MobileNet v2 image classification model in real time on the CPU.</strong></li></ul><ul id="2288a1a0-b5e1-4e8f-8bed-d2fc7818d8d8" class="bulleted-list"><li style="list-style-type:disc">Set up and activate a virtual environment named &quot;dlonedge&quot; for this experiment (to avoid conflicts in libraries) as below:</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="abb11c86-8849-44f1-9b00-55beb053a9ee" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt install python3-venv
python3 -m venv dlonedge
source dlonedge/bin/activate</code></pre><ul id="8cd36a63-3597-4743-82c7-0fa69e76bd85" class="bulleted-list"><li style="list-style-type:disc">Installing PyTorch and OpenCV:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="b6f894fc-b07e-45bf-8259-8cb8a5ce1c96" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install torch torchvision torchaudio
pip install opencv-python
pip install numpy --upgrade</code></pre></li></ul><ul id="00dd9ec0-03b5-4f99-894b-db5a60d62cbd" class="bulleted-list"><li style="list-style-type:disc">Same as last lab, for video capture weâ€™re going to be using OpenCV to stream the video frames. The model we are going to use in this lab is MobileNetV2, which takes in image sizes of 224x224. We are targeting 30fps for the model but we will request a slightly higher framerate of 36 fps than that so there is always enough frames and bandwidth of image pre-processing and model prediction.</li></ul><ul id="7acf8101-6108-400c-8457-7b5f316e6a67" class="bulleted-list"><li style="list-style-type:disc"><strong>Part 1.</strong> <a href="https://www.notion.so/Codes/mobile_net.py">sample code</a> is used to directly load pre-trained MobileNetV2 model, doing model inference and finally, Observe the fps as shown in screenshot below when run on RaspberryPi 4B. As shown, with no optimization of model, we could only achieve of 5-6 fps much below our desired target.<figure id="35958573-ffe5-47f6-aefb-ef26bc62ae28" class="image"><a href="https://github.com/user-attachments/assets/8e3cf302-45f3-41c9-85a5-a1bd118d30c4"><img src="https://github.com/user-attachments/assets/8e3cf302-45f3-41c9-85a5-a1bd118d30c4"/></a></figure><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80ea-9af5-ffa4d118e0ab" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import time

import torch
import numpy as np
from torchvision import models, transforms
from torchvision.models.quantization import MobileNet_V2_QuantizedWeights

import cv2
from PIL import Image

quantize = False

if quantize:
    torch.backends.quantized.engine = &#x27;qnnpack&#x27;


cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)
cap.set(cv2.CAP_PROP_FPS, 36)

preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

weights = MobileNet_V2_QuantizedWeights.DEFAULT
classes = weights.meta[&quot;categories&quot;]
net = models.quantization.mobilenet_v2(pretrained=True, quantize=quantize)

started = time.time()
last_logged = time.time()
frame_count = 0

with torch.no_grad():
    while True:
        # read frame
        ret, image = cap.read()
        if not ret:
            raise RuntimeError(&quot;failed to read frame&quot;)

        # convert opencv output from BGR to RGB
        image = image[:, :, [2, 1, 0]]
        permuted = image

        # preprocess
        input_tensor = preprocess(image)

        # create a mini-batch as expected by the model
        input_batch = input_tensor.unsqueeze(0)

        # run model
        output = net(input_batch)


        # Uncomment below 5 lines to print top 10 predictions
        #top = list(enumerate(output[0].softmax(dim=0)))
        #top.sort(key=lambda x: x[1], reverse=True)
        #for idx, val in top[:10]:
        #    print(f&quot;{val.item()*100:.2f}% {classes[idx]}&quot;)
        #print(f&quot;========================================================================&quot;)
        
        # log model performance
        frame_count += 1
        now = time.time()
        if now - last_logged &gt; 1:
            print(f&quot;============={frame_count / (now-last_logged)} fps =================&quot;)
            last_logged = now
            frame_count = 0
</code></pre></li></ul><ul id="07944975-7ab3-4a42-b5f0-ce252b4a328b" class="bulleted-list"><li style="list-style-type:disc"><strong>Part 2.</strong> Edit line number 11 as shown below to enable quantization in <a href="https://www.notion.so/Codes/mobile_net.py">sample code</a> to use quantized version of MobileNetV2 model.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a66584be-7c16-40dd-8694-0290edcfc6cb" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">quantize = True
</code></pre><p id="d6ee0915-bbaf-4f5c-a554-1df8e4f8221c" class="">Finally, observe the fps as shown in screenshot below after using quantized model of MobileNetV2. We can now achieve close to 30 fps as required because of smaller footprint of quantized model.</p><figure id="bc0faa00-9cf3-44bb-a71d-f0821e503773" class="image"><a href="https://github.com/user-attachments/assets/7086f300-4edf-4c41-a799-c496001ee1d1"><img src="https://github.com/user-attachments/assets/7086f300-4edf-4c41-a799-c496001ee1d1"/></a></figure><p id="f8285b61-0671-4ada-8e10-5fc4b020bc92" class=""><a href="https://pytorch.org/docs/stable/quantization.html">Quantization</a> techniques enable computations and tensor storage at reduced bitwidths compared to floating-point precision. In a quantized model, some or all operations use this lower precision, resulting in a smaller model size and the ability to leverage hardware-accelerated vector operations.</p></li></ul><ul id="867676a1-3b9d-45fc-8c9d-060665025e06" class="bulleted-list"><li style="list-style-type:disc"><strong>Part 3.</strong> Uncomment lines 57-61 in <a href="https://www.notion.so/Codes/mobile_net.py">sample code</a> to print the top 10 predictions in real-time as shown in below video.<p id="98561e53-e70a-45bd-85db-747a72be8207" class=""><a href="https://github.com/user-attachments/assets/5ee2a4c8-1988-4021-b194-aa0786a1ebfc">https://github.com/user-attachments/assets/5ee2a4c8-1988-4021-b194-aa0786a1ebfc</a></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80bb-bc59-f513897c473a" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">#!/usr/bin/env python3
&quot;&quot;&quot;
Top-Level Explanation:
This script demonstrates real-time image classification using a pre-trained MobileNet_V2 model from PyTorch.
It captures frames from a webcam using OpenCV, preprocesses the images to match the model&#x27;s input requirements,
and performs inference to classify the image. The code can run either in a standard or quantized mode for efficiency.
It logs the model&#x27;s performance in frames per second (fps). This demo is useful for understanding neural network
inference, model quantization, and real-time computer vision applications in Python.
&quot;&quot;&quot;

import time  # For timing and performance measurement.

import torch  # PyTorch library for deep learning.
import numpy as np  # NumPy for numerical operations and array handling.
from torchvision import models, transforms  # Pre-trained models and image transforms.
from torchvision.models.quantization import MobileNet_V2_QuantizedWeights  # Quantized weights for MobileNet V2.

import cv2  # OpenCV for video capture and image processing.
from PIL import Image  # PIL for image handling if needed (not used directly in this script).

# Flag to decide whether to run in quantized mode or not.
quantize = False

# If quantization is enabled, set the quantization engine.
if quantize:
    torch.backends.quantized.engine = &#x27;qnnpack&#x27;
    # Q: What is quantization in neural networks?
    # A: Quantization reduces the precision of the weights and activations, leading to faster inference and reduced memory usage.

# Open the default webcam.
cap = cv2.VideoCapture(0)
# Set the frame dimensions to 224x224 pixels, which is the standard input size for MobileNet.
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)
# Set the desired frames per second.
cap.set(cv2.CAP_PROP_FPS, 36)
# Q: Why is it important to set the frame size to 224x224?
# A: The MobileNet_V2 model expects 224x224 input images, so we need to match this dimension for proper inference.

# Define the preprocessing transformations.
preprocess = transforms.Compose([
    transforms.ToTensor(),  # Convert the image to a PyTorch tensor.
    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize using ImageNet&#x27;s mean...
                         std=[0.229, 0.224, 0.225]),   # ...and standard deviation.
])
# Q: Why normalize the image?
# A: Normalization scales the pixel values to a standard range, matching the conditions the model was trained under.

# Load the default quantized weights for MobileNet_V2 if quantization is enabled.
weights = MobileNet_V2_QuantizedWeights.DEFAULT
classes = weights.meta[&quot;categories&quot;]  # Retrieve class names from the model metadata.

# Load the pre-trained MobileNet_V2 model; use quantization if specified.
net = models.quantization.mobilenet_v2(pretrained=True, quantize=quantize)
# Q: What is MobileNet_V2?
# A: MobileNet_V2 is a lightweight convolutional neural network optimized for mobile and embedded vision applications.

# Variables for performance measurement.
started = time.time()
last_logged = time.time()
frame_count = 0

# Disable gradient computation for inference (saves memory and computation).
with torch.no_grad():
    while True:
        # Read a frame from the webcam.
        ret, image = cap.read()
        if not ret:
            raise RuntimeError(&quot;failed to read frame&quot;)
        
        # Convert the OpenCV image from BGR to RGB.
        image = image[:, :, [2, 1, 0]]
        # Q: Why convert from BGR to RGB?
        # A: OpenCV uses BGR by default, but the model expects images in RGB format.
        permuted = image (line deleted)

        # Preprocess the image: convert to tensor and normalize.
        input_tensor = preprocess(image)
        
        # Create a mini-batch (add batch dimension) as expected by the model.
        input_batch = input_tensor.unsqueeze(0)
        
        # Run the model on the input batch.
        output = net(input_batch)
        
        # Uncomment the following lines to print top 10 predictions:
        # top = list(enumerate(output[0].softmax(dim=0)))
        # top.sort(key=lambda x: x[1], reverse=True)
        # for idx, val in top[:10]:
        #     print(f&quot;{val.item()*100:.2f}% {classes[idx]}&quot;)
        # print(f&quot;========================================================================&quot;)
        # Q: What does softmax do in this context?
        # A: Softmax converts the raw model outputs (logits) into probabilities, making it easier to interpret predictions.

        # Log the model&#x27;s performance by calculating frames per second (fps).
        frame_count += 1
        now = time.time()
        if now - last_logged &gt; 1:
            print(f&quot;============={frame_count / (now - last_logged)} fps =================&quot;)
            last_logged = now
            frame_count = 0

        # Optional: Add code here to display the frame or further process the output.
        # For example, use cv2.imshow(&quot;Frame&quot;, image) if you wish to see the live video.

        # Exit condition could be added here if integrating with cv2.imshow (e.g., if cv2.waitKey(1) == ord(&#x27;q&#x27;): break)</code></pre></li></ul><p id="da9b6734-a606-4d2a-be1e-ad43316325b3" class=""><strong>3. Quantization using Pytorch</strong></p><ul id="5b70f468-fd47-4e2f-a831-abac8dac89b9" class="bulleted-list"><li style="list-style-type:disc">Neural networks typically use 32-bit floating point precision for activations, weights, and computations. Quantization reduces this precision to smaller data types (like 8-bit integers), decreasing memory and speeding up computation. This compression is not lossless, as lower precision sacrifices dynamic range and resolution. Thus, a balance must be struck between model accuracy and the efficiency gains from quantization.</li></ul><ul id="c651354a-a4ad-496c-b345-99583dfd7733" class="bulleted-list"><li style="list-style-type:disc">In this section, we would learn how to use Pytorch to perform different quantization methods on a neural network architecture. There are two ways in general to quantize a deep learning model:<ol type="1" id="b651c9f5-9e6e-4e45-ae96-390583a7b31d" class="numbered-list" start="1"><li>Post Training Quantization: After we have a trained model, we can convert the model to a quantized model by converting 32 bit floating point weights and activations to 8 bit integer, but we may see some accuracy loss for some types of models.</li></ol><ol type="1" id="eaec0459-7110-4ed1-9bc2-d9650f27b3bd" class="numbered-list" start="2"><li>Quantization Aware Training: During training, we insert fake quantization operators into the model to simulate the quantization behavior and convert the model to a quantized model after training based on the model with fake quantize operators. This is harder to apply than post-training quantization since it requires retraining the model, but typically gives better accuracy.</li></ol></li></ul><ul id="b5dbf27e-1033-4d64-bdf5-a27ba93c84a2" class="bulleted-list"><li style="list-style-type:disc">Please refer to <a href="https://www.notion.so/Codes/PyTorch_Quantisation.ipynb">sample jupyter notebook file</a>, which demonstrates how to quantized a pre-trained model using post-training quantization approach as well as quantization aware training. Please run the sample code preferably in google colab if you do not have computer with good hardware specs.</li></ul><ul id="1bfaecc5-ba04-8033-9119-ffce30418992" class="bulleted-list"><li style="list-style-type:disc">refer to this <a href="https://github.com/munir2200963/edge_labs/blob/main/deeplearning/PyTorch_Quantisation.ipynb">https://github.com/munir2200963/edge_labs/blob/main/deeplearning/PyTorch_Quantisation.ipynb</a></li></ul><hr id="1bfaecc5-ba04-80c3-aa40-e408d4334ef6"/><h3 id="22662203-1a4c-4bde-a873-1d8b9b1bd598" class="">ðŸ§  <strong>Running Deep Learning Model on Pi (Real-time Inference)</strong></h3><h3 id="2aaacdac-c01f-4146-8ae1-ad10f8bb6b3f" class="">ðŸ‘ï¸ MobileNetV2 for Image Classification</h3><ul id="00a47c37-8c49-4c74-8921-0b1d00c02cbf" class="bulleted-list"><li style="list-style-type:disc">Use webcam + OpenCV to feed live frames (224x224) into a MobileNetV2 model.</li></ul><ul id="33540f4f-cfc7-443a-95fe-3423fbec7e04" class="bulleted-list"><li style="list-style-type:disc">Capture and preprocess frames â†’ pass into model â†’ display predictions.</li></ul><h3 id="bf7aa66d-005c-446d-afcd-113d31ff6f2f" class="">FPS Observation:</h3><ul id="fd711937-dfc9-4463-9888-6009745607b8" class="bulleted-list"><li style="list-style-type:disc">Without quantization: ~5â€“6 FPS</li></ul><ul id="745a1553-f09a-453e-9892-5bb56b4b021b" class="bulleted-list"><li style="list-style-type:disc">With quantization: ~30 FPS âœ…<blockquote id="3abf183f-1c8b-4b52-9050-541d35b5b200" class="">Enable quantization in code by setting:</blockquote><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="918b1f22-de23-463f-a228-12428f09ff18" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">quantize = True</code></pre></li></ul><h3 id="324687d6-eb73-48af-b8fd-e59ef9d8a4dc" class="">ðŸ›  Key Steps in Code</h3><ul id="c7c1ecdb-d4d7-4c2e-a592-507389b03ff8" class="bulleted-list"><li style="list-style-type:disc">Resize webcam feed to 224x224</li></ul><ul id="b4462a42-cc42-4763-93b2-554b66602bdd" class="bulleted-list"><li style="list-style-type:disc">Normalize images to match ImageNet stats</li></ul><ul id="0a359caa-ee37-4ed1-8fad-91b2b85e344d" class="bulleted-list"><li style="list-style-type:disc">Use <code>net = models.quantization.mobilenet_v2(pretrained=True, quantize=True)</code></li></ul><ul id="bc9d4b53-579d-4e3d-9a13-5919cdb880b2" class="bulleted-list"><li style="list-style-type:disc">Convert logits to probabilities using <code>.softmax(dim=0)</code></li></ul><hr id="1bfaecc5-ba04-80d3-b156-c0fa09990e48"/><h3 id="7b2df56a-a6f0-47a9-90d1-5c00c5fe0efd" class="">âš¡ What is Quantization?</h3><blockquote id="a63236ac-c043-4276-a300-d6ce31f8d36c" class="">Quantization = Compressing model by reducing bit-precision of weights &amp; activations (from FP32 to INT8)</blockquote><h3 id="a401f1d1-3b90-471a-b1f1-e0241ce69c1d" class="">ðŸ”¸ Two Main Types:</h3><table id="1bfaecc5-ba04-8098-8f12-f73be277b925" class="simple-table"><thead class="simple-table-header"><tr id="d445c2fe-afa2-4cd4-a672-3fa0150da2a0"><th id="LYr]" class="simple-table-header-color simple-table-header">Method</th><th id="?vCu" class="simple-table-header-color simple-table-header">Description</th><th id=":BJz" class="simple-table-header-color simple-table-header">Pros</th><th id="_vZy" class="simple-table-header-color simple-table-header">Cons</th></tr></thead><tbody><tr id="7f055eaa-a478-4b4a-8b66-8909426b18b5"><td id="LYr]" class="">Post-Training Quantization</td><td id="?vCu" class="">Apply quantization after training</td><td id=":BJz" class="">Simple</td><td id="_vZy" class="">Some accuracy loss</td></tr><tr id="2a1a669b-95f1-4df6-8457-0c13d37d6b1f"><td id="LYr]" class="">Quantization Aware Training (QAT)</td><td id="?vCu" class="">Simulate quantization during training</td><td id=":BJz" class="">Higher accuracy</td><td id="_vZy" class="">Requires retraining</td></tr></tbody></table><p id="9b4bc8c4-d2c1-4edf-ba6a-8c4f1930a53b" class=""><strong>4. Homework and Optional Exercise</strong></p><ul id="768f4875-3b25-438c-ad9b-9ce22916fbb6" class="bulleted-list"><li style="list-style-type:disc">Try running quantized version of some large language models (like llama, mixtral etc.) on Raspberry Pi. This <a href="https://www.dfrobot.com/blog-13498.html">link</a> demonstrates some of the LLMs on Raspberry Pi 4 and 5.<p id="557091a5-2763-44b8-b3b7-9934c16bd95e" class=""><strong>Objective:</strong> Deploy lightweight Large Language Models on Raspberry Pi using quantized versions.</p><h3 id="706f1049-18ca-488b-aa39-45b860e802ad" class="">ðŸ”§ Tools to Try:</h3><ul id="5ee2cfc9-6a32-4d5c-9dc4-e32a0d1d230d" class="bulleted-list"><li style="list-style-type:circle"><code><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></code>: C++ inference of LLaMA on CPU (supports quantization: INT4, INT8)</li></ul><ul id="9243c491-55f7-41a3-bd6f-8fa6b181b0f7" class="bulleted-list"><li style="list-style-type:circle"><code><a href="https://gpt4all.io/">gpt4all</a></code>: LLMs optimized for local devices</li></ul><ul id="428960ff-c0a1-42e8-9fb6-b7e3d199c59c" class="bulleted-list"><li style="list-style-type:circle"><code><a href="https://ollama.ai/">Ollama</a></code>: Local LLM runner that supports Raspberry Pi 5 (via Ubuntu ARM64)</li></ul><h3 id="f27eeeb2-bb1a-47d0-97d8-0e563dc149a9" class="">âœ… Steps:</h3><ol type="1" id="4280cb30-3240-42cb-8ca7-30960572ae09" class="numbered-list" start="1"><li>Clone and build <code>llama.cpp</code> on Raspberry Pi:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e2ea8d4f-18be-4305-a15b-0c5d85c3e76d" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make</code></pre></li></ol><ol type="1" id="67305112-ecaf-4e2a-a791-5c01a2abe005" class="numbered-list" start="2"><li>Download a <strong>quantized model</strong> (like <code>tinyllama-1.1B-chat.Q4_K_M.gguf</code>)<blockquote id="68461b47-f3f3-47f0-b46c-aad6b524bcf1" class="">Quantized models are much smaller and optimized for CPU inference.</blockquote></li></ol><ol type="1" id="0b6cd093-32f6-4f15-af18-0ab624736637" class="numbered-list" start="3"><li>Run interactive chat:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="44603898-dfde-4e66-b71c-75beb75e452c" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">./main -m models/tinyllama.gguf -p &quot;What is Raspberry Pi?&quot; -n 100</code></pre></li></ol><h3 id="c883a620-715a-43fc-86a7-805ee4d7646d" class="">ðŸ“Š What to Observe:</h3><ul id="08138bc4-1e89-4635-896e-aad716c460f6" class="bulleted-list"><li style="list-style-type:circle">Model size (e.g., TinyLLaMA: ~500MB with INT4)</li></ul><ul id="8625dd43-261a-49ab-818a-9d48d50f2b8b" class="bulleted-list"><li style="list-style-type:circle">Inference time (tokens/sec)</li></ul><ul id="91f7f0e4-ee13-4c9a-b2d6-627df7301525" class="bulleted-list"><li style="list-style-type:circle">RAM usage (should be &lt;4GB)</li></ul><ul id="4cfbca90-088b-4758-b873-3c9b5a35624c" class="bulleted-list"><li style="list-style-type:circle">CPU load (watch <code>htop</code>)</li></ul><h3 id="fd6dc223-6dc2-4c46-9be7-01751486f0e5" class="">ðŸ” Result:</h3><p id="0f1fcc9f-b523-48cf-98c1-fb29b4546cab" class="">Raspberry Pi can run <strong>basic chat models</strong> like TinyLLaMA or LLaMA2-7B (with huge latency). Use Pi 5 for better performance or offload inference using NPU (like Coral).</p></li></ul><ul id="bc31b427-65ed-4c01-9a7b-cba1d075f422" class="bulleted-list"><li style="list-style-type:disc">Take any complex Deep Learning Model like resnet, mobileNet OR your own architecure and try different quantization methods as explained in section 3. Once deployed on RaspberryPi, Observe the size, performance and speed of the quantized model.<p id="4c3b44c7-46bc-4152-8c74-03ab1602a3e9" class=""><strong>Objective:</strong> Apply different quantization methods on a complex model and compare performance.</p><h3 id="2799a512-0ff8-411c-8885-f4a78578514b" class="">âœ… Steps:</h3><ol type="1" id="a28e9a17-e3cd-4b1c-b0f6-fe1e6cff7842" class="numbered-list" start="1"><li>Choose a model (e.g. ResNet18, MobileNetV2, or your own trained PyTorch model).</li></ol><ol type="1" id="8b25c1f9-842e-423e-a52d-4d5c6a073e0e" class="numbered-list" start="2"><li>Apply <strong>Post-Training Quantization</strong> (easy)<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="6f6addd0-dfe7-40c4-aa91-30bb99ffd779" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import torch.quantization
model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)</code></pre></li></ol><ol type="1" id="2b584aa6-05ab-4d10-b777-ddda904a836f" class="numbered-list" start="3"><li>(Optional) Apply <strong>Quantization Aware Training (QAT)</strong> (more accurate but needs retraining)<ul id="383e6418-2686-480d-9675-670c3e2c61cb" class="bulleted-list"><li style="list-style-type:disc">Use PyTorchâ€™s <code>prepare_qat</code> and retrain the model.</li></ul><ul id="14ecfb8c-5c76-4d5e-8803-fa0b61ea9598" class="bulleted-list"><li style="list-style-type:disc">Refer to this notebook:<p id="52b8e683-6683-4050-8d7e-f897cdcd62aa" class=""><a href="https://github.com/munir2200963/edge_labs/blob/main/deeplearning/PyTorch_Quantisation.ipynb">https://github.com/munir2200963/edge_labs/blob/main/deeplearning/PyTorch_Quantisation.ipynb</a></p></li></ul></li></ol><ol type="1" id="71f41609-281e-4b8e-a7e5-82f43555817e" class="numbered-list" start="4"><li>Convert model to <code>.pt</code> and run it on Raspberry Pi (e.g., load into the webcam classifier).</li></ol><ol type="1" id="1bfaecc5-ba04-80ac-9e50-e35810e6cf10" class="numbered-list" start="5"><li></li></ol><figure id="1bfaecc5-ba04-8003-850c-c30c57702eee" class="image"><a href="image.png"><img style="width:624px" src="image.png"/></a></figure><h3 id="f18455fd-4064-49c6-9174-83cc47c001a8" class="">âœ¨ Tip:</h3><p id="6c7a3671-44de-4ce9-92b9-e9a43bb96f41" class="">Use <code>torch.utils.benchmark.Timer()</code> to measure precise latency during testing.</p><h3 id="b427274b-1cc6-477b-981e-29c99c57ca4b" class="">ðŸ”¹ <strong>3. BONUS: Run Quantized Model on Live Webcam Feed (Advanced)</strong></h3><ul id="31312b45-576c-4f1f-a217-463bc7eeb2dd" class="bulleted-list"><li style="list-style-type:circle">Take your quantized model from above.</li></ul><ul id="e3c98136-e582-484a-ba62-46c61de1e797" class="bulleted-list"><li style="list-style-type:circle">Plug into the OpenCV real-time classification pipeline (like in the lab).</li></ul><ul id="323c2e70-60d0-415a-8d92-dc17866dd584" class="bulleted-list"><li style="list-style-type:circle">Log FPS and overlay prediction text on video using <code>cv2.putText</code>.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="267d0ae8-7fd9-4c66-94fc-c75042b86340" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">cv2.putText(image, f&quot;{label} ({confidence:.2f})&quot;, (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)</code></pre><h3 id="5c30d1f9-1d1b-4bcb-9826-60708d679f74" class="">ðŸ§  Final Insights for Quiz / Lab Reporting</h3><ul id="3bdae71c-d6a7-47ac-bd0e-4f1d092b8bed" class="bulleted-list"><li style="list-style-type:circle"><strong>Quantization helps edge devices</strong> by:<ul id="4cd820ff-9f7d-451b-a0f5-ed4025d3a3ea" class="bulleted-list"><li style="list-style-type:square">Reducing model size</li></ul><ul id="d50df55a-d8e6-476a-bcab-7c68165bdbe1" class="bulleted-list"><li style="list-style-type:square">Speeding up inference</li></ul><ul id="03b213b4-15fc-41e6-85fe-9b08e6b50643" class="bulleted-list"><li style="list-style-type:square">Lowering memory &amp; power usage</li></ul></li></ul><ul id="8baa35a0-0eb7-4213-a233-c44fc24bd389" class="bulleted-list"><li style="list-style-type:circle"><strong>Trade-off</strong>: Slight accuracy drop depending on the quantization method used</li></ul><ul id="3ef0f80a-d319-441b-b06f-74e253d93a27" class="bulleted-list"><li style="list-style-type:circle"><strong>Best Practice</strong>: Try Post-Q first, then QAT if accuracy drops too much</li></ul><ul id="80862f50-6d8f-4df4-8b96-682ae1dcf501" class="bulleted-list"><li style="list-style-type:circle"><strong>Compare</strong> before &amp; after:<ul id="89b22bbc-aafa-462e-b112-760e3885601d" class="bulleted-list"><li style="list-style-type:square">FPS (on Raspberry Pi)</li></ul><ul id="800def55-1149-452b-a2af-10e4cc00487d" class="bulleted-list"><li style="list-style-type:square">Model <code>.pt</code> or <code>.onnx</code> file size</li></ul><ul id="33dfa805-ca27-43b3-a845-9d146be30885" class="bulleted-list"><li style="list-style-type:square">CPU usage (<code>htop</code> or <code>time</code> command)</li></ul></li></ul></li></ul></div></details><details open=""><summary style="font-weight:600;font-size:1.25em;line-height:1.3;margin:0">Week 8 MQTT</summary><div class="indented"><h2 id="1bfaecc5-ba04-8080-9ba8-f38cd6c56935" class=""><strong>IoT Communications: MQTT</strong></h2><p id="1bfaecc5-ba04-804c-b2ec-c0a84da284bb" class=""><strong>Objective:</strong> To learn how to install and configure an MQTT broker, create MQTT publisher and subscriber clients, and test communication between them on a Raspberry Pi 400.</p><p id="1bfaecc5-ba04-80aa-8300-d8fb8c3e4eef" class=""><strong>Prerequisites:</strong></p><ol type="1" id="1bfaecc5-ba04-80db-b131-e03a2e73679a" class="numbered-list" start="1"><li>Raspberry Pi 400 with an operating system (e.g., Raspbian) already set up.</li></ol><ol type="1" id="1bfaecc5-ba04-8073-8293-e6277cb9be33" class="numbered-list" start="2"><li>Internet connection for the Raspberry Pi.</li></ol><p id="1bfaecc5-ba04-80df-8a54-d2c4d3283324" class=""><strong>Materials:</strong></p><ol type="1" id="1bfaecc5-ba04-8000-87c6-ca7a17af840a" class="numbered-list" start="1"><li>Raspberry Pi 400.</li></ol><ol type="1" id="1bfaecc5-ba04-8059-bd7e-e5518442938b" class="numbered-list" start="2"><li>Power supply for Raspberry Pi.</li></ol><ol type="1" id="1bfaecc5-ba04-8050-aa26-c2a6336975c1" class="numbered-list" start="3"><li>Keyboard, mouse, and monitor for Raspberry Pi (if not using SSH).</li></ol><ol type="1" id="1bfaecc5-ba04-8054-85fb-c55ecc395b2d" class="numbered-list" start="4"><li>MQTT broker (e.g., Mosquitto).</li></ol><ol type="1" id="1bfaecc5-ba04-8078-93c8-c9e62c483143" class="numbered-list" start="5"><li>MQTT publisher and subscriber clients (e.g., Python Paho MQTT library).</li></ol><p id="1bfaecc5-ba04-80f1-8ce9-ede083f4a512" class=""><strong>Introduction:</strong><br/>MQTT stands for Message Queue Telemetry Transport. It is a publish/subscribe messaging transport protocol that is lightweight, open, and designed to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in the Internet of Things (IoT) contexts where a small code footprint is required and network bandwidth is scarce. The protocol runs over TCP/IP, or over other similar network protocols that provide ordered, lossless, bidirectional connections.<br/></p><p id="1bfaecc5-ba04-8026-8975-c827656e5db9" class="">MQTT can be broken down into the following components:</p><p id="1bfaecc5-ba04-80d3-b4c0-ea9bad09acbf" class=""><strong>MQTT Broker:</strong> The broker accepts messages from clients (publishers) and then delivers them to any interested clients (subscribers). Each message must belong to a specific topic. The broker is a program running on a device that acts as an intermediary between clients who publish messages and clients who have made subscriptions.</p><p id="1bfaecc5-ba04-80bd-b0e4-ce6edf3b6a7b" class=""><strong>Topic:</strong> A namespace (or place) for messages on the broker. Clients must subscribe to and/or publish to a topic.</p><p id="1bfaecc5-ba04-80e1-8d50-e70cbb49bac0" class=""><strong>MQTT Client:</strong> The client is a â€˜deviceâ€™ that either publishes a message to a specific topic or subscribes to a topic, or both. A publisher is a client that sends a message to the broker, using a topic name. While a subscriber informs the broker which topics it is interested in. Once subscribed, the broker sends messages published to that topic. A client can subscribe to multiple topics. However, a client must first establish a connection with the broker and can take the following actions:<br/>â€¢    Publish messages that other clients might be interested in (subscribed to).<br/>â€¢    Subscribe to a specific topic to receive messages from the publishers.<br/>â€¢    Unsubscribe to remove a request for the messages.<br/>â€¢    Disconnect from the Broker.<br/></p><figure id="1bfaecc5-ba04-80cc-bdf9-d029f4d07580" class="image"><a href="https://github.com/drfuzzi/INF2009_MQTT/assets/108112390/26517ab1-d700-48cd-bfbd-4d7511ecfc9a"><img src="https://github.com/drfuzzi/INF2009_MQTT/assets/108112390/26517ab1-d700-48cd-bfbd-4d7511ecfc9a"/></a></figure><p id="1bfaecc5-ba04-8066-bc0a-e18b5ae1f843" class="">Figure 1: An example of MQTT implementation</p><p id="1bfaecc5-ba04-80be-9c80-e5f52c41f045" class=""><strong>Lab Exercise:</strong></p><p id="1bfaecc5-ba04-80a4-ab91-e242a67ca22f" class=""><strong>1. Install and Configure the MQTT Broker on a Raspberry Pi 400:</strong></p><p id="1bfaecc5-ba04-8038-9259-d5af8805240b" class="">a. Update the Raspberry Pi&#x27;s package list (optional):</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80d8-bc67-ccead31585f1" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt update</code></pre><p id="1bfaecc5-ba04-80c5-8ef6-eeac842b00f8" class="">b. Install the Mosquitto MQTT broker:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80d0-9f4f-ee23297c37e3" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo apt install mosquitto</code></pre><p id="1bfaecc5-ba04-808e-a34e-f6b3123727e8" class="">c. Locate the Mosquitto Configuration File:</p><ul id="1bfaecc5-ba04-8049-9ff3-fcef0d352fae" class="bulleted-list"><li style="list-style-type:disc">Use a text editor to open the <code>mosquitto.conf</code> file located in <code>/etc/mosquitto/</code> using the following command:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80e1-8626-cb7042ac4f8c" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo nano /etc/mosquitto/mosquitto.conf</code></pre></li></ul><p id="1bfaecc5-ba04-800a-beab-c30927ed1314" class="">d. Edit the Mosquitto Configuration file:</p><ul id="1bfaecc5-ba04-801f-b200-f66f70a9f3f9" class="bulleted-list"><li style="list-style-type:disc">Inside the <code>mosquitto.conf</code> file, you&#x27;ll find various configuration options. Be careful when editing this file to avoid introducing errors.</li></ul><ul id="1bfaecc5-ba04-8007-8a63-e1b0975b86c5" class="bulleted-list"><li style="list-style-type:disc">Include the following into the configuration file to allow the brokker to listen to port 1883 and allow anonymous clients to connect and use the MQTT broker which means that clients can connect without providing a username and password:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8069-a574-fcb6419a0292" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">listener 1883
allow_anonymous true</code></pre></li></ul><p id="1bfaecc5-ba04-800c-aabd-c3f38c3ca856" class="">e. Start your mosquitto broker manually:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-808d-938c-cb7b87fbb298" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo mosquitto -c /etc/mosquitto/mosquitto.conf</code></pre><p id="1bfaecc5-ba04-8073-9875-f9afbea77e70" class=""><strong>2. Enable Mosquitto Broker to run on boot (optional)</strong></p><p id="1bfaecc5-ba04-8026-aa57-c45305f88081" class="">a. Start and enable Mosquitto to run on boot:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-805d-be2e-ddff9c564659" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo systemctl start mosquitto
sudo systemctl enable mosquitto</code></pre><p id="1bfaecc5-ba04-80dc-8b2a-c3ddd923c89c" class="">b. Restarting Mosquitto Broker to apply the new configuration by using the following command:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8052-8cee-c0d8e85052a4" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo systemctl restart mosquitto</code></pre><p id="1bfaecc5-ba04-8042-bf3c-fb49f3463145" class="">c. Verify that Mosquitto is running:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80e7-a7c6-cb649ea0acd2" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">systemctl status mosquitto</code></pre><p id="1bfaecc5-ba04-8034-a3ce-cde1757119d0" class="">d. Disable and stop the Mosquitto MQTT broker:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80f2-b01a-d6c731b6ec41" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">sudo systemctl disable mosquitto
sudo systemctl stop mosquitto</code></pre><p id="1bfaecc5-ba04-80c5-82e1-d110fbf10c93" class=""><strong>3. Install and Configure the MQTT Client (Publisher and/or Subscriber) on another Raspberry Pi 400:</strong></p><p id="1bfaecc5-ba04-80a8-9857-e0c0b9fc8074" class="">a. (Remember) to activate the Virtual Environment:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8096-b4e7-c7ff8f59e688" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">source myenv/bin/activate</code></pre><p id="1bfaecc5-ba04-803b-9983-cd9eb12f1dbd" class="">b. Install the Python Paho MQTT library for both publisher and subscriber:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80fb-8443-f0d935a64f2c" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">pip install paho-mqtt</code></pre><p id="1bfaecc5-ba04-8029-af27-e20ddcc75741" class="">c. Create a Python script for the MQTT Publisher (<code>mqtt_publisher.py</code>). Ensure the &quot;localhost&quot; is changed to the IP address of the Broker, e.g. &quot;192.168.50.115&quot;:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-807c-929e-d3073bac11d5" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import paho.mqtt.client as mqtt
import time

client = mqtt.Client(&quot;Publisher&quot;)
client.connect(&quot;localhost&quot;, 1883)

while True:
    client.publish(&quot;test/topic&quot;, &quot;Hello, MQTT!&quot;)
    time.sleep(5)</code></pre><p id="1bfaecc5-ba04-805a-ba7f-e0c1f19861d7" class="">d. Create a Python script for the MQTT Subscriber (<code>mqtt_subscriber.py</code>). Ensure the &quot;localhost&quot; is changed to the IP address of the Broker, e.g. &quot;192.168.50.115&quot;:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8013-8429-c398c38bbb15" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">import paho.mqtt.client as mqtt

def on_message(client, userdata, message):
    print(f&quot;Received message &#x27;{message.payload.decode()}&#x27; on topic &#x27;{message.topic}&#x27;&quot;)

client = mqtt.Client(&quot;Subscriber&quot;)
client.on_message = on_message
client.connect(&quot;localhost&quot;, 1883)
client.subscribe(&quot;test/topic&quot;)
client.loop_forever()</code></pre><p id="1bfaecc5-ba04-8047-b8d4-fa55d498a04a" class=""><strong>4. Testing your MQTT Communication:</strong></p><p id="1bfaecc5-ba04-80f3-b9b7-ca9684ef7807" class="">a. Open two terminal windows on the Raspberry Pi.</p><p id="1bfaecc5-ba04-803c-b631-fb3849e2da8c" class="">b. In the first terminal, run the MQTT subscriber:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-807d-a008-cf2e15f84d31" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">python3 mqtt_subscriber.py</code></pre><p id="1bfaecc5-ba04-80c5-a4a1-c1c446fade9b" class="">c. In the second terminal, run the MQTT publisher:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8006-8dd6-dec300c38c33" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">python3 mqtt_publisher.py</code></pre><p id="1bfaecc5-ba04-809f-a990-e2d3d16ee2cf" class="">d. (Remember) to activate the Virtual Environment <strong>before</strong> running the python scripts:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80c4-adce-d09c5b240467" class="code"><code class="language-Shell" style="white-space:pre-wrap;word-break:break-all">source myenv/bin/activate</code></pre><p id="1bfaecc5-ba04-8059-a763-cfaae91f6029" class="">e. Observe the messages being published and received in the subscriber terminal.</p><p id="1bfaecc5-ba04-80d9-8f2f-d55a3534a7c1" class=""><strong>Lab Assignment:</strong><br/>Create two Python scripts building upon this lab session to capture an image from a webcam when it receives a message through a subscriber topic (pick an appropriate topic) and subsequently transmit the captured image as a publisher via MQTT. This lab assignment will challenge you to combine webcam access, MQTT communication, and message handling to develop a practical IoT application. Your task is to refine the provided script, ensuring seamless image capture and MQTT integration.<br/></p><figure id="1bfaecc5-ba04-800a-8430-eeaeb07f25bb" class="image"><a href="https://github.com/drfuzzi/INF2009_MQTT/assets/108112390/bd2e0190-e973-4565-b8bb-1311d804a436"><img src="https://github.com/drfuzzi/INF2009_MQTT/assets/108112390/bd2e0190-e973-4565-b8bb-1311d804a436"/></a></figure><p id="1bfaecc5-ba04-80b9-bab2-d91bd00d93f9" class="">Figure 2: Overview of the Image Request System via MQTT</p><p id="1bfaecc5-ba04-80f1-9745-d6543c47d8e6" class="">We&#x27;ll have three key components:</p><ol type="1" id="1bfaecc5-ba04-8048-a715-f6f3df51ed32" class="numbered-list" start="1"><li><strong>Laptop (Publisher &amp; Subscriber)</strong><ul id="1bfaecc5-ba04-806a-bbbd-f97bcc22d788" class="bulleted-list"><li style="list-style-type:disc">Publishes a request to capture an image.</li></ul><ul id="1bfaecc5-ba04-802f-adf0-d2e71aa7ba77" class="bulleted-list"><li style="list-style-type:disc">Subscribes to receive the image.</li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-804b-b3af-dc203e269c22" class="numbered-list" start="2"><li>Raspberry Pi (Webcam Device - Publisher &amp; Subscriber)<ul id="1bfaecc5-ba04-804e-9988-dcfcbaa2d55e" class="bulleted-list"><li style="list-style-type:disc">Subscribes to the request.</li></ul><ul id="1bfaecc5-ba04-8057-84df-d0874c51ab46" class="bulleted-list"><li style="list-style-type:disc">Captures an image and publishes it.</li></ul></li></ol><ol type="1" id="1bfaecc5-ba04-8087-9bb5-c36050df1a2e" class="numbered-list" start="3"><li>MQTT Broker (e.g., Mosquitto)<ul id="1bfaecc5-ba04-80bb-9aa5-f8318e79a7c5" class="bulleted-list"><li style="list-style-type:disc">Forwards messages between the laptop and Raspberry Pi</li></ul></li></ol><hr id="1bfaecc5-ba04-80f3-87d6-d0b0b4bc28c7"/><p id="1bfaecc5-ba04-8038-beca-e1064fd4b945" class="">1. Laptop: Requesting Image and Receiving Image</p><figure id="1bfaecc5-ba04-80a9-99e1-cb116036a998" class="image"><a href="image%201.png"><img style="width:682px" src="image%201.png"/></a></figure><p id="1bfaecc5-ba04-809c-a663-d4a01d12bc10" class="">The laptop (<strong>192.168.2.1</strong>)<br/>â— Publishes <br/><mark class="highlight-teal">&quot;capture_image&quot;</mark> request.<br/>â— Subscribes to <br/><mark class="highlight-teal">&quot;image/data&quot;</mark> to receive the image</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-8046-9757-cc8ebdb83b4b" class="code"><code class="language-Python">import paho.mqtt.client as mqtt
import base64
import cv2
import numpy as np
import time
BROKER = &quot;192.168.2.2&quot; # Raspberry Pi&#x27;s IP Address
TOPIC_REQUEST = &quot;image/request&quot;
TOPIC_IMAGE = &quot;image/data&quot;
# Function to handle incoming images
def on_message(client, userdata, message):
&quot;&quot;&quot;Receives image and displays it.&quot;&quot;&quot;
print(&quot;ðŸ–¼ Image received. Decoding...&quot;)
# Decode Base64 image data
image_bytes = base64.b64decode(message.payload)
image_array = np.frombuffer(image_bytes, dtype=np.uint8)
img = cv2.imdecode(image_array, cv2.IMREAD_COLOR)
if img is not None:
cv2.imshow(&quot;Received Image&quot;, img)
cv2.imwrite(&quot;received_image.jpg&quot;, img) # Save the image
cv2.waitKey(3000) # Display for 3 seconds
cv2.destroyAllWindows()
print(&quot;âœ… Image saved as &#x27;received_image.jpg&#x27;.&quot;)
else:
print(&quot;âŒ Failed to decode image.&quot;)
# Set up MQTT client
client = mqtt.Client(&quot;Laptop_Client&quot;)
client.on_message = on_message
client.connect(BROKER, 1883)
# Subscribe to receive images
client.subscribe(TOPIC_IMAGE)
# Send request to capture an image
print(&quot;ðŸ“¢ Sending image request...&quot;)
client.publish(TOPIC_REQUEST, &quot;capture_image&quot;)
# Start loop to listen for the image
client.loop_forever()</code></pre><p id="1bfaecc5-ba04-80bc-afbc-fb76b15490c7" class="">
</p><p id="1bfaecc5-ba04-80a5-a735-d4bbb750a5f6" class="">2. Raspberry Pi (Web Camera Device) (<strong>192.168.2.2)</strong></p><figure id="1bfaecc5-ba04-801a-8753-e681fdf5f36b" class="image"><a href="image%202.png"><img style="width:682px" src="image%202.png"/></a></figure><p id="1bfaecc5-ba04-8088-bd75-e3a6dd2f17a4" class="">The webcam device:<br/>â— Subscribes to <br/><mark class="highlight-teal">&quot;image/request&quot;</mark> for requests.<br/>â— Captures an image when it receives a request.<br/>â— Publishes the image to <br/><mark class="highlight-teal">&quot;image/data&quot;</mark>.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1bfaecc5-ba04-80ea-9efc-ea5a5bf47296" class="code"><code class="language-Python">import cv2
import paho.mqtt.client as mqtt
import numpy as np
import base64
BROKER = &quot;192.168.2.2&quot; # Raspberry Pi&#x27;s IP Address
TOPIC_REQUEST = &quot;image/request&quot;
TOPIC_IMAGE = &quot;image/data&quot;
# Function to handle incoming requests
def on_message(client, userdata, message):
&quot;&quot;&quot;Receives request and captures an image.&quot;&quot;&quot;
if message.payload.decode() == &quot;capture_image&quot;:
print(&quot;ðŸ“¸ Capture request received. Taking image...&quot;)
# Capture image from the webcam
cam = cv2.VideoCapture(0) # Use default camera
ret, frame = cam.read()
cam.release()
if ret:
# Convert image to bytes
_, buffer = cv2.imencode(&#x27;.jpg&#x27;, frame)
image_bytes = buffer.tobytes()
# Encode image as Base64
image_b64 = base64.b64encode(image_bytes).decode()
# Publish the image to MQTT
client.publish(TOPIC_IMAGE, image_b64)
print(&quot;âœ… Image captured and sent via MQTT.&quot;)
else:
print(&quot;âŒ Failed to capture image.&quot;)
# Set up MQTT subscriber for request
client = mqtt.Client(&quot;Webcam_Client&quot;)
client.on_message = on_message
client.connect(BROKER, 1883)
# Subscribe to the request topic
client.subscribe(TOPIC_REQUEST)
print(&quot;ðŸŽ¥ Waiting for image capture requests...&quot;)
client.loop_forever()</code></pre><p id="1bfaecc5-ba04-80e3-97fa-c2d214d47134" class=""><strong>Results</strong></p><p id="1bfaecc5-ba04-80cf-bdb0-ca559fb65fde" class="">On Raspberry Pi</p><figure id="1bfaecc5-ba04-807e-979f-f82c33f0deda" class="image"><a href="image%203.png"><img style="width:681.9833374023438px" src="image%203.png"/></a></figure><p id="1bfaecc5-ba04-809e-8d4e-d00f07c7f034" class="">On Jetson Nano</p><figure id="1bfaecc5-ba04-80e4-914d-e086aee1cc5e" class="image"><a href="image%204.png"><img style="width:682px" src="image%204.png"/></a></figure></div></details></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>